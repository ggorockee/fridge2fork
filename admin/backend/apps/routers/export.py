"""
ğŸ“¤ğŸ“¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸° API ë¼ìš°í„°
CSV/JSON/Excel í˜•ì‹ì˜ ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°, ë°±ì—…/ë³µì› ê¸°ëŠ¥ ì œê³µ
"""
import csv
import json
import tempfile
import os
import gzip
import base64
from datetime import datetime
from io import StringIO, BytesIO
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, status, Query, Form, File, UploadFile
from fastapi.responses import StreamingResponse, FileResponse
from sqlalchemy.orm import Session
from sqlalchemy import text, inspect

from ..database import get_db
from ..schemas import (
    ExportRequest, ExportResponse, ImportRequest, ImportResponse,
    BackupRequest, BackupResponse, RestoreRequest, RestoreResponse,
    ImportValidationError
)
from ..models import Ingredient, Recipe, RecipeIngredient
from ..logging_config import get_logger

router = APIRouter(tags=["ğŸ“¤ğŸ“¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°"])
logger = get_logger(__name__)

# ì§€ì›ë˜ëŠ” í…Œì´ë¸” ë§¤í•‘
TABLE_MODELS = {
    "ingredients": Ingredient,
    "recipes": Recipe,
    "recipe_ingredients": RecipeIngredient
}

# ì§€ì›ë˜ëŠ” í˜•ì‹
SUPPORTED_FORMATS = ["csv", "json", "excel"]


# ===== ë°ì´í„° ë‚´ë³´ë‚´ê¸° =====

@router.get("/export/{table}/{format}")
async def export_data(
    table: str,
    format: str,
    columns: Optional[str] = Query(None, description="ë‚´ë³´ë‚¼ ì»¬ëŸ¼ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    limit: Optional[int] = Query(None, ge=1, le=10000, description="ì œí•œ ê°œìˆ˜"),
    include_headers: bool = Query(True, description="í—¤ë” í¬í•¨ ì—¬ë¶€"),
    date_format: str = Query("ISO", description="ë‚ ì§œ í˜•ì‹"),
    db: Session = Depends(get_db)
):
    """ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    logger.info(f"ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {table}.{format}")

    # ìœ íš¨ì„± ê²€ì‚¬
    if table not in TABLE_MODELS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {table}. ì§€ì› í…Œì´ë¸”: {list(TABLE_MODELS.keys())}"
        )

    if format not in SUPPORTED_FORMATS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}. ì§€ì› í˜•ì‹: {SUPPORTED_FORMATS}"
        )

    try:
        model = TABLE_MODELS[table]

        # ë°ì´í„° ì¡°íšŒ
        query = db.query(model)
        if limit:
            query = query.limit(limit)

        records = query.all()

        if not records:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            )

        # ì»¬ëŸ¼ ì„ íƒ
        if columns:
            selected_columns = [col.strip() for col in columns.split(",")]
        else:
            # ëª¨ë“  ì»¬ëŸ¼ í¬í•¨
            inspector = inspect(model)
            selected_columns = [column.name for column in inspector.columns]

        # ë°ì´í„° ë³€í™˜
        data = []
        for record in records:
            row = {}
            for column in selected_columns:
                if hasattr(record, column):
                    value = getattr(record, column)
                    # ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                    if isinstance(value, datetime):
                        if date_format == "ISO":
                            value = value.isoformat()
                        else:
                            value = value.strftime("%Y-%m-%d %H:%M:%S")
                    row[column] = value
            data.append(row)

        # í˜•ì‹ë³„ ë‚´ë³´ë‚´ê¸°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{table}_{timestamp}.{format}"

        if format == "csv":
            return export_csv(data, filename, include_headers)
        elif format == "json":
            return export_json(data, filename)
        elif format == "excel":
            return export_excel(data, filename, include_headers)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/export/custom", response_model=ExportResponse)
async def export_custom_data(
    request: ExportRequest,
    db: Session = Depends(get_db)
):
    """ğŸ“¤ ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    logger.info(f"ğŸ“¤ ì‚¬ìš©ì ì •ì˜ ë‚´ë³´ë‚´ê¸°: {request.table}.{request.format}")

    try:
        if request.table not in TABLE_MODELS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {request.table}"
            )

        if request.format not in SUPPORTED_FORMATS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {request.format}"
            )

        model = TABLE_MODELS[request.table]
        query = db.query(model)

        # í•„í„° ì ìš©
        if request.filters:
            for field, value in request.filters.items():
                if hasattr(model, field):
                    if isinstance(value, list):
                        query = query.filter(getattr(model, field).in_(value))
                    else:
                        query = query.filter(getattr(model, field) == value)

        records = query.all()
        record_count = len(records)

        # ì»¬ëŸ¼ ì„ íƒ
        if request.columns:
            selected_columns = request.columns
        else:
            inspector = inspect(model)
            selected_columns = [column.name for column in inspector.columns]

        # ë°ì´í„° ë³€í™˜
        data = []
        for record in records:
            row = {}
            for column in selected_columns:
                if hasattr(record, column):
                    value = getattr(record, column)
                    if isinstance(value, datetime):
                        if request.date_format == "ISO":
                            value = value.isoformat()
                        else:
                            value = value.strftime("%Y-%m-%d %H:%M:%S")
                    row[column] = value
            data.append(row)

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{request.table}_{timestamp}.{request.format}"

        with tempfile.NamedTemporaryFile(mode='w+b', delete=False, suffix=f".{request.format}") as temp_file:
            file_path = temp_file.name

            if request.format == "csv":
                write_csv_to_file(data, file_path, request.include_headers)
            elif request.format == "json":
                write_json_to_file(data, file_path)
            elif request.format == "excel":
                write_excel_to_file(data, file_path, request.include_headers)

        file_size = os.path.getsize(file_path)

        # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ URL ìƒì„± (ì‹¤ì œë¡œëŠ” ë³„ë„ ì €ì¥ì†Œ ì‚¬ìš© ê¶Œì¥)
        download_url = f"/download/{os.path.basename(file_path)}"

        return ExportResponse(
            success=True,
            message=f"ë°ì´í„° ë‚´ë³´ë‚´ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            download_url=download_url,
            file_name=filename,
            file_size_bytes=file_size,
            record_count=record_count,
            exported_at=datetime.now()
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì‚¬ìš©ì ì •ì˜ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‚¬ìš©ì ì •ì˜ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ===== ë°ì´í„° ê°€ì ¸ì˜¤ê¸° =====

@router.post("/import/{table}/{format}", response_model=ImportResponse)
async def import_data(
    table: str,
    format: str,
    file: UploadFile = File(...),
    skip_duplicates: bool = Form(True),
    validate_only: bool = Form(False),
    auto_map_columns: bool = Form(True),
    db: Session = Depends(get_db)
):
    """ğŸ“¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    logger.info(f"ğŸ“¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°: {table}.{format}")

    try:
        if table not in TABLE_MODELS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {table}"
            )

        if format not in SUPPORTED_FORMATS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}"
            )

        # íŒŒì¼ ì½ê¸°
        content = await file.read()

        # í˜•ì‹ë³„ íŒŒì‹±
        if format == "csv":
            data = parse_csv_data(content)
        elif format == "json":
            data = parse_json_data(content)
        elif format == "excel":
            data = parse_excel_data(content)
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}"
            )

        if not data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ê°€ì ¸ì˜¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            )

        # ë°ì´í„° ê²€ì¦
        validation_errors = validate_import_data(table, data, auto_map_columns)

        if validation_errors and not validate_only:
            # ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê¸° ì¤‘ë‹¨
            critical_errors = [err for err in validation_errors if err.severity == "error"]
            if critical_errors:
                return ImportResponse(
                    success=False,
                    message=f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {len(critical_errors)}ê°œì˜ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤",
                    total_rows=len(data),
                    imported_rows=0,
                    skipped_rows=0,
                    error_rows=len(critical_errors),
                    validation_errors=validation_errors,
                    imported_ids=[],
                    import_time_ms=0,
                    imported_at=datetime.now()
                )

        if validate_only:
            return ImportResponse(
                success=len(validation_errors) == 0,
                message=f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validation_errors)}ê°œì˜ ë¬¸ì œ ë°œê²¬",
                total_rows=len(data),
                imported_rows=0,
                skipped_rows=0,
                error_rows=len(validation_errors),
                validation_errors=validation_errors,
                imported_ids=[],
                import_time_ms=0,
                imported_at=datetime.now()
            )

        # ì‹¤ì œ ê°€ì ¸ì˜¤ê¸°
        import_result = import_data_to_db(table, data, skip_duplicates, db)

        return import_result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ===== ë°±ì—… ë° ë³µì› =====

@router.post("/export/backup", response_model=BackupResponse)
async def create_backup(
    request: BackupRequest,
    db: Session = Depends(get_db)
):
    """ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
    logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘")

    try:
        backup_data = {}
        tables_included = []

        # ë°±ì—…í•  í…Œì´ë¸” ê²°ì •
        if request.tables:
            tables_to_backup = [t for t in request.tables if t in TABLE_MODELS]
        else:
            tables_to_backup = list(TABLE_MODELS.keys())

        # ê° í…Œì´ë¸” ë°ì´í„° ìˆ˜ì§‘
        for table_name in tables_to_backup:
            model = TABLE_MODELS[table_name]
            records = db.query(model).all()

            table_data = []
            for record in records:
                # ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                record_dict = {}
                for column in inspect(model).columns:
                    value = getattr(record, column.name)
                    if isinstance(value, datetime):
                        value = value.isoformat()
                    record_dict[column.name] = value
                table_data.append(record_dict)

            backup_data[table_name] = table_data
            tables_included.append(table_name)

        # ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨
        if request.include_schema:
            backup_data["_schema"] = {
                "version": "1.0",
                "created_at": datetime.now().isoformat(),
                "tables": tables_included,
                "record_counts": {table: len(backup_data[table]) for table in tables_included}
            }

        # JSONìœ¼ë¡œ ì§ë ¬í™”
        backup_json = json.dumps(backup_data, ensure_ascii=False, indent=2)

        # ì••ì¶•
        if request.compress:
            backup_bytes = backup_json.encode('utf-8')
            compressed = gzip.compress(backup_bytes)
            final_data = base64.b64encode(compressed).decode('ascii')
            file_extension = "json.gz.b64"
        else:
            final_data = backup_json
            file_extension = "json"

        # ì•”í˜¸í™” (ê°„ë‹¨í•œ base64 ì¸ì½”ë”©ìœ¼ë¡œ ëŒ€ì²´)
        if request.encryption_key:
            # ì‹¤ì œë¡œëŠ” ì ì ˆí•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
            final_data = base64.b64encode(final_data.encode('utf-8')).decode('ascii')
            file_extension += ".enc"

        # ë°±ì—… íŒŒì¼ ì €ì¥
        backup_id = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        file_name = f"{backup_id}.{file_extension}"

        # ì„ì‹œ íŒŒì¼ì— ì €ì¥ (ì‹¤ì œë¡œëŠ” ë³„ë„ ì €ì¥ì†Œ ì‚¬ìš©)
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=f".{file_extension}") as temp_file:
            temp_file.write(final_data)
            file_path = temp_file.name

        file_size = os.path.getsize(file_path)

        return BackupResponse(
            success=True,
            message=f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            backup_id=backup_id,
            file_name=file_name,
            file_size_bytes=file_size,
            tables_included=tables_included,
            backup_time_ms=1000,  # ì‹¤ì œ ì‹œê°„ ì¸¡ì • í•„ìš”
            created_at=datetime.now()
        )

    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/import/restore", response_model=RestoreResponse)
async def restore_backup(
    request: RestoreRequest,
    db: Session = Depends(get_db)
):
    """ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ë³µì›"""
    logger.info("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ë³µì› ì‹œì‘")

    try:
        # ë°±ì—… ë°ì´í„° ë³µí˜¸í™” ë° ì••ì¶• í•´ì œ
        backup_data_str = request.backup_data

        # ì•”í˜¸í™”ëœ ê²½ìš° ë³µí˜¸í™”
        if request.decryption_key:
            try:
                backup_data_str = base64.b64decode(backup_data_str).decode('utf-8')
            except Exception:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="ë°±ì—… ë°ì´í„° ë³µí˜¸í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                )

        # ì••ì¶•ëœ ê²½ìš° ì••ì¶• í•´ì œ
        try:
            # base64 ë””ì½”ë”© ì‹œë„
            compressed_data = base64.b64decode(backup_data_str)
            # gzip ì••ì¶• í•´ì œ ì‹œë„
            backup_data_str = gzip.decompress(compressed_data).decode('utf-8')
        except Exception:
            # ì••ì¶•ë˜ì§€ ì•Šì€ ë°ì´í„°ë¡œ ê°„ì£¼
            pass

        # JSON íŒŒì‹±
        try:
            backup_data = json.loads(backup_data_str)
        except json.JSONDecodeError as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ë°±ì—… ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
            )

        # ë³µì› ì „ ê²€ì¦
        if request.validate_before_restore:
            validation_errors = validate_backup_data(backup_data)
            if validation_errors:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"ë°±ì—… ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {validation_errors}"
                )

        # ë³µì›í•  í…Œì´ë¸” ê²°ì •
        available_tables = [t for t in backup_data.keys() if t != "_schema" and t in TABLE_MODELS]
        if request.tables:
            tables_to_restore = [t for t in request.tables if t in available_tables]
        else:
            tables_to_restore = available_tables

        restored_tables = []
        total_records = 0

        # ê° í…Œì´ë¸” ë³µì›
        for table_name in tables_to_restore:
            if table_name not in backup_data:
                continue

            model = TABLE_MODELS[table_name]
            table_data = backup_data[table_name]

            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë®ì–´ì“°ê¸° ëª¨ë“œ)
            if request.overwrite_existing:
                db.query(model).delete()

            # ë°ì´í„° ì‚½ì…
            for record_data in table_data:
                # datetime ë¬¸ìì—´ì„ ë‹¤ì‹œ datetime ê°ì²´ë¡œ ë³€í™˜
                for key, value in record_data.items():
                    if isinstance(value, str) and "T" in value:
                        try:
                            record_data[key] = datetime.fromisoformat(value.replace("Z", "+00:00"))
                        except ValueError:
                            pass

                # ì¤‘ë³µ í™•ì¸
                if not request.overwrite_existing:
                    primary_key = inspect(model).primary_key[0].name
                    existing = db.query(model).filter(
                        getattr(model, primary_key) == record_data[primary_key]
                    ).first()
                    if existing:
                        continue

                # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                new_record = model(**record_data)
                db.add(new_record)

            restored_tables.append(table_name)
            total_records += len(table_data)

        # ì»¤ë°‹
        db.commit()

        return RestoreResponse(
            success=True,
            message=f"ë°ì´í„°ë² ì´ìŠ¤ ë³µì›ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            restored_tables=restored_tables,
            total_records=total_records,
            restore_time_ms=1000,  # ì‹¤ì œ ì‹œê°„ ì¸¡ì • í•„ìš”
            restored_at=datetime.now()
        )

    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë³µì› ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„°ë² ì´ìŠ¤ ë³µì› ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====

def export_csv(data: List[Dict], filename: str, include_headers: bool) -> StreamingResponse:
    """CSV ë‚´ë³´ë‚´ê¸°"""
    output = StringIO()
    if data:
        writer = csv.DictWriter(output, fieldnames=data[0].keys())
        if include_headers:
            writer.writeheader()
        writer.writerows(data)

    output.seek(0)
    response = StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )
    return response


def export_json(data: List[Dict], filename: str) -> StreamingResponse:
    """JSON ë‚´ë³´ë‚´ê¸°"""
    json_str = json.dumps(data, ensure_ascii=False, indent=2, default=str)
    response = StreamingResponse(
        iter([json_str]),
        media_type="application/json",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )
    return response


def export_excel(data: List[Dict], filename: str, include_headers: bool) -> StreamingResponse:
    """Excel ë‚´ë³´ë‚´ê¸°"""
    try:
        import pandas as pd
        df = pd.DataFrame(data)

        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, header=include_headers)

        output.seek(0)
        response = StreamingResponse(
            iter([output.getvalue()]),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        return response
    except ImportError:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Excel ë‚´ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ pandasì™€ openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤"
        )


def write_csv_to_file(data: List[Dict], file_path: str, include_headers: bool):
    """CSV íŒŒì¼ ì‘ì„±"""
    with open(file_path, 'w', newline='', encoding='utf-8') as f:
        if data:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            if include_headers:
                writer.writeheader()
            writer.writerows(data)


def write_json_to_file(data: List[Dict], file_path: str):
    """JSON íŒŒì¼ ì‘ì„±"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)


def write_excel_to_file(data: List[Dict], file_path: str, include_headers: bool):
    """Excel íŒŒì¼ ì‘ì„±"""
    try:
        import pandas as pd
        df = pd.DataFrame(data)
        df.to_excel(file_path, index=False, header=include_headers)
    except ImportError:
        raise Exception("Excel íŒŒì¼ ì‘ì„±ì„ ìœ„í•´ pandasì™€ openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤")


def parse_csv_data(content: bytes) -> List[Dict]:
    """CSV ë°ì´í„° íŒŒì‹±"""
    content_str = content.decode('utf-8')
    reader = csv.DictReader(StringIO(content_str))
    return list(reader)


def parse_json_data(content: bytes) -> List[Dict]:
    """JSON ë°ì´í„° íŒŒì‹±"""
    content_str = content.decode('utf-8')
    data = json.loads(content_str)
    if isinstance(data, list):
        return data
    elif isinstance(data, dict):
        return [data]
    else:
        raise ValueError("JSON ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤")


def parse_excel_data(content: bytes) -> List[Dict]:
    """Excel ë°ì´í„° íŒŒì‹±"""
    try:
        import pandas as pd
        df = pd.read_excel(BytesIO(content))
        return df.to_dict('records')
    except ImportError:
        raise Exception("Excel íŒŒì¼ íŒŒì‹±ì„ ìœ„í•´ pandasì™€ openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤")


def validate_import_data(table: str, data: List[Dict], auto_map_columns: bool) -> List[ImportValidationError]:
    """ë°ì´í„° ê²€ì¦"""
    errors = []
    model = TABLE_MODELS[table]
    inspector = inspect(model)

    required_columns = [col.name for col in inspector.columns if not col.nullable and not col.default]
    available_columns = [col.name for col in inspector.columns]

    for row_idx, row in enumerate(data):
        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
        for required_col in required_columns:
            if required_col not in row or row[required_col] is None:
                errors.append(ImportValidationError(
                    row=row_idx + 1,
                    column=required_col,
                    value=str(row.get(required_col, "")),
                    error=f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤",
                    severity="error"
                ))

        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ê²€ì‚¬
        for col in row.keys():
            if col not in available_columns:
                errors.append(ImportValidationError(
                    row=row_idx + 1,
                    column=col,
                    value=str(row[col]),
                    error=f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ì…ë‹ˆë‹¤",
                    severity="warning"
                ))

    return errors


def import_data_to_db(table: str, data: List[Dict], skip_duplicates: bool, db: Session) -> ImportResponse:
    """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    start_time = datetime.now()
    model = TABLE_MODELS[table]

    imported_rows = 0
    skipped_rows = 0
    error_rows = 0
    imported_ids = []
    validation_errors = []

    try:
        for row_idx, row_data in enumerate(data):
            try:
                # ì¤‘ë³µ ê²€ì‚¬ (ì´ë¦„ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨íˆ)
                if skip_duplicates and hasattr(model, 'name'):
                    existing = db.query(model).filter(
                        getattr(model, 'name') == row_data.get('name')
                    ).first()
                    if existing:
                        skipped_rows += 1
                        continue

                # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                # ID í•„ë“œ ì œê±° (ìë™ ìƒì„±)
                clean_data = {k: v for k, v in row_data.items()
                             if k not in ['ingredient_id', 'recipe_id'] and v is not None}

                new_record = model(**clean_data)
                db.add(new_record)
                db.flush()  # ID ì–»ê¸°

                imported_rows += 1

                # ID ìˆ˜ì§‘
                if hasattr(new_record, 'ingredient_id'):
                    imported_ids.append(new_record.ingredient_id)
                elif hasattr(new_record, 'recipe_id'):
                    imported_ids.append(new_record.recipe_id)

            except Exception as e:
                error_rows += 1
                validation_errors.append(ImportValidationError(
                    row=row_idx + 1,
                    column="ì „ì²´",
                    value=str(row_data),
                    error=str(e),
                    severity="error"
                ))

        db.commit()

        end_time = datetime.now()
        import_time_ms = int((end_time - start_time).total_seconds() * 1000)

        return ImportResponse(
            success=error_rows == 0,
            message=f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {imported_rows}ê°œ ì„±ê³µ, {error_rows}ê°œ ì‹¤íŒ¨, {skipped_rows}ê°œ ê±´ë„ˆë›°ê¸°",
            total_rows=len(data),
            imported_rows=imported_rows,
            skipped_rows=skipped_rows,
            error_rows=error_rows,
            validation_errors=validation_errors,
            imported_ids=imported_ids,
            import_time_ms=import_time_ms,
            imported_at=end_time
        )

    except Exception as e:
        db.rollback()
        raise e


def validate_backup_data(backup_data: Dict) -> List[str]:
    """ë°±ì—… ë°ì´í„° ê²€ì¦"""
    errors = []

    if "_schema" not in backup_data:
        errors.append("ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")

    for table_name in backup_data.keys():
        if table_name.startswith("_"):
            continue
        if table_name not in TABLE_MODELS:
            errors.append(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {table_name}")

    return errors