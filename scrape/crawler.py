"""
ë§Œê°œì˜ ë ˆì‹œí”¼ (10000recipe.com) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³ ,
ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª©í‘œí•œ ìˆ˜ëŸ‰ì˜ ë ˆì‹œí”¼ë¥¼ ìˆ˜ì§‘í•  ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ë©°,
ê° ë ˆì‹œí”¼ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë™ì‹œì— ìš”ì²­í•˜ì—¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ë©°, ë™ì‹œì— recipes.jsonl íŒŒì¼ì—ë„ ê¸°ë¡ë©ë‹ˆë‹¤.

ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:
- requests: ì´ˆê¸° í˜ì´ì§€ ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
- beautifulsoup4: HTMLì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©
- aiohttp: ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ìœ„í•´ ì‚¬ìš©
- psycopg2-binary: PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ í†µì‹ í•˜ê¸° ìœ„í•´ ì‚¬ìš©
- python-dotenv: í™˜ê²½ ë³€ìˆ˜ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©

ì‚¬ìš©ë²•:
    1. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (.env.example ì°¸ê³ )
    2. pip install -r requirements.txt
    3. python crawler.py

CI í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì—…ë°ì´íŠ¸
"""

import asyncio
import json
import re
import time
import os
import logging
import aiohttp
import requests
import psycopg2
from psycopg2.pool import SimpleConnectionPool
from bs4 import BeautifulSoup
from urllib.robotparser import RobotFileParser
from dotenv import load_dotenv
from datetime import datetime

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • (Configuration) ---
load_dotenv()

# --- ë¡œê¹… ì„¤ì • ---
def setup_logging():
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ë¡œê·¸ í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ë§Œ ì‚¬ìš© (Pod ë¡œê·¸ë¡œ í™•ì¸)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(console_handler)
    
    return logger

# ë¡œê¹… ì´ˆê¸°í™”
logger = setup_logging()

TARGET_RECIPE_COUNT = 100
BASE_URL = "https://www.10000recipe.com"
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
HEADERS = {'User-Agent': USER_AGENT}
OUTPUT_FILENAME = "recipes.jsonl"
CONCURRENT_REQUESTS = 3  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œì–´

# --- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ---
DB_POOL = None
try:
    DATABASE_URL = (
        f"dbname={os.getenv('POSTGRES_DB')} "
        f"user={os.getenv('POSTGRES_USER')} "
        f"password={os.getenv('POSTGRES_PASSWORD')} "
        f"host={os.getenv('POSTGRES_SERVER')} "
        f"port={os.getenv('POSTGRES_PORT')}"
    )
    DB_POOL = SimpleConnectionPool(minconn=1, maxconn=5, dsn=DATABASE_URL)
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ìƒì„± ì„±ê³µ")
    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´: {os.getenv('POSTGRES_SERVER')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}")
except Exception as e:
    logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ìƒì„± ì‹¤íŒ¨: {e}")
    DB_POOL = None


# --- CSS ì„ íƒì (Selectors) ---
SELECTORS = {
    "recipe_links_primary": "li.common_sp_list_li a.common_sp_link",
    "recipe_links_fallback": "div.common_sp_thumb a",
    "title": "div.view2_summary h3",
    "image": "#main_thumbs",
    "description": "#recipeIntro",
    "ingredients_container": "#divConfirmedMaterialArea ul",
    "ingredient_item": "li",
}

# --- robots.txt íŒŒì„œ ì„¤ì • ---
rp = RobotFileParser()
rp.set_url(f"{BASE_URL}/robots.txt")
rp.read()

# --- Helper Functions ---
def parse_ingredient(text):
    """
    ë‹¨ì¼ ì¬ë£Œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì´ë¦„, ìˆ˜ëŸ‰, ë‹¨ìœ„ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
    ì˜ˆ: "ë‹¤ì§„ ë§ˆëŠ˜ 1/2í°ìˆ " -> {'name': 'ë‹¤ì§„ ë§ˆëŠ˜', 'quantity_from': 0.5, 'quantity_to': None, 'unit': 'í°ìˆ '}
    """
    original_text = text.replace("êµ¬ë§¤", "").strip()
    text = original_text

    # ìˆ˜ëŸ‰/ë‹¨ìœ„ íŒ¨í„´ ë§¤ì¹­ (ë¬¸ìì—´ ëì—ì„œë¶€í„°)
    pattern = r"(.+?)\s*([\d./~-]+)\s*([\wê°€-í£]*)$"
    match = re.match(pattern, text)
    
    name, quant_from, quant_to, unit = None, None, None, None

    if match:
        name = match.group(1).strip()
        quantity_str = match.group(2)
        unit = match.group(3).strip() if match.group(3) else None

        try:
            if '~' in quantity_str or '-' in quantity_str:
                parts = re.split(r'[~-]', quantity_str)
                quant_from = float(parts[0])
                quant_to = float(parts[1])
            elif '/' in quantity_str:
                parts = quantity_str.split('/')
                quant_from = float(parts[0]) / float(parts[1])
            else:
                quant_from = float(quantity_str)
        except (ValueError, IndexError, ZeroDivisionError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
            name = original_text
            quant_from, quant_to, unit = None, None, None
    else:
        name = text

    return {"name": name, "quantity_from": quant_from, "quantity_to": quant_to, "unit": unit}

# --- Database Functions ---
async def insert_recipe_data(pool, recipe_data):
    """
    ë‹¨ì¼ ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— Upsertí•©ë‹ˆë‹¤.
    """
    conn = None
    try:
        conn = pool.getconn()
        cursor = conn.cursor()

        # 1. recipes í…Œì´ë¸”ì— Upsert
        cursor.execute(
            """INSERT INTO recipes (url, title, description, image_url)
               VALUES (%s, %s, %s, %s)
               ON CONFLICT (url) DO UPDATE SET
                 title = EXCLUDED.title,
                 description = EXCLUDED.description,
                 image_url = EXCLUDED.image_url
               RETURNING recipe_id;""",
            (recipe_data['url'], recipe_data['title'], recipe_data['description'], recipe_data['image_url'])
        )
        recipe_id = cursor.fetchone()[0]

        # 2. ingredients ë° recipe_ingredients í…Œì´ë¸” ì²˜ë¦¬
        for ing in recipe_data['ingredients']:
            # 2a. ingredients í…Œì´ë¸”ì— Upsert (DO NOTHING)
            cursor.execute(
                """INSERT INTO ingredients (name)
                   VALUES (%s)
                   ON CONFLICT (name) DO NOTHING
                   RETURNING ingredient_id;""",
                (ing['name'],)
            )
            result = cursor.fetchone()
            if result:
                ingredient_id = result[0]
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ì—¬ idê°€ ë°˜í™˜ë˜ì§€ ì•Šì€ ê²½ìš°, idë¥¼ ì¡°íšŒ
                cursor.execute("SELECT ingredient_id FROM ingredients WHERE name = %s;", (ing['name'],))
                ingredient_id = cursor.fetchone()[0]

            # 2b. recipe_ingredients í…Œì´ë¸”ì— Upsert
            cursor.execute(
                """INSERT INTO recipe_ingredients (recipe_id, ingredient_id, quantity_from, quantity_to, unit)
                   VALUES (%s, %s, %s, %s, %s)
                   ON CONFLICT (recipe_id, ingredient_id) DO UPDATE SET
                     quantity_from = EXCLUDED.quantity_from,
                     quantity_to = EXCLUDED.quantity_to,
                     unit = EXCLUDED.unit;""",
                (recipe_id, ingredient_id, ing['quantity_from'], ing['quantity_to'], ing['unit'])
            )
        
        conn.commit()
        logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {recipe_data['title']} (ì¬ë£Œ {len(recipe_data['ingredients'])}ê°œ)")

    except (Exception, psycopg2.DatabaseError) as error:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {recipe_data.get('title', 'N/A')} - {error}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            pool.putconn(conn)

# --- Synchronous Functions ---
def get_all_recipe_urls(target_count):
    """
    ëª©í‘œ ìˆ˜ëŸ‰ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° ëª¨ë“  ë ˆì‹œí”¼ì˜ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸš€ ë ˆì‹œí”¼ URL ìˆ˜ì§‘ ì‹œì‘ - ëª©í‘œ: {target_count}ê°œ")
    start_time = time.time()
    recipe_urls = set()
    page = 1
    while len(recipe_urls) < target_count:
        list_url = f"{BASE_URL}/recipe/list.html?page={page}"
        if not rp.can_fetch(USER_AGENT, list_url):
            logger.warning(f"âš ï¸ robots.txtì— ì˜í•´ í˜ì´ì§€ {page} ì ‘ê·¼ì´ ì°¨ë‹¨ë¨")
            break

        logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘... (í˜„ì¬ ìˆ˜ì§‘: {len(recipe_urls)}/{target_count})")
        try:
            response = requests.get(list_url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            
            links_on_page = soup.select(SELECTORS["recipe_links_primary"]) or soup.select(SELECTORS["recipe_links_fallback"])
            
            if not links_on_page:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            new_links_count = 0
            for link in links_on_page:
                href = link['href']
                full_url = BASE_URL + href if not href.startswith('http') else href
                if rp.can_fetch(USER_AGENT, full_url):
                    recipe_urls.add(full_url)
                    new_links_count += 1
                if len(recipe_urls) >= target_count:
                    break
            
            logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ - ìƒˆë¡œ ìˆ˜ì§‘ëœ ë§í¬: {new_links_count}ê°œ")
            page += 1
            time.sleep(2) # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e} - 5ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(5)

    elapsed_time = time.time() - start_time
    logger.info(f"ğŸ¯ URL ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(recipe_urls)}ê°œ ìˆ˜ì§‘ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
    return list(recipe_urls)[:target_count]

# --- Asynchronous Functions ---
async def scrape_recipe_details(session, url, semaphore):
    """
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°œë³„ ë ˆì‹œí”¼ í˜ì´ì§€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìŠ¤í¬ë©í•©ë‹ˆë‹¤.
    """
    async with semaphore:
        try:
            await asyncio.sleep(1) # ì¶”ê°€ì ì¸ ìš”ì²­ ê°„ ì§€ì—°
            logger.debug(f"ğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            async with session.get(url, headers=HEADERS) as response:
                response.raise_for_status()
                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                title = soup.select_one(SELECTORS["title"]).get_text(strip=True) if soup.select_one(SELECTORS["title"]) else "ì œëª© ì—†ìŒ"
                image_url = soup.select_one(SELECTORS["image"])['src'] if soup.select_one(SELECTORS["image"]) else "ì´ë¯¸ì§€ ì—†ìŒ"
                description = soup.select_one(SELECTORS["description"]).get_text(strip=True) if soup.select_one(SELECTORS["description"]) else "ì„¤ëª… ì—†ìŒ"

                ingredients = []
                container = soup.select_one(SELECTORS["ingredients_container"])
                if container:
                    items = container.select(SELECTORS["ingredient_item"])
                    for item in items:
                        full_text = re.sub(r'\s+', ' ', item.get_text(separator=" ").replace('\n', '')).strip()
                        if full_text:
                            ingredients.append(parse_ingredient(full_text))

                logger.debug(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {title} (ì¬ë£Œ {len(ingredients)}ê°œ)")
                return {
                    "url": url,
                    "title": title,
                    "image_url": image_url,
                    "description": description,
                    "ingredients": ingredients,
                }
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url} - {e}")
            return None

async def main():
    """
    ë¹„ë™ê¸° í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ê´„í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ ë§Œê°œì˜ ë ˆì‹œí”¼ í¬ë¡¤ëŸ¬ ì‹œì‘")
    logger.info(f"ğŸ“Š ì„¤ì •: ëª©í‘œ {TARGET_RECIPE_COUNT}ê°œ, ë™ì‹œ ìš”ì²­ {CONCURRENT_REQUESTS}ê°œ")
    logger.info("=" * 60)
    
    if not DB_POOL:
        logger.error("ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    start_time = time.time()
    recipe_urls = get_all_recipe_urls(TARGET_RECIPE_COUNT)
    logger.info(f"\nğŸ“‹ ì´ {len(recipe_urls)}ê°œì˜ URLì— ëŒ€í•œ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    scraped_count = 0
    failed_count = 0
    db_tasks = []

    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession() as session:
        logger.info("ğŸ”„ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        tasks = [scrape_recipe_details(session, url, semaphore) for url in recipe_urls]
        results = await asyncio.gather(*tasks)
        
        logger.info("ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì²˜ë¦¬ ë° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            for i, result in enumerate(results, 1):
                if result:
                    # 1. íŒŒì¼ì— ê¸°ë¡ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                    scraped_count += 1
                    
                    # 2. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    db_tasks.append(insert_recipe_data(DB_POOL, result))
                    
                    # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
                    if i % 10 == 0:
                        logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i}/{len(results)} ({i/len(results)*100:.1f}%)")
                else:
                    failed_count += 1

            if db_tasks:
                logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘...")
                await asyncio.gather(*db_tasks)
    
    total_time = time.time() - start_time
    logger.info("=" * 60)
    logger.info("ğŸ‰ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
    logger.info(f"   âœ… ì„±ê³µ: {scraped_count}ê°œ")
    logger.info(f"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"   â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"   ğŸ“ ì¶œë ¥íŒŒì¼: {OUTPUT_FILENAME}")
    logger.info("=" * 60)

if __name__ == "__main__":
    try:
        logger.info(f"ğŸ• í¬ë¡¤ëŸ¬ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.warning("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if DB_POOL:
            DB_POOL.closeall()
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ì„ ëª¨ë‘ ë‹«ì•˜ìŠµë‹ˆë‹¤.")
        logger.info(f"ğŸ• í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")