"""
ë§Œê°œì˜ ë ˆì‹œí”¼ (10000recipe.com) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³ ,
ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª©í‘œí•œ ìˆ˜ëŸ‰ì˜ ë ˆì‹œí”¼ë¥¼ ìˆ˜ì§‘í•  ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ë©°,
ê° ë ˆì‹œí”¼ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë™ì‹œì— ìš”ì²­í•˜ì—¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ë©°, ë™ì‹œì— recipes.jsonl íŒŒì¼ì—ë„ ê¸°ë¡ë©ë‹ˆë‹¤.

ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:
- requests: ì´ˆê¸° í˜ì´ì§€ ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
- beautifulsoup4: HTMLì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©
- aiohttp: ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ìœ„í•´ ì‚¬ìš©
- psycopg2-binary: PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ í†µì‹ í•˜ê¸° ìœ„í•´ ì‚¬ìš©
- python-dotenv: í™˜ê²½ ë³€ìˆ˜ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©

ì‚¬ìš©ë²•:
    1. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (.env.example ì°¸ê³ )
    2. pip install -r requirements.txt
    3. python crawler.py

CI í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì—…ë°ì´íŠ¸
"""

import asyncio
import json
import re
import time
import os
import logging
import aiohttp
import requests
import psycopg2
from psycopg2.pool import SimpleConnectionPool
from bs4 import BeautifulSoup
from urllib.robotparser import RobotFileParser
from dotenv import load_dotenv
from datetime import datetime
import psutil

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • (Configuration) ---
load_dotenv()

# --- ë¡œê¹… ì„¤ì • ---
def setup_logging():
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ë¡œê·¸ í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ë§Œ ì‚¬ìš© (Pod ë¡œê·¸ë¡œ í™•ì¸)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(console_handler)
    
    return logger

# ë¡œê¹… ì´ˆê¸°í™”
logger = setup_logging()

# í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ì œì–´ (ConfigMapìœ¼ë¡œ ì£¼ì…)
TARGET_RECIPE_COUNT = int(os.getenv('TARGET_RECIPE_COUNT', '100'))
BASE_URL = "https://www.10000recipe.com"
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
HEADERS = {'User-Agent': USER_AGENT}
OUTPUT_FILENAME = "recipes.jsonl"
CONCURRENT_REQUESTS = int(os.getenv('CONCURRENT_REQUESTS', '2'))  # ë§¤ìš° ë³´ìˆ˜ì 
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '5'))  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
REQUEST_DELAY = float(os.getenv('REQUEST_DELAY', '3.0'))  # ìš”ì²­ ê°„ ì§€ì—°
BATCH_DELAY = float(os.getenv('BATCH_DELAY', '5.0'))  # ë°°ì¹˜ ê°„ ì§€ì—°
PROGRESS_INTERVAL = int(os.getenv('PROGRESS_INTERVAL', '10'))  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²©
MEMORY_CHECK_INTERVAL = int(os.getenv('MEMORY_CHECK_INTERVAL', '50'))  # ë©”ëª¨ë¦¬ ì²´í¬ ê°„ê²©

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì²­í¬ ì„¤ì •
CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '100'))  # í•œ ë²ˆì— ì²˜ë¦¬í•  URL ì²­í¬ í¬ê¸°
CHUNK_DELAY = float(os.getenv('CHUNK_DELAY', '10.0'))  # ì²­í¬ ê°„ ì§€ì—° (ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œê°„)

# --- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ---
DB_POOL = None
try:
    DATABASE_URL = (
        f"dbname={os.getenv('POSTGRES_DB')} "
        f"user={os.getenv('POSTGRES_USER')} "
        f"password={os.getenv('POSTGRES_PASSWORD')} "
        f"host={os.getenv('POSTGRES_SERVER')} "
        f"port={os.getenv('POSTGRES_PORT')}"
    )
    DB_POOL = SimpleConnectionPool(minconn=1, maxconn=5, dsn=DATABASE_URL)
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ìƒì„± ì„±ê³µ")
    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´: {os.getenv('POSTGRES_SERVER')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}")
except Exception as e:
    logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ìƒì„± ì‹¤íŒ¨: {e}")
    DB_POOL = None


# --- CSS ì„ íƒì (Selectors) ---
SELECTORS = {
    "recipe_links_primary": "li.common_sp_list_li a.common_sp_link",
    "recipe_links_fallback": "div.common_sp_thumb a",
    "title": "div.view2_summary h3",
    "image": "#main_thumbs",
    "description": "#recipeIntro",
    "ingredients_container": "#divConfirmedMaterialArea ul",
    "ingredient_item": "li",
}

# --- robots.txt íŒŒì„œ ì„¤ì • ---
rp = RobotFileParser()
rp.set_url(f"{BASE_URL}/robots.txt")
rp.read()

# --- Helper Functions ---
def log_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¡œê¹…í•©ë‹ˆë‹¤."""
    try:
        memory = psutil.virtual_memory()
        logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory.percent}% ({memory.used / 1024**3:.2f}GB / {memory.total / 1024**3:.2f}GB)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80% ì´ìƒì´ë©´ ê²½ê³ 
        if memory.percent > 80:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory.percent}%")
    except Exception as e:
        logger.debug(f"ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

def parse_ingredient(text):
    """
    ë‹¨ì¼ ì¬ë£Œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì´ë¦„, ìˆ˜ëŸ‰, ë‹¨ìœ„ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
    ì˜ˆ: "ë‹¤ì§„ ë§ˆëŠ˜ 1/2í°ìˆ " -> {'name': 'ë‹¤ì§„ ë§ˆëŠ˜', 'quantity_from': 0.5, 'quantity_to': None, 'unit': 'í°ìˆ '}
    ì˜ˆ: "ì²­ì–‘ê³ ì¶” ì•½ê°„" -> {'name': 'ì²­ì–‘ê³ ì¶”', 'is_vague': True, 'vague_description': 'ì•½ê°„', 'importance': 'optional'}
    """
    original_text = text.replace("êµ¬ë§¤", "").strip()
    text = original_text

    # ëª¨í˜¸í•œ ìˆ˜ëŸ‰ í‘œí˜„ ì²´í¬
    vague_indicators = ["ì•½ê°„", "ì ë‹¹íˆ", "ì¡°ê¸ˆ", "ë§ì´", "ì ë‹¹ëŸ‰"]
    
    for indicator in vague_indicators:
        if indicator in text:
            # ëª¨í˜¸í•œ í‘œí˜„ì„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°±ë„ ì •ë¦¬
            clean_name = text.replace(indicator, "").strip()
            # ì¶”ê°€ ê³µë°± ì œê±°
            clean_name = re.sub(r'\s+', ' ', clean_name)
            
            return {
                "name": clean_name,
                "quantity_from": None,
                "quantity_to": None,
                "unit": None,
                "is_vague": True,
                "vague_description": indicator,
                "importance": "optional"
            }

    # ì •í™•í•œ ìˆ˜ëŸ‰ì´ ìˆëŠ” ê²½ìš° ê¸°ì¡´ ë¡œì§
    pattern = r"(.+?)\s*([\d./~-]+)\s*([\wê°€-í£]*)$"
    match = re.match(pattern, text)
    
    name, quant_from, quant_to, unit = None, None, None, None

    if match:
        name = match.group(1).strip()
        quantity_str = match.group(2)
        unit = match.group(3).strip() if match.group(3) else None

        try:
            if '~' in quantity_str or '-' in quantity_str:
                parts = re.split(r'[~-]', quantity_str)
                quant_from = float(parts[0])
                quant_to = float(parts[1])
            elif '/' in quantity_str:
                parts = quantity_str.split('/')
                quant_from = float(parts[0]) / float(parts[1])
            else:
                quant_from = float(quantity_str)
        except (ValueError, IndexError, ZeroDivisionError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
            name = original_text
            quant_from, quant_to, unit = None, None, None
    else:
        name = text

    return {
        "name": name, 
        "quantity_from": quant_from, 
        "quantity_to": quant_to, 
        "unit": unit,
        "is_vague": False,
        "importance": "essential"
    }

# --- Database Functions ---
async def insert_recipe_batch(pool, recipe_batch):
    """
    ì—¬ëŸ¬ ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    ëª¨ë“  ë ˆì‹œí”¼ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    conn = None
    try:
        conn = pool.getconn()
        cursor = conn.cursor()
        
        success_count = 0
        
        # ë°°ì¹˜ ë‚´ ëª¨ë“  ë ˆì‹œí”¼ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬
        for recipe_data in recipe_batch:
            try:
                # 1. recipes í…Œì´ë¸”ì— Upsert
                cursor.execute(
                    """INSERT INTO recipes (url, title, description, image_url)
                       VALUES (%s, %s, %s, %s)
                       ON CONFLICT (url) DO UPDATE SET
                         title = EXCLUDED.title,
                         description = EXCLUDED.description,
                         image_url = EXCLUDED.image_url
                       RETURNING recipe_id;""",
                    (recipe_data['url'], recipe_data['title'], recipe_data['description'], recipe_data['image_url'])
                )
                recipe_id = cursor.fetchone()[0]

                # 2. ingredients ë° recipe_ingredients í…Œì´ë¸” ì²˜ë¦¬
                for ing in recipe_data['ingredients']:
                    # 2a. ingredients í…Œì´ë¸”ì— Upsert (is_vague, vague_description í¬í•¨)
                    cursor.execute(
                        """INSERT INTO ingredients (name, is_vague, vague_description)
                           VALUES (%s, %s, %s)
                           ON CONFLICT (name) DO UPDATE SET
                             is_vague = EXCLUDED.is_vague,
                             vague_description = EXCLUDED.vague_description
                           RETURNING ingredient_id;""",
                        (ing['name'], ing.get('is_vague', False), ing.get('vague_description'))
                    )
                    result = cursor.fetchone()
                    if result:
                        ingredient_id = result[0]
                    else:
                        # ì´ë¯¸ ì¡´ì¬í•˜ì—¬ idê°€ ë°˜í™˜ë˜ì§€ ì•Šì€ ê²½ìš°, idë¥¼ ì¡°íšŒ
                        cursor.execute("SELECT ingredient_id FROM ingredients WHERE name = %s;", (ing['name'],))
                        ingredient_id = cursor.fetchone()[0]

                    # 2b. recipe_ingredients í…Œì´ë¸”ì— Upsert (importance í¬í•¨)
                    cursor.execute(
                        """INSERT INTO recipe_ingredients (recipe_id, ingredient_id, quantity_from, quantity_to, unit, importance)
                           VALUES (%s, %s, %s, %s, %s, %s)
                           ON CONFLICT (recipe_id, ingredient_id) DO UPDATE SET
                             quantity_from = EXCLUDED.quantity_from,
                             quantity_to = EXCLUDED.quantity_to,
                             unit = EXCLUDED.unit,
                             importance = EXCLUDED.importance;""",
                        (recipe_id, ingredient_id, ing['quantity_from'], ing['quantity_to'], 
                         ing['unit'], ing.get('importance', 'essential'))
                    )
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ ë‚´ ë ˆì‹œí”¼ ì €ì¥ ì‹¤íŒ¨: {recipe_data.get('title', 'N/A')} - {e}")
                # ê°œë³„ ë ˆì‹œí”¼ ì‹¤íŒ¨ëŠ” ì „ì²´ ë°°ì¹˜ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ
                continue
        
        # ë°°ì¹˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì»¤ë°‹
        conn.commit()
        logger.info(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}/{len(recipe_batch)}ê°œ ì„±ê³µ (íŠ¸ëœì­ì…˜ ì»¤ë°‹)")
        
    except (Exception, psycopg2.DatabaseError) as error:
        logger.error(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {error}")
        if conn:
            conn.rollback()
            logger.error("ğŸ”„ ë°°ì¹˜ íŠ¸ëœì­ì…˜ ë¡¤ë°± ì™„ë£Œ")
    finally:
        if conn:
            pool.putconn(conn)

async def insert_recipe_data(pool, recipe_data):
    """
    ë‹¨ì¼ ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— Upsertí•©ë‹ˆë‹¤.
    """
    conn = None
    try:
        conn = pool.getconn()
        cursor = conn.cursor()

        # 1. recipes í…Œì´ë¸”ì— Upsert
        cursor.execute(
            """INSERT INTO recipes (url, title, description, image_url)
               VALUES (%s, %s, %s, %s)
               ON CONFLICT (url) DO UPDATE SET
                 title = EXCLUDED.title,
                 description = EXCLUDED.description,
                 image_url = EXCLUDED.image_url
               RETURNING recipe_id;""",
            (recipe_data['url'], recipe_data['title'], recipe_data['description'], recipe_data['image_url'])
        )
        recipe_id = cursor.fetchone()[0]

        # 2. ingredients ë° recipe_ingredients í…Œì´ë¸” ì²˜ë¦¬
        for ing in recipe_data['ingredients']:
            # 2a. ingredients í…Œì´ë¸”ì— Upsert (DO NOTHING)
            cursor.execute(
                """INSERT INTO ingredients (name)
                   VALUES (%s)
                   ON CONFLICT (name) DO NOTHING
                   RETURNING ingredient_id;""",
                (ing['name'],)
            )
            result = cursor.fetchone()
            if result:
                ingredient_id = result[0]
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ì—¬ idê°€ ë°˜í™˜ë˜ì§€ ì•Šì€ ê²½ìš°, idë¥¼ ì¡°íšŒ
                cursor.execute("SELECT ingredient_id FROM ingredients WHERE name = %s;", (ing['name'],))
                ingredient_id = cursor.fetchone()[0]

            # 2b. recipe_ingredients í…Œì´ë¸”ì— Upsert
            cursor.execute(
                """INSERT INTO recipe_ingredients (recipe_id, ingredient_id, quantity_from, quantity_to, unit)
                   VALUES (%s, %s, %s, %s, %s)
                   ON CONFLICT (recipe_id, ingredient_id) DO UPDATE SET
                     quantity_from = EXCLUDED.quantity_from,
                     quantity_to = EXCLUDED.quantity_to,
                     unit = EXCLUDED.unit;""",
                (recipe_id, ingredient_id, ing['quantity_from'], ing['quantity_to'], ing['unit'])
            )
        
        conn.commit()
        logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {recipe_data['title']} (ì¬ë£Œ {len(recipe_data['ingredients'])}ê°œ)")

    except (Exception, psycopg2.DatabaseError) as error:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {recipe_data.get('title', 'N/A')} - {error}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            pool.putconn(conn)

# --- Synchronous Functions ---
def get_recipe_urls_generator(target_count):
    """
    ëª©í‘œ ìˆ˜ëŸ‰ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° ë ˆì‹œí”¼ URLì„ ìƒì„±ê¸°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ URLì„ í•œ ë²ˆì— ëª¨ë‘ ìˆ˜ì§‘í•˜ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸš€ ë ˆì‹œí”¼ URL ìˆ˜ì§‘ ì‹œì‘ - ëª©í‘œ: {target_count}ê°œ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)")
    start_time = time.time()
    collected_count = 0
    page = 1
    
    while collected_count < target_count:
        list_url = f"{BASE_URL}/recipe/list.html?page={page}"
        if not rp.can_fetch(USER_AGENT, list_url):
            logger.warning(f"âš ï¸ robots.txtì— ì˜í•´ í˜ì´ì§€ {page} ì ‘ê·¼ì´ ì°¨ë‹¨ë¨")
            break

        logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘... (í˜„ì¬ ìˆ˜ì§‘: {collected_count}/{target_count})")
        try:
            response = requests.get(list_url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            
            links_on_page = soup.select(SELECTORS["recipe_links_primary"]) or soup.select(SELECTORS["recipe_links_fallback"])
            
            if not links_on_page:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            new_links_count = 0
            for link in links_on_page:
                if collected_count >= target_count:
                    break
                    
                href = link['href']
                full_url = BASE_URL + href if not href.startswith('http') else href
                if rp.can_fetch(USER_AGENT, full_url):
                    yield full_url
                    collected_count += 1
                    new_links_count += 1
            
            logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ - ìƒˆë¡œ ìˆ˜ì§‘ëœ ë§í¬: {new_links_count}ê°œ")
            page += 1
            time.sleep(REQUEST_DELAY) # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e} - {REQUEST_DELAY * 2}ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(REQUEST_DELAY * 2)

    elapsed_time = time.time() - start_time
    logger.info(f"ğŸ¯ URL ìˆ˜ì§‘ ì™„ë£Œ - ì´ {collected_count}ê°œ ìˆ˜ì§‘ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

# --- Asynchronous Functions ---
async def scrape_recipe_details(session, url, semaphore):
    """
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°œë³„ ë ˆì‹œí”¼ í˜ì´ì§€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìŠ¤í¬ë©í•©ë‹ˆë‹¤.
    """
    async with semaphore:
        try:
            await asyncio.sleep(REQUEST_DELAY) # ì¶”ê°€ì ì¸ ìš”ì²­ ê°„ ì§€ì—°
            logger.debug(f"ğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            async with session.get(url, headers=HEADERS) as response:
                response.raise_for_status()
                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                title = soup.select_one(SELECTORS["title"]).get_text(strip=True) if soup.select_one(SELECTORS["title"]) else "ì œëª© ì—†ìŒ"
                image_url = soup.select_one(SELECTORS["image"])['src'] if soup.select_one(SELECTORS["image"]) else "ì´ë¯¸ì§€ ì—†ìŒ"
                description = soup.select_one(SELECTORS["description"]).get_text(strip=True) if soup.select_one(SELECTORS["description"]) else "ì„¤ëª… ì—†ìŒ"

                ingredients = []
                container = soup.select_one(SELECTORS["ingredients_container"])
                if container:
                    items = container.select(SELECTORS["ingredient_item"])
                    for item in items:
                        full_text = re.sub(r'\s+', ' ', item.get_text(separator=" ").replace('\n', '')).strip()
                        if full_text:
                            ingredients.append(parse_ingredient(full_text))

                logger.debug(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {title} (ì¬ë£Œ {len(ingredients)}ê°œ)")
                return {
                    "url": url,
                    "title": title,
                    "image_url": image_url,
                    "description": description,
                    "ingredients": ingredients,
                }
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url} - {e}")
            return None

async def process_chunk(session, urls_chunk, semaphore):
    """
    URL ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ”„ ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {len(urls_chunk)}ê°œ URL")
    
    # ì²­í¬ ë‚´ì—ì„œë§Œ ë¹„ë™ê¸° ì²˜ë¦¬
    tasks = [scrape_recipe_details(session, url, semaphore) for url in urls_chunk]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ê²°ê³¼ ì²˜ë¦¬ ë° ì¦‰ì‹œ DB ì €ì¥
    valid_results = []
    for result in results:
        if isinstance(result, Exception):
            logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {result}")
            continue
        if result:
            valid_results.append(result)
    
    # ì²­í¬ ê²°ê³¼ë¥¼ ì¦‰ì‹œ DBì— ì €ì¥
    if valid_results:
        logger.info(f"ğŸ’¾ ì²­í¬ ê²°ê³¼ ì €ì¥: {len(valid_results)}ê°œ")
        await insert_recipe_batch(DB_POOL, valid_results)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    import gc
    gc.collect()
    
    logger.info(f"âœ… ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {len(valid_results)}ê°œ ì„±ê³µ")
    return len(valid_results)

async def main():
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ê´„í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    URLì„ í•œ ë²ˆì— ëª¨ë‘ ìˆ˜ì§‘í•˜ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ OOMì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ ë§Œê°œì˜ ë ˆì‹œí”¼ í¬ë¡¤ëŸ¬ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)")
    logger.info(f"ğŸ“Š ì„¤ì •: ëª©í‘œ {TARGET_RECIPE_COUNT}ê°œ, ë™ì‹œ ìš”ì²­ {CONCURRENT_REQUESTS}ê°œ")
    logger.info(f"ğŸ“Š ì²­í¬ í¬ê¸°: {CHUNK_SIZE}ê°œ, ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}ê°œ")
    logger.info(f"ğŸ“Š ìš”ì²­ ì§€ì—°: {REQUEST_DELAY}ì´ˆ, ì²­í¬ ì§€ì—°: {CHUNK_DELAY}ì´ˆ")
    logger.info("=" * 60)
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
    log_memory_usage()
    
    if not DB_POOL:
        logger.error("ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    start_time = time.time()
    total_scraped = 0
    total_failed = 0
    chunk_idx = 0
    
    # URL ìƒì„±ê¸°ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    url_generator = get_recipe_urls_generator(TARGET_RECIPE_COUNT)
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    
    async with aiohttp.ClientSession() as session:
        current_chunk = []
        
        for url in url_generator:
            current_chunk.append(url)
            
            # ì²­í¬ê°€ ê°€ë“ ì°¨ë©´ ì²˜ë¦¬
            if len(current_chunk) >= CHUNK_SIZE:
                chunk_idx += 1
                logger.info(f"ğŸ”„ ì²­í¬ {chunk_idx} ì²˜ë¦¬ ì‹œì‘ ({len(current_chunk)}ê°œ URL)")
                
                # ì²­í¬ ì²˜ë¦¬
                scraped_count = await process_chunk(session, current_chunk, semaphore)
                total_scraped += scraped_count
                total_failed += len(current_chunk) - scraped_count
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = min(chunk_idx * CHUNK_SIZE, TARGET_RECIPE_COUNT)
                logger.info(f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress}/{TARGET_RECIPE_COUNT} ({progress/TARGET_RECIPE_COUNT*100:.1f}%)")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                log_memory_usage()
                
                # ì²­í¬ ê°„ ì§€ì—°
                logger.info(f"â³ ë‹¤ìŒ ì²­í¬ê¹Œì§€ {CHUNK_DELAY}ì´ˆ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œê°„)")
                await asyncio.sleep(CHUNK_DELAY)
                
                # ì²­í¬ ì´ˆê¸°í™”
                current_chunk = []
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬ (ë‚¨ì€ URLë“¤)
        if current_chunk:
            chunk_idx += 1
            logger.info(f"ğŸ”„ ë§ˆì§€ë§‰ ì²­í¬ {chunk_idx} ì²˜ë¦¬ ì‹œì‘ ({len(current_chunk)}ê°œ URL)")
            
            scraped_count = await process_chunk(session, current_chunk, semaphore)
            total_scraped += scraped_count
            total_failed += len(current_chunk) - scraped_count
            
            # ìµœì¢… ì§„í–‰ë¥  í‘œì‹œ
            logger.info(f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {TARGET_RECIPE_COUNT}/{TARGET_RECIPE_COUNT} (100.0%)")
            log_memory_usage()
    
    total_time = time.time() - start_time
    logger.info("=" * 60)
    logger.info("ğŸ‰ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
    logger.info(f"   âœ… ì„±ê³µ: {total_scraped}ê°œ")
    logger.info(f"   âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
    logger.info(f"   â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"   ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ")
    logger.info("=" * 60)

if __name__ == "__main__":
    try:
        logger.info(f"ğŸ• í¬ë¡¤ëŸ¬ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.warning("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if DB_POOL:
            DB_POOL.closeall()
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ì„ ëª¨ë‘ ë‹«ì•˜ìŠµë‹ˆë‹¤.")
        logger.info(f"ğŸ• í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")