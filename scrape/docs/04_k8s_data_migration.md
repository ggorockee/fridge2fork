# Kubernetes ÌôòÍ≤Ω Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Í∞ÄÏù¥Îìú

## Í∞úÏöî

Kubernetes ÌÅ¥Îü¨Ïä§ÌÑ∞ ÎÇ¥Ïùò PostgreSQL Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê CSV Îç∞Ïù¥ÌÑ∞Î•º ÏïàÏ†ÑÌïòÍ≤å ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖòÌïòÎäî Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïô∏Î∂ÄÏóêÏÑú ÏßÅÏ†ë Ï†ëÍ∑ºÏù¥ Î∂àÍ∞ÄÎä•Ìïú ÌôòÍ≤ΩÏóêÏÑú Docker Ïª®ÌÖåÏù¥ÎÑàÎ•º Ïù¥Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ï†ÑÎûµÏùÑ Ï†úÏãúÌï©ÎãàÎã§.

## Î¨∏Ï†ú ÏÉÅÌô©

- PostgreSQLÏù¥ Kubernetes ÌÅ¥Îü¨Ïä§ÌÑ∞ ÎÇ¥Î∂ÄÏóê Ï°¥Ïû¨
- Ïô∏Î∂ÄÏóêÏÑú ÏßÅÏ†ë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ëÍ∑º Î∂àÍ∞Ä
- ÎåÄÏö©Îüâ CSV ÌååÏùº (3Í∞ú ÌååÏùº)ÏùÑ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÏûÖÎ†• ÌïÑÏöî
- Ïû¨Î£å Ï†ïÍ∑úÌôî Îì± Î≥µÏû°Ìïú Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î°úÏßÅ ÌïÑÏöî

## Ìï¥Í≤∞ Ï†ÑÎûµ

### 1. Docker Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïª®ÌÖåÏù¥ÎÑà

**ÏïÑÌÇ§ÌÖçÏ≤ò**:
```
Î°úÏª¨ CSV ÌååÏùº ‚Üí Docker Ïª®ÌÖåÏù¥ÎÑà ‚Üí K8s PostgreSQL
```

**Ïû•Ï†ê**:
- ÎÑ§Ìä∏ÏõåÌÅ¨ Í≤©Î¶¨ ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ Í∞ÄÎä•
- ÏùºÌöåÏÑ± ÏûëÏóÖÏúºÎ°ú Ïª®ÌÖåÏù¥ÎÑà ÏûêÎèô Ï¢ÖÎ£å
- ÌôòÍ≤Ω ÎèÖÎ¶ΩÏ†Å Ïã§Ìñâ
- Î°úÍπÖ Î∞è Î™®ÎãàÌÑ∞ÎßÅ Í∞ÄÎä•

### 2. Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Î∞©Î≤ï ÎπÑÍµê

| Î∞©Î≤ï | Ïû•Ï†ê | Îã®Ï†ê | Ï∂îÏ≤úÎèÑ |
|------|------|------|--------|
| Job Ïª®ÌÖåÏù¥ÎÑà | K8s ÎÑ§Ïù¥Ìã∞Î∏å, ÏûêÎèô Ï†ïÎ¶¨ | Ïù¥ÎØ∏ÏßÄ ÎπåÎìú ÌïÑÏöî | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| InitContainer | Ïï±Í≥º Ìï®Íªò Î∞∞Ìè¨ | Ïï± Î∞∞Ìè¨ÏãúÎßàÎã§ Ïã§Ìñâ | ‚≠ê‚≠ê‚≠ê |
| ÏßÅÏ†ë Pod Ïã§Ìñâ | Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ | ÏàòÎèô Ï†ïÎ¶¨ ÌïÑÏöî | ‚≠ê‚≠ê |
| kubectl exec | Ï¶âÏÑù Ïã§Ìñâ | ÎåÄÏö©Îüâ Ï≤òÎ¶¨ Î∂ÄÏ†ÅÌï© | ‚≠ê |

## Íµ¨ÌòÑ Î∞©Ïïà

### 1. Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Docker Ïù¥ÎØ∏ÏßÄ

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

# ÏãúÏä§ÌÖú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò
COPY requirements-migration.txt .
RUN pip install --no-cache-dir -r requirements-migration.txt

# Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìú Î≥µÏÇ¨
COPY app/ /app/
COPY scripts/ /scripts/
COPY datas/ /datas/

# ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
WORKDIR /scripts

# ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
CMD ["python", "migrate_data.py"]
```

**requirements-migration.txt**:
```txt
sqlalchemy==2.0.23
asyncpg==0.29.0
psycopg2-binary==2.9.9
pandas==2.1.3
python-dotenv==1.0.0
tqdm==4.66.1
```

### 2. Kubernetes Job Îß§ÎãàÌéòÏä§Ìä∏

**k8s/data-migration-job.yaml**:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: fridge2fork-data-migration
  namespace: default
spec:
  # ÏôÑÎ£å ÌõÑ ÏûêÎèô ÏÇ≠Ï†ú (ÏÑ†ÌÉùÏ†Å)
  ttlSecondsAfterFinished: 600
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: data-migrator
        image: fridge2fork/data-migrator:latest
        env:
        - name: DATABASE_HOST
          value: "postgresql-service"  # K8s ÏÑúÎπÑÏä§Î™Ö
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_NAME
          value: "fridge2fork_db"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: password
        - name: BATCH_SIZE
          value: "1000"
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        # Î≥ºÎ•® ÎßàÏö¥Ìä∏ (Î°úÍ∑∏ ÌôïÏù∏Ïö©)
        volumeMounts:
        - name: migration-logs
          mountPath: /logs
      volumes:
      - name: migration-logs
        emptyDir: {}
```

### 3. Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïä§ÌÅ¨Î¶ΩÌä∏

**scripts/migrate_data.py** (ÌïµÏã¨ Î°úÏßÅ):
```python
#!/usr/bin/env python3
"""
K8s ÌôòÍ≤ΩÏö© Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïä§ÌÅ¨Î¶ΩÌä∏
CSV ÌååÏùºÏùÑ PostgreSQLÎ°ú Î∞∞Ïπò Ï≤òÎ¶¨
"""

import os
import asyncio
import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from tqdm import tqdm

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/logs/migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataMigrator:
    def __init__(self):
        self.database_url = self._build_database_url()
        self.engine = create_async_engine(self.database_url)
        self.session_factory = sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        self.batch_size = int(os.getenv('BATCH_SIZE', '1000'))

    def _build_database_url(self) -> str:
        """ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ URL Íµ¨ÏÑ±"""
        host = os.getenv('DATABASE_HOST', 'localhost')
        port = os.getenv('DATABASE_PORT', '5432')
        name = os.getenv('DATABASE_NAME', 'fridge2fork_db')
        user = os.getenv('DATABASE_USER', 'postgres')
        password = os.getenv('DATABASE_PASSWORD', '')

        return f"postgresql+asyncpg://{user}:{password}@{host}:{port}/{name}"

    async def run_migration(self):
        """Ï†ÑÏ≤¥ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÌîÑÎ°úÏÑ∏Ïä§ Ïã§Ìñâ"""
        try:
            logger.info("üöÄ Îç∞Ïù¥ÌÑ∞ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÏãúÏûë")

            # 1. Ïó∞Í≤∞ ÌÖåÏä§Ìä∏
            await self._test_connection()

            # 2. CSV ÌååÏùº Î™©Î°ù ÌôïÏù∏
            csv_files = self._get_csv_files()
            logger.info(f"üìÅ Î∞úÍ≤¨Îêú CSV ÌååÏùº: {len(csv_files)}Í∞ú")

            # 3. Í∞Å ÌååÏùº Ï≤òÎ¶¨
            total_processed = 0
            for csv_file in csv_files:
                processed = await self._process_csv_file(csv_file)
                total_processed += processed

            logger.info(f"‚úÖ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÏôÑÎ£å: Ï¥ù {total_processed}Í∞ú Î†àÏΩîÎìú Ï≤òÎ¶¨")

        except Exception as e:
            logger.error(f"‚ùå ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïã§Ìå®: {e}")
            raise
        finally:
            await self.engine.dispose()

    async def _test_connection(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"""
        async with self.session_factory() as session:
            result = await session.execute("SELECT version()")
            version = result.fetchone()[0]
            logger.info(f"üîó Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ: {version}")

    def _get_csv_files(self) -> List[Path]:
        """CSV ÌååÏùº Î™©Î°ù Î∞òÌôò"""
        data_dir = Path("/datas")
        csv_files = []

        # Ïù∏ÏΩîÎî©Î≥Ñ ÌååÏùº Í∑∏Î£π
        euc_kr_files = [
            "TB_RECIPE_SEARCH-20231130.csv",
            "TB_RECIPE_SEARCH-220701.csv"
        ]
        utf8_files = [
            "TB_RECIPE_SEARCH_241226.csv"
        ]

        for filename in euc_kr_files + utf8_files:
            file_path = data_dir / filename
            if file_path.exists():
                csv_files.append(file_path)
                logger.info(f"üìÑ CSV ÌååÏùº Î∞úÍ≤¨: {filename}")

        return csv_files

    async def _process_csv_file(self, csv_file: Path) -> int:
        """Í∞úÎ≥Ñ CSV ÌååÏùº Ï≤òÎ¶¨"""
        logger.info(f"üìù Ï≤òÎ¶¨ ÏãúÏûë: {csv_file.name}")

        # Ïù∏ÏΩîÎî© Í∞êÏßÄ
        encoding = 'euc-kr' if 'TB_RECIPE_SEARCH-' in csv_file.name else 'utf-8'

        try:
            # Ï≤≠ÌÅ¨ Îã®ÏúÑÎ°ú CSV ÏùΩÍ∏∞
            chunk_iterator = pd.read_csv(
                csv_file,
                encoding=encoding,
                chunksize=self.batch_size,
                dtype=str,  # Î™®Îì† Ïª¨ÎüºÏùÑ Î¨∏ÏûêÏó¥Î°ú ÏùΩÍ∏∞
                keep_default_na=False
            )

            total_rows = 0
            for chunk_num, chunk in enumerate(chunk_iterator):
                logger.info(f"üì¶ Ï≤≠ÌÅ¨ {chunk_num + 1} Ï≤òÎ¶¨ Ï§ë ({len(chunk)}Í∞ú Ìñâ)")

                # Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú Î∞è Î≥ÄÌôò
                processed_data = await self._process_chunk(chunk)

                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
                await self._save_to_database(processed_data)

                total_rows += len(chunk)
                logger.info(f"‚úÖ Ï≤≠ÌÅ¨ {chunk_num + 1} ÏôÑÎ£å (ÎàÑÏ†Å: {total_rows}Í∞ú)")

            logger.info(f"üéâ {csv_file.name} Ï≤òÎ¶¨ ÏôÑÎ£å: {total_rows}Í∞ú Ìñâ")
            return total_rows

        except Exception as e:
            logger.error(f"‚ùå {csv_file.name} Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            raise

    async def _process_chunk(self, chunk: pd.DataFrame) -> List[Dict[str, Any]]:
        """Îç∞Ïù¥ÌÑ∞ Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ Î∞è Ï†ïÍ∑úÌôî"""
        processed_recipes = []

        for _, row in chunk.iterrows():
            try:
                # Î†àÏãúÌîº Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò
                recipe_data = {
                    'rcp_sno': row.get('RCP_SNO', ''),
                    'title': row.get('RCP_TTL', ''),
                    'cooking_name': row.get('CKG_NM', ''),
                    'registrant_id': row.get('RGTR_ID', ''),
                    'registrant_name': row.get('RGTR_NM', ''),
                    'inquiry_count': self._safe_int(row.get('INQ_CNT', 0)),
                    'recommendation_count': self._safe_int(row.get('RCMM_CNT', 0)),
                    'scrap_count': self._safe_int(row.get('SRAP_CNT', 0)),
                    'cooking_method': row.get('CKG_MTH_ACTO_NM', ''),
                    'cooking_situation': row.get('CKG_STA_ACTO_NM', ''),
                    'cooking_material_category': row.get('CKG_MTRL_ACTO_NM', ''),
                    'cooking_kind': row.get('CKG_KND_ACTO_NM', ''),
                    'introduction': row.get('CKG_IPDC', ''),
                    'raw_ingredients': row.get('CKG_MTRL_CN', ''),
                    'serving_size': row.get('CKG_INBUN_NM', ''),
                    'difficulty': row.get('CKG_DODF_NM', ''),
                    'cooking_time': row.get('CKG_TIME_NM', ''),
                    'image_url': row.get('RCP_IMG_URL', ''),
                    'registered_at': self._parse_datetime(row.get('FIRST_REG_DT', ''))
                }

                # Ïû¨Î£å ÌååÏã± (ÏÉÅÏÑ∏ Íµ¨ÌòÑÏùÄ Î≥ÑÎèÑ Î™®ÎìàÎ°ú)
                recipe_data['ingredients'] = await self._parse_ingredients(
                    row.get('CKG_MTRL_CN', '')
                )

                processed_recipes.append(recipe_data)

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Ìñâ Ï≤òÎ¶¨ Ïã§Ìå® (RCP_SNO: {row.get('RCP_SNO', 'unknown')}): {e}")
                continue

        return processed_recipes

    def _safe_int(self, value: Any) -> int:
        """ÏïàÏ†ÑÌïú Ï†ïÏàò Î≥ÄÌôò"""
        try:
            return int(value) if value and str(value).strip() else 0
        except (ValueError, TypeError):
            return 0

    def _parse_datetime(self, dt_str: str) -> str:
        """ÎÇ†ÏßúÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ ÌååÏã±"""
        if not dt_str or len(dt_str) < 14:
            return None
        try:
            # YYYYMMDDHHMMSS ÌòïÏãùÏùÑ YYYY-MM-DD HH:MM:SSÎ°ú Î≥ÄÌôò
            return f"{dt_str[:4]}-{dt_str[4:6]}-{dt_str[6:8]} {dt_str[8:10]}:{dt_str[10:12]}:{dt_str[12:14]}"
        except:
            return None

    async def _parse_ingredients(self, ingredients_str: str) -> List[Dict[str, Any]]:
        """Ïû¨Î£å Î¨∏ÏûêÏó¥ ÌååÏã±"""
        # Í∞ÑÎã®Ìïú ÌååÏã± Íµ¨ÌòÑ (Ïã§Ï†úÎ°úÎäî Îçî Î≥µÏû°Ìïú Î°úÏßÅ ÌïÑÏöî)
        if not ingredients_str:
            return []

        # "[Ïû¨Î£å]" Ï†úÍ±∞ ÌõÑ "|"Î°ú Î∂ÑÎ¶¨
        clean_str = ingredients_str.replace('[Ïû¨Î£å]', '').strip()
        ingredients = [ing.strip() for ing in clean_str.split('|') if ing.strip()]

        parsed_ingredients = []
        for ing in ingredients:
            # Í∏∞Î≥∏Ï†ÅÏù∏ ÌååÏã± (Ïã§Ï†úÎ°úÎäî Ï†ïÍ∑úÏãù ÏÇ¨Ïö©)
            parsed_ingredients.append({
                'raw_text': ing,
                'name': ing.split()[0] if ing.split() else ing,
                'quantity_text': ing
            })

        return parsed_ingredients

    async def _save_to_database(self, recipes_data: List[Dict[str, Any]]):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Î∞∞Ïπò Ï†ÄÏû•"""
        async with self.session_factory() as session:
            try:
                # Ïã§Ï†ú Íµ¨ÌòÑÏãúÏóêÎäî SQLAlchemy Î™®Îç∏ ÏÇ¨Ïö©
                # Ïó¨Í∏∞ÏÑúÎäî Í∞ÑÎã®Ìïú ÏòàÏãúÎßå Ï†úÍ≥µ

                for recipe_data in recipes_data:
                    # recipes ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•
                    # ingredients Ï†ïÍ∑úÌôî Î∞è Ï†ÄÏû•
                    # recipe_ingredients Í¥ÄÍ≥Ñ Ï†ÄÏû•
                    pass

                await session.commit()
                logger.debug(f"üíæ {len(recipes_data)}Í∞ú Î†àÏãúÌîº Ï†ÄÏû• ÏôÑÎ£å")

            except Exception as e:
                await session.rollback()
                logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ïã§Ìå®: {e}")
                raise

async def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    migrator = DataMigrator()
    await migrator.run_migration()

if __name__ == "__main__":
    asyncio.run(main())
```

## Ïã§Ìñâ Î∞©Î≤ï

### 1. Docker Ïù¥ÎØ∏ÏßÄ ÎπåÎìú
```bash
# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ÏóêÏÑú Ïã§Ìñâ
docker build -t fridge2fork/data-migrator:latest -f docker/Dockerfile.migration .

# Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú (Ïª®ÌÖåÏù¥ÎÑà Î†àÏßÄÏä§Ìä∏Î¶¨Ïóê)
docker push fridge2fork/data-migrator:latest
```

### 2. Kubernetes ÏãúÌÅ¨Î¶ø ÏÉùÏÑ±
```bash
# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ëÏÜç Ï†ïÎ≥¥
kubectl create secret generic postgresql-secret \
  --from-literal=username=fridge2fork_user \
  --from-literal=password=your_password
```

### 3. Job Ïã§Ìñâ
```bash
# Job ÏÉùÏÑ± Î∞è Ïã§Ìñâ
kubectl apply -f k8s/data-migration-job.yaml

# Ïã§Ìñâ ÏÉÅÌÉú ÌôïÏù∏
kubectl get jobs
kubectl get pods -l job-name=fridge2fork-data-migration

# Î°úÍ∑∏ ÌôïÏù∏
kubectl logs -l job-name=fridge2fork-data-migration -f
```

### 4. ÏßÑÌñâ ÏÉÅÌô© Î™®ÎãàÌÑ∞ÎßÅ
```bash
# Job ÏÉÅÌÉú ÌôïÏù∏
kubectl describe job fridge2fork-data-migration

# Pod Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Îüâ ÌôïÏù∏
kubectl top pod -l job-name=fridge2fork-data-migration

# ÏôÑÎ£å ÌõÑ Ï†ïÎ¶¨ (ttlSecondsAfterFinished ÏÑ§Ï†ïÏãú ÏûêÎèô)
kubectl delete job fridge2fork-data-migration
```

## Î™®ÎãàÌÑ∞ÎßÅ Î∞è Ìä∏Îü¨Î∏îÏäàÌåÖ

### ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ï†úÎì§

1. **Î©îÎ™®Î¶¨ Î∂ÄÏ°±**:
   - JobÏùò memory limits Ï¶ùÍ∞Ä
   - Î∞∞Ïπò ÌÅ¨Í∏∞ (BATCH_SIZE) Í∞êÏÜå

2. **Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®**:
   - ÏÑúÎπÑÏä§Î™Ö Î∞è Ìè¨Ìä∏ ÌôïÏù∏
   - ÏãúÌÅ¨Î¶ø Ï†ïÎ≥¥ Í≤ÄÏ¶ù
   - ÎÑ§Ìä∏ÏõåÌÅ¨ Ï†ïÏ±Ö ÌôïÏù∏

3. **CSV Ïù∏ÏΩîÎî© Î¨∏Ï†ú**:
   - ÌååÏùºÎ≥Ñ Ïù∏ÏΩîÎî© ÏÑ§Ï†ï ÌôïÏù∏
   - ÌïúÍ∏Ä Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Í≤ÄÏ¶ù

4. **Job Ïã§Ìñâ Ïã§Ìå®**:
   - Ïù¥ÎØ∏ÏßÄ pull Í∂åÌïú ÌôïÏù∏
   - Î¶¨ÏÜåÏä§ Ìï†ÎãπÎüâ Í≤ÄÌÜ†
   - Î≥ºÎ•® ÎßàÏö¥Ìä∏ ÏÉÅÌÉú ÌôïÏù∏

### ÏÑ±Îä• ÌäúÎãù

1. **Î≥ëÎ†¨ Ï≤òÎ¶¨**: Ïó¨Îü¨ JobÏùÑ ÌååÏùºÎ≥ÑÎ°ú Î∂ÑÌï† Ïã§Ìñâ
2. **Î∞∞Ïπò ÌÅ¨Í∏∞ Ï°∞Ï†ï**: Î©îÎ™®Î¶¨ÏôÄ Ï≤òÎ¶¨ ÏÜçÎèÑ ÏµúÏ†ÅÌôî
3. **Î¶¨ÏÜåÏä§ Ìï†Îãπ**: CPU/Memory ÏöîÏ≤≠Îüâ ÏµúÏ†ÅÌôî
4. **Ïù∏Îç±Ïä§ ÎπÑÌôúÏÑ±Ìôî**: ÎåÄÎüâ ÏÇΩÏûÖÏãú ÏûÑÏãú ÎπÑÌôúÏÑ±Ìôî

Ïù¥ Î∞©Î≤ïÏùÑ ÌÜµÌï¥ Kubernetes ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÏïàÏ†ÑÌïòÍ≥† Ìö®Ïú®Ï†ÅÏúºÎ°ú CSV Îç∞Ïù¥ÌÑ∞Î•º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖòÌï† Ïàò ÏûàÏäµÎãàÎã§.