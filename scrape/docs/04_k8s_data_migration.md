# Kubernetes í™˜ê²½ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

## ê°œìš”

Kubernetes í´ëŸ¬ìŠ¤í„° ë‚´ì˜ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— CSV ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ì—ì„œ Docker ì»¨í…Œì´ë„ˆë¥¼ ì´ìš©í•œ ë°ì´í„° ë¡œë”© ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.

## ë¬¸ì œ ìƒí™©

- PostgreSQLì´ Kubernetes í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì— ì¡´ì¬
- ì™¸ë¶€ì—ì„œ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ë¶ˆê°€
- ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ (3ê°œ íŒŒì¼)ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì…ë ¥ í•„ìš”
- ì¬ë£Œ ì •ê·œí™” ë“± ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ í•„ìš”

## í•´ê²° ì „ëµ

### 1. Docker ê¸°ë°˜ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì»¨í…Œì´ë„ˆ

**ì•„í‚¤í…ì²˜**:
```
ë¡œì»¬ CSV íŒŒì¼ â†’ Docker ì»¨í…Œì´ë„ˆ â†’ K8s PostgreSQL
```

**ì¥ì **:
- ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- ì¼íšŒì„± ì‘ì—…ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ ìë™ ì¢…ë£Œ
- í™˜ê²½ ë…ë¦½ì  ì‹¤í–‰
- ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥

### 2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë°©ë²• ë¹„êµ

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì¶”ì²œë„ |
|------|------|------|--------|
| Job ì»¨í…Œì´ë„ˆ | K8s ë„¤ì´í‹°ë¸Œ, ìë™ ì •ë¦¬ | ì´ë¯¸ì§€ ë¹Œë“œ í•„ìš” | â­â­â­â­â­ |
| InitContainer | ì•±ê³¼ í•¨ê»˜ ë°°í¬ | ì•± ë°°í¬ì‹œë§ˆë‹¤ ì‹¤í–‰ | â­â­â­ |
| ì§ì ‘ Pod ì‹¤í–‰ | ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ | ìˆ˜ë™ ì •ë¦¬ í•„ìš” | â­â­ |
| kubectl exec | ì¦‰ì„ ì‹¤í–‰ | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ë¶€ì í•© | â­ |

## êµ¬í˜„ ë°©ì•ˆ

### 1. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ Docker ì´ë¯¸ì§€

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements-migration.txt .
RUN pip install --no-cache-dir -r requirements-migration.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app/ /app/
COPY scripts/ /scripts/
COPY datas/ /datas/

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /scripts

# ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
CMD ["python", "migrate_data.py"]
```

**requirements-migration.txt**:
```txt
sqlalchemy==2.0.23
asyncpg==0.29.0
psycopg2-binary==2.9.9
pandas==2.1.3
python-dotenv==1.0.0
tqdm==4.66.1
```

### 2. Kubernetes Job ë§¤ë‹ˆí˜ìŠ¤íŠ¸

**k8s/data-migration-job.yaml**:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: fridge2fork-data-migration
  namespace: default
spec:
  # ì™„ë£Œ í›„ ìë™ ì‚­ì œ (ì„ íƒì )
  ttlSecondsAfterFinished: 600
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: data-migrator
        image: fridge2fork/data-migrator:latest
        env:
        - name: DATABASE_HOST
          value: "postgresql-service"  # K8s ì„œë¹„ìŠ¤ëª…
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_NAME
          value: "fridge2fork_db"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-secret
              key: password
        - name: BATCH_SIZE
          value: "1000"
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        # ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ë¡œê·¸ í™•ì¸ìš©)
        volumeMounts:
        - name: migration-logs
          mountPath: /logs
      volumes:
      - name: migration-logs
        emptyDir: {}
```

### 3. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

**scripts/migrate_data.py** (í•µì‹¬ ë¡œì§):
```python
#!/usr/bin/env python3
"""
K8s í™˜ê²½ìš© ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
CSV íŒŒì¼ì„ PostgreSQLë¡œ ë°°ì¹˜ ì²˜ë¦¬
"""

import os
import asyncio
import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/logs/migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataMigrator:
    def __init__(self):
        self.database_url = self._build_database_url()
        self.engine = create_async_engine(self.database_url)
        self.session_factory = sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        self.batch_size = int(os.getenv('BATCH_SIZE', '1000'))

    def _build_database_url(self) -> str:
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ URL êµ¬ì„±"""
        host = os.getenv('DATABASE_HOST', 'localhost')
        port = os.getenv('DATABASE_PORT', '5432')
        name = os.getenv('DATABASE_NAME', 'fridge2fork_db')
        user = os.getenv('DATABASE_USER', 'postgres')
        password = os.getenv('DATABASE_PASSWORD', '')

        return f"postgresql+asyncpg://{user}:{password}@{host}:{port}/{name}"

    async def run_migration(self):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")

            # 1. ì—°ê²° í…ŒìŠ¤íŠ¸
            await self._test_connection()

            # 2. CSV íŒŒì¼ ëª©ë¡ í™•ì¸
            csv_files = self._get_csv_files()
            logger.info(f"ğŸ“ ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")

            # 3. ê° íŒŒì¼ ì²˜ë¦¬
            total_processed = 0
            for csv_file in csv_files:
                processed = await self._process_csv_file(csv_file)
                total_processed += processed

            logger.info(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ì´ {total_processed}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬")

        except Exception as e:
            logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            raise
        finally:
            await self.engine.dispose()

    async def _test_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        async with self.session_factory() as session:
            result = await session.execute("SELECT version()")
            version = result.fetchone()[0]
            logger.info(f"ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ: {version}")

    def _get_csv_files(self) -> List[Path]:
        """CSV íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        data_dir = Path("/datas")
        csv_files = []

        # ì¸ì½”ë”©ë³„ íŒŒì¼ ê·¸ë£¹
        euc_kr_files = [
            "TB_RECIPE_SEARCH-20231130.csv",
            "TB_RECIPE_SEARCH-220701.csv"
        ]
        utf8_files = [
            "TB_RECIPE_SEARCH_241226.csv"
        ]

        for filename in euc_kr_files + utf8_files:
            file_path = data_dir / filename
            if file_path.exists():
                csv_files.append(file_path)
                logger.info(f"ğŸ“„ CSV íŒŒì¼ ë°œê²¬: {filename}")

        return csv_files

    async def _process_csv_file(self, csv_file: Path) -> int:
        """ê°œë³„ CSV íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ“ ì²˜ë¦¬ ì‹œì‘: {csv_file.name}")

        # ì¸ì½”ë”© ê°ì§€
        encoding = 'euc-kr' if 'TB_RECIPE_SEARCH-' in csv_file.name else 'utf-8'

        try:
            # ì²­í¬ ë‹¨ìœ„ë¡œ CSV ì½ê¸°
            chunk_iterator = pd.read_csv(
                csv_file,
                encoding=encoding,
                chunksize=self.batch_size,
                dtype=str,  # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ì½ê¸°
                keep_default_na=False
            )

            total_rows = 0
            for chunk_num, chunk in enumerate(chunk_iterator):
                logger.info(f"ğŸ“¦ ì²­í¬ {chunk_num + 1} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ í–‰)")

                # ë°ì´í„° ì •ì œ ë° ë³€í™˜
                processed_data = await self._process_chunk(chunk)

                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                await self._save_to_database(processed_data)

                total_rows += len(chunk)
                logger.info(f"âœ… ì²­í¬ {chunk_num + 1} ì™„ë£Œ (ëˆ„ì : {total_rows}ê°œ)")

            logger.info(f"ğŸ‰ {csv_file.name} ì²˜ë¦¬ ì™„ë£Œ: {total_rows}ê°œ í–‰")
            return total_rows

        except Exception as e:
            logger.error(f"âŒ {csv_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    async def _process_chunk(self, chunk: pd.DataFrame) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì²­í¬ ì²˜ë¦¬ ë° ì •ê·œí™”"""
        processed_recipes = []

        for _, row in chunk.iterrows():
            try:
                # ë ˆì‹œí”¼ ë°ì´í„° ë³€í™˜
                recipe_data = {
                    'rcp_sno': row.get('RCP_SNO', ''),
                    'title': row.get('RCP_TTL', ''),
                    'cooking_name': row.get('CKG_NM', ''),
                    'registrant_id': row.get('RGTR_ID', ''),
                    'registrant_name': row.get('RGTR_NM', ''),
                    'inquiry_count': self._safe_int(row.get('INQ_CNT', 0)),
                    'recommendation_count': self._safe_int(row.get('RCMM_CNT', 0)),
                    'scrap_count': self._safe_int(row.get('SRAP_CNT', 0)),
                    'cooking_method': row.get('CKG_MTH_ACTO_NM', ''),
                    'cooking_situation': row.get('CKG_STA_ACTO_NM', ''),
                    'cooking_material_category': row.get('CKG_MTRL_ACTO_NM', ''),
                    'cooking_kind': row.get('CKG_KND_ACTO_NM', ''),
                    'introduction': row.get('CKG_IPDC', ''),
                    'raw_ingredients': row.get('CKG_MTRL_CN', ''),
                    'serving_size': row.get('CKG_INBUN_NM', ''),
                    'difficulty': row.get('CKG_DODF_NM', ''),
                    'cooking_time': row.get('CKG_TIME_NM', ''),
                    'image_url': row.get('RCP_IMG_URL', ''),
                    'registered_at': self._parse_datetime(row.get('FIRST_REG_DT', ''))
                }

                # ì¬ë£Œ íŒŒì‹± (ìƒì„¸ êµ¬í˜„ì€ ë³„ë„ ëª¨ë“ˆë¡œ)
                recipe_data['ingredients'] = await self._parse_ingredients(
                    row.get('CKG_MTRL_CN', '')
                )

                processed_recipes.append(recipe_data)

            except Exception as e:
                logger.warning(f"âš ï¸ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨ (RCP_SNO: {row.get('RCP_SNO', 'unknown')}): {e}")
                continue

        return processed_recipes

    def _safe_int(self, value: Any) -> int:
        """ì•ˆì „í•œ ì •ìˆ˜ ë³€í™˜"""
        try:
            return int(value) if value and str(value).strip() else 0
        except (ValueError, TypeError):
            return 0

    def _parse_datetime(self, dt_str: str) -> str:
        """ë‚ ì§œì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        if not dt_str or len(dt_str) < 14:
            return None
        try:
            # YYYYMMDDHHMMSS í˜•ì‹ì„ YYYY-MM-DD HH:MM:SSë¡œ ë³€í™˜
            return f"{dt_str[:4]}-{dt_str[4:6]}-{dt_str[6:8]} {dt_str[8:10]}:{dt_str[10:12]}:{dt_str[12:14]}"
        except:
            return None

    async def _parse_ingredients(self, ingredients_str: str) -> List[Dict[str, Any]]:
        """ì¬ë£Œ ë¬¸ìì—´ íŒŒì‹±"""
        # ê°„ë‹¨í•œ íŒŒì‹± êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        if not ingredients_str:
            return []

        # "[ì¬ë£Œ]" ì œê±° í›„ "|"ë¡œ ë¶„ë¦¬
        clean_str = ingredients_str.replace('[ì¬ë£Œ]', '').strip()
        ingredients = [ing.strip() for ing in clean_str.split('|') if ing.strip()]

        parsed_ingredients = []
        for ing in ingredients:
            # ê¸°ë³¸ì ì¸ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ì •ê·œì‹ ì‚¬ìš©)
            parsed_ingredients.append({
                'raw_text': ing,
                'name': ing.split()[0] if ing.split() else ing,
                'quantity_text': ing
            })

        return parsed_ingredients

    async def _save_to_database(self, recipes_data: List[Dict[str, Any]]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°°ì¹˜ ì €ì¥"""
        async with self.session_factory() as session:
            try:
                # ì‹¤ì œ êµ¬í˜„ì‹œì—ëŠ” SQLAlchemy ëª¨ë¸ ì‚¬ìš©
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ

                for recipe_data in recipes_data:
                    # recipes í…Œì´ë¸”ì— ì €ì¥
                    # ingredients ì •ê·œí™” ë° ì €ì¥
                    # recipe_ingredients ê´€ê³„ ì €ì¥
                    pass

                await session.commit()
                logger.debug(f"ğŸ’¾ {len(recipes_data)}ê°œ ë ˆì‹œí”¼ ì €ì¥ ì™„ë£Œ")

            except Exception as e:
                await session.rollback()
                logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                raise

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    migrator = DataMigrator()
    await migrator.run_migration()

if __name__ == "__main__":
    asyncio.run(main())
```

## ì‹¤í–‰ ë°©ë²•

### 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
docker build -t fridge2fork/data-migrator:latest -f docker/Dockerfile.migration .

# ì´ë¯¸ì§€ í‘¸ì‹œ (ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—)
docker push fridge2fork/data-migrator:latest
```

### 2. Kubernetes ì‹œí¬ë¦¿ ìƒì„±
```bash
# ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì† ì •ë³´
kubectl create secret generic postgresql-secret \
  --from-literal=username=fridge2fork_user \
  --from-literal=password=your_password
```

### 3. Job ì‹¤í–‰
```bash
# Job ìƒì„± ë° ì‹¤í–‰
kubectl apply -f k8s/data-migration-job.yaml

# ì‹¤í–‰ ìƒíƒœ í™•ì¸
kubectl get jobs
kubectl get pods -l job-name=fridge2fork-data-migration

# ë¡œê·¸ í™•ì¸
kubectl logs -l job-name=fridge2fork-data-migration -f
```

### 4. ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
```bash
# Job ìƒíƒœ í™•ì¸
kubectl describe job fridge2fork-data-migration

# Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
kubectl top pod -l job-name=fridge2fork-data-migration

# ì™„ë£Œ í›„ ì •ë¦¬ (ttlSecondsAfterFinished ì„¤ì •ì‹œ ìë™)
kubectl delete job fridge2fork-data-migration
```

## ëª¨ë‹ˆí„°ë§ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **ë©”ëª¨ë¦¬ ë¶€ì¡±**:
   - Jobì˜ memory limits ì¦ê°€
   - ë°°ì¹˜ í¬ê¸° (BATCH_SIZE) ê°ì†Œ

2. **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨**:
   - ì„œë¹„ìŠ¤ëª… ë° í¬íŠ¸ í™•ì¸
   - ì‹œí¬ë¦¿ ì •ë³´ ê²€ì¦
   - ë„¤íŠ¸ì›Œí¬ ì •ì±… í™•ì¸

3. **CSV ì¸ì½”ë”© ë¬¸ì œ**:
   - íŒŒì¼ë³„ ì¸ì½”ë”© ì„¤ì • í™•ì¸
   - í•œê¸€ ë°ì´í„° ì²˜ë¦¬ ê²€ì¦

4. **Job ì‹¤í–‰ ì‹¤íŒ¨**:
   - ì´ë¯¸ì§€ pull ê¶Œí•œ í™•ì¸
   - ë¦¬ì†ŒìŠ¤ í• ë‹¹ëŸ‰ ê²€í† 
   - ë³¼ë¥¨ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸

### ì„±ëŠ¥ íŠœë‹

1. **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ Jobì„ íŒŒì¼ë³„ë¡œ ë¶„í•  ì‹¤í–‰
2. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: ë©”ëª¨ë¦¬ì™€ ì²˜ë¦¬ ì†ë„ ìµœì í™”
3. **ë¦¬ì†ŒìŠ¤ í• ë‹¹**: CPU/Memory ìš”ì²­ëŸ‰ ìµœì í™”
4. **ì¸ë±ìŠ¤ ë¹„í™œì„±í™”**: ëŒ€ëŸ‰ ì‚½ì…ì‹œ ì„ì‹œ ë¹„í™œì„±í™”

ì´ ë°©ë²•ì„ í†µí•´ Kubernetes í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ CSV ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ë§ˆì´ê·¸ë ˆì´ì…˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.