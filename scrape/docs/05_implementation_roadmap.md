# Fridge2Fork 프로젝트 구현 로드맵

## 프로젝트 개요

**목표**: 냉장고 재료 기반 레시피 추천 시스템 구축
**기술 스택**: FastAPI + PostgreSQL + Kubernetes
**데이터**: 한국 레시피 CSV 3개 파일 (약 20만+ 레코드)
**API 프리픽스**: `/v1`

## 전체 Phase 요약

| Phase | 목표 | 예상 소요시간 | 전제조건 |
|-------|------|---------------|----------|
| **Phase 1** | 환경 준비 및 설정 | 2-3시간 | 기본 시스템 환경 |
| **Phase 2** | 데이터베이스 스키마 구축 | 1-2시간 | Phase 1 완료 |
| **Phase 3** | 로컬 데이터 마이그레이션 | 3-4시간 | Phase 2 완료 |
| **Phase 4** | Kubernetes 환경 배포 | 2-3시간 | Phase 3 완료 + K8s 클러스터 |

## 전체 진행상황 추적표

### 📋 전체 완료도 체크리스트
- [x] **Phase 1**: 개발 환경 설정 완료
- [x] **Phase 2**: 데이터베이스 스키마 구축 완료
- [x] **Phase 3**: 로컬 데이터 마이그레이션 완료 (코드 작성 완료, 실행 대기)
- [x] **Phase 4**: Kubernetes 환경 배포 완료 (코드 작성 완료, 배포 대기)
- [ ] **검증**: 전체 시스템 통합 테스트 완료

---

## Phase 1: 환경 준비 및 설정

### 🎯 목표
개발에 필요한 Conda 환경, 패키지, PostgreSQL 및 프로젝트 구조를 설정합니다.

### 📋 Phase 1 체크리스트

#### 1.1 Conda 환경 설정
- [x] **Conda 환경 생성**: Python 3.12.11 환경을 이름 `fridge2fork`로 생성
- [x] **환경 활성화**: 생성된 conda 환경 활성화
- [x] **Python 버전 검증**: Python 3.12.11 버전 및 환경 목록 확인

#### 1.2 패키지 설치
비동기 처리를 통해 CSV 데이터 마이그레이션을 효율적으로 수행할 수 있습니다.
- [x] **requirements-api.txt 파일 생성**: 다음 주요 패키지들을 포함하여 생성
  - FastAPI 0.104.1, SQLAlchemy 2.0.23, Alembic 1.12.1
  - PostgreSQL 드라이버: asyncpg, psycopg2-binary
  - 데이터 처리: pandas, numpy
  - 테스트 및 개발 도구: pytest, black, isort, flake8, mypy
  - 유틸리티: click, rich, tqdm, konlpy, hanja
- [x] **패키지 설치 실행**: pip install -r requirements-api.txt 실행
- [x] **설치 검증**: 주요 패키지 import 테스트 수행


#### 1.4 환경변수 설정
환경변수 키를 .env 파일에 작성하면 개발자가 실제 값을 채워넣어 환경을 구성합니다.
- [x] **.env 파일 생성**: 다음 설정을 포함한 환경변수 파일 생성
  - 데이터베이스 연결 정보: URL, 호스트, 포트, 이름, 사용자, 비밀번호
  - 애플리케이션 설정: 이름, 버전, 설명, 디버그 모드
  - API 설정: v1 프리픽스, 시크릿 키
  - 데이터 설정: CSV 디렉토리, 배치 크기, 최대 워커 수
  - 로깅 설정: 로그 레벨, 포맷
- [x] **환경변수 로드 테스트**: dotenv로 환경변수 로드 및 DATABASE_URL 확인

#### 1.5 프로젝트 구조 생성
- [x] **디렉토리 구조 생성**: app, api, core, crud, db, models, schemas, services, utils, migrations, scripts, tests, docs, datas 디렉토리 생성
- [x] **__init__.py 파일 생성**: 모든 Python 패키지 디렉토리에 빈 `__init__.py` 파일 생성
- [x] **프로젝트 구조 확인**: tree 명령어로 전체 디렉토리 구조 확인

#### 1.6 개발 도구 설정
- [x] **.gitignore 파일 생성**: Python, 환경변수, 데이터베이스, IDE, 로그, 테스트 관련 파일들을 포함한 .gitignore 파일 생성

### ✅ Phase 1 완료 검증

다음 검증 작업들이 모두 성공해야 Phase 1이 완료됩니다:

1. **Python 환경 확인**: Python 3.11 버전 확인
2. **주요 패키지 import 테스트**: fastapi, sqlalchemy, pandas, asyncpg 패키지 import 성공
4. **프로젝트 구조 확인**: 생성된 모든 디렉토리 존재 확인

### 🚨 Phase 1 트러블슈팅

**문제 1: conda 환경 활성화 실패**
- `conda init` 실행 후 터미널 재시작 후 다시 시도

**문제 3: 패키지 설치 실패**
- pip 업그레이드 후 conda-forge 채널 사용 고려

---

## Phase 2: 데이터베이스 스키마 구축

### 🎯 목표
기존에 구현된 데이터베이스 스키마를 기반으로 Alembic을 사용하여 데이터베이스를 구성하고 전문검색 기능을 설정합니다.

### 🚀 개발 접근 방식
**코드 우선 개발**: 로컬 데이터베이스 설치 없이 코드를 먼저 작성하고, 실제 배포 환경에서 테스트 및 검증하는 방식으로 진행합니다.

#### 개발 프로세스
1. **코드 작성**: Alembic 설정, 모델 정의, 마이그레이션 스크립트 작성
2. **배포 테스트**: 실제 PostgreSQL 환경에서 코드 실행 및 검증
3. **피드백 반영**: 실행 결과를 바탕으로 코드 수정 및 개선
4. **반복 개선**: 배포 → 테스트 → 수정 사이클 반복

### 📋 Phase 2 체크리스트

#### 2.1 Alembic 초기화
- [x] **Alembic 초기화**: `alembic init migrations` 명령어로 마이그레이션 환경 초기화
- [x] **alembic.ini 설정 수정**: sqlalchemy.url 라인을 환경변수에서 읽도록 수정
- [x] **migrations/env.py 설정**: 환경변수 로드, Base 메타데이터 설정, 온라인/오프라인 마이그레이션 함수 구현

#### 2.2 데이터베이스 모델 정의
- [x] **기본 클래스 생성 (app/db/base_class.py)**: SQLAlchemy declarative base 클래스 생성
- [x] **레시피 모델 생성 (app/models/recipe.py)**: 4개 주요 모델 정의
  - Recipe: 레시피 기본 정보 (제목, 등록자, 조회수, 요리방법, 이미지 등)
  - IngredientCategory: 재료 카테고리 (육류, 해산물, 채소류 등)
  - Ingredient: 재료 정보 (이름, 정규화명, 카테고리, 모호성 여부)
  - RecipeIngredient: 레시피-재료 연결 (수량, 단위, 필수여부)
- [x] **모델 초기화 파일**: `app/models/__init__.py`에서 모든 모델 export
- [x] **데이터베이스 base 파일**: `app/db/base.py`에서 Alembic 자동생성을 위한 모델 import

#### 2.3 첫 번째 마이그레이션 생성 및 실행
- [x] **첫 번째 마이그레이션 생성**: `scripts/create_initial_migration.py` 스크립트로 테이블 생성 마이그레이션 생성
- [x] **마이그레이션 실행**: `scripts/run_migration.py` 스크립트로 데이터베이스에 테이블 생성
- [x] **테이블 생성 확인**: 스크립트 내에서 마이그레이션 상태 및 히스토리 확인

#### 2.4 기본 데이터 삽입
- [x] **기본 카테고리 데이터 삽입**: `scripts/insert_basic_data.py` 스크립트 생성 및 실행
  - 8개 기본 카테고리 생성: 육류, 해산물, 채소류, 양념류, 곡류, 유제품, 가공식품, 조미료
  - 비동기 SQLAlchemy 세션 사용
  - 트랜잭션 커밋 및 완료 메시지 출력

#### 2.5 전문검색 설정
- [x] **전문검색 마이그레이션 생성**: `scripts/create_fulltext_migration.py` 스크립트로 인덱스 생성 마이그레이션 생성
- [x] **전문검색 마이그레이션 파일 수정**:
  - PostgreSQL pg_trgm 확장 설치
  - 레시피 제목/요리명에 한국어 전문검색 GIN 인덱스 생성
  - 재료명에 전문검색 및 트라이그램 인덱스 생성
  - 성능 최적화를 위한 복합 인덱스 생성
- [x] **전문검색 마이그레이션 실행**: `scripts/run_migration.py` 스크립트로 인덱스 생성

#### 2.6 종합 실행 스크립트
- [x] **전체 Phase 2 실행**: `scripts/setup_database.py` 스크립트 생성
  - 모든 Phase 2 단계를 순차적으로 실행
  - 각 단계별 성공/실패 상태 추적
  - 상세한 실행 결과 및 로그 출력

### ✅ Phase 2 완료 검증

다음 검증 작업들이 모두 성공해야 Phase 2가 완료됩니다:

1. **테이블 생성 확인**: recipes, ingredients, recipe_ingredients, ingredient_categories 테이블 존재 확인
2. **인덱스 생성 확인**: 전문검색(fts) 및 트라이그램(trgm) 인덱스 생성 확인
3. **기본 데이터 확인**: ingredient_categories 테이블에 8개 카테고리 데이터 확인
4. **마이그레이션 상태 확인**: alembic current 및 history 명령어로 마이그레이션 상태 확인

### 🚨 Phase 2 트러블슈팅

**문제 1: Alembic autogenerate 실패**
- Base 모델 import 확인 후 수동 마이그레이션 생성

**문제 2: PostgreSQL 확장 설치 실패**
- 온프레미스 환경에서는 개발자가 직접 서버에 접속하여 작업해야 하므로 상세 가이드를 제시합니다
- 현재 사용자 권한 확인 후 수동으로 pg_trgm 확장 설치

---

## Phase 3: 로컬 데이터 마이그레이션

### 🎯 목표
CSV 파일을 분석하고 데이터를 정규화하여 PostgreSQL 데이터베이스에 배치 입력합니다.

### 📋 Phase 3 체크리스트

#### 3.1 CSV 파일 준비 및 분석
- [x] **CSV 파일 확인**: datas 디렉토리에서 예상 CSV 파일들 존재 확인
  - TB_RECIPE_SEARCH-20231130.csv (78MB)
  - TB_RECIPE_SEARCH-220701.csv (53MB)
  - TB_RECIPE_SEARCH_241226.csv (17MB)
- [x] **CSV 파일 인코딩 및 구조 분석**: 파일 인코딩 확인, 헤더 및 행 수 확인
- [x] **샘플 데이터 분석 스크립트 생성**: `scripts/analyze_csv.py` 스크립트 생성 및 실행하여 각 파일의 컬럼 구조와 데이터 샘플 분석

#### 3.2 재료 파싱 로직 구현
- [x] **재료 파싱 모듈 생성**: `app/utils/ingredient_parser.py` 모듈 구현
  - 수량 패턴 정의 (단일 수량, 분수, 범위)
  - 모호한 표현 처리 (적당량, 약간, 조금 등)
  - 단위 정규화 (큰술, 작은술, 컵, g, kg, ml, L 등)
  - 재료명 정리 및 표기 통일 (대파→파, 계란→달걀 등)
  - 파싱 결과: 재료명, 수량, 단위, 필수여부, 표시순서

#### 3.3 데이터 마이그레이션 스크립트 구현
- [x] **메인 마이그레이션 스크립트 생성**: `scripts/migrate_csv_data.py` 구현
  - CSVDataMigrator 클래스로 전체 마이그레이션 관리
  - 비동기 SQLAlchemy 세션 사용
  - 배치 단위 처리 (기본 100개)
  - 캐시 시스템 (재료, 카테고리)
  - 로깅 및 진행상황 추적 (tqdm 프로그레스바)
  - 중복 레시피 체크 및 건너뛰기
  - 재료 자동 카테고리 분류 (키워드 기반)
  - 에러 처리 및 롤백 메커니즘
  - 명령행 인자로 chunk-size, max-records 조정 가능

#### 3.4 마이그레이션 검증
- [x] **검증 스크립트 생성**: `scripts/verify_migration.py` 스크립트 작성
  - 데이터 통계 확인 (레시피, 재료, 연결 수)
  - 데이터 품질 검증 (빈 제목, 재료 없는 레시피 등)
  - 카테고리별 재료 분포 확인
  - 검색 기능 테스트
  - 샘플 데이터 출력
  - 인덱스 상태 확인

#### 3.5 마이그레이션 실행
- [ ] **마이그레이션 실행**: `python scripts/migrate_csv_data.py` 명령어로 전체 마이그레이션 실행
- [ ] **진행 상황 모니터링**: 로그 파일 및 데이터베이스 상태를 실시간으로 모니터링
- [ ] **검증 실행**: `python scripts/verify_migration.py`로 마이그레이션 결과 검증

### ✅ Phase 3 완료 검증

다음 검증 작업들이 모두 성공해야 Phase 3이 완료됩니다:

1. **데이터 입력 확인**: 모든 테이블(recipes, ingredients, recipe_ingredients, ingredient_categories)에 데이터가 정상적으로 입력되었는지 확인
2. **데이터 품질 확인**: 제목이 비어있는 레시피, 재료가 없는 레시피, 중복 레시피 데이터 검증
3. **검색 기능 테스트**: 한국어 전문검색 및 재료 기반 검색 기능이 정상 동작하는지 확인

### 🚨 Phase 3 트러블슈팅

**문제 1: 메모리 부족**
- 배치 크기를 500으로 감소하여 메모리 사용량 최적화
- 마이그레이션 스크립트 실행 시 BATCH_SIZE 환경변수 조정

**문제 2: 인코딩 오류**
- CSV 파일의 인코딩을 file 명령어나 chardet으로 재확인
- 필요시 iconv를 사용하여 EUC-KR에서 UTF-8로 수동 변환

**문제 3: 데이터베이스 연결 타임아웃**
- 연결 풀 설정을 조정하여 pool_size와 max_overflow 값 증가
- DATABASE_URL에 연결 풀 파라미터 추가

---

## Phase 4: Kubernetes 환경 배포

### 🎯 목표
Docker 컨테이너로 데이터 마이그레이션 Job을 생성하고 Kubernetes 환경에서 실행합니다.

### 📋 Phase 4 체크리스트

#### 4.1 Docker 이미지 빌드 준비
- [x] **Dockerfile.migration 생성**: 마이그레이션 전용 Docker 이미지 정의
  - Python 3.12-slim 베이스 이미지
  - requirements-api.txt 사용 (기존 패키지 활용)
  - PostgreSQL 클라이언트 포함
  - Health check 설정
- [x] **entrypoint.sh 작성**: 컨테이너 진입점 스크립트
  - PostgreSQL 연결 대기 로직
  - Alembic 마이그레이션 실행
  - 기본 데이터 삽입
  - CSV 데이터 마이그레이션
  - 검증 프로세스

#### 4.2 Kubernetes 리소스 정의
- [x] **ConfigMap 생성 (k8s/configmap.yaml)**: 환경 설정
  - 마이그레이션 모드 설정
  - 청크 크기 및 로깅 설정
- [x] **Secret 생성 (k8s/secret.yaml)**: 민감 정보
  - DATABASE_URL 저장
- [x] **Job 정의 (k8s/job.yaml)**: 일회성 마이그레이션 작업
  - Init 컨테이너로 PostgreSQL 대기
  - 리소스 제한 설정
  - PVC for CSV 파일
- [x] **CronJob 정의 (k8s/cronjob.yaml)**: 주기적 데이터 업데이트
  - 매주 일요일 새벽 2시 실행
  - 증분 업데이트 모드

#### 4.3 배포 자동화
- [x] **배포 스크립트 작성 (scripts/deploy_k8s.sh)**: 자동화된 배포
  - Docker 이미지 빌드 및 푸시
  - Kubernetes 리소스 배포
  - Job 실행 및 모니터링
  - 리소스 정리 기능


## 전체 시스템 통합 검증

### 🎯 최종 검증 체크리스트

모든 Phase가 완료된 후 다음 검증을 수행합니다:

#### 전체 시스템 상태 확인
- [ ] **데이터베이스 전체 상태**: 테이블별 데이터 건수 및 삽입/업데이트/삭제 통계 확인
- [ ] **인덱스 사용률 확인**: 생성된 인덱스들의 스캔 횟수 및 사용률 모니터링

#### 검색 성능 테스트
- [ ] **전문검색 성능**: 한국어 전문검색 쿼리의 실행 계획 및 성능 분석
- [ ] **재료 기반 검색 성능**: 복수 재료 조건을 통한 레시피 검색 성능 테스트

#### 데이터 품질 검증
- [ ] **데이터 무결성 체크**: 레시피-재료 관계 테이블의 외래키 무결성 검증 및 고아 레코드 확인

### 📊 성공 기준

다음 조건들을 모두 만족해야 프로젝트가 성공적으로 완료된 것으로 간주합니다:

1. **데이터 완성도**: 레시피 데이터 90% 이상 성공적으로 마이그레이션
2. **검색 성능**: 전문검색 응답시간 1초 이내
3. **데이터 품질**: 외래키 무결성 100% 보장
4. **시스템 안정성**: Kubernetes Job 성공 완료
5. **재현 가능성**: 모든 단계가 문서화되고 실행 가능

---

## 부록: 추가 유틸리티

### 개발 편의 스크립트

#### 전체 상태 확인 스크립트
시스템 전체 상태를 한 번에 확인할 수 있는 자동화 스크립트를 제공합니다. 데이터베이스 연결, 테이블 데이터 현황, 마이그레이션 Job 상태, 검색 기능 동작 여부를 종합적으로 점검할 수 있습니다.

이 종합 구현 로드맵을 통해 개발자는 Fridge2Fork 프로젝트를 체계적이고 안전하게 구축할 수 있습니다. 각 Phase를 순차적으로 완료하고 검증하여 안정적인 레시피 추천 시스템을 구축하시기 바랍니다.