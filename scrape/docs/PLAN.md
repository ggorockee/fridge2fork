# `crawler.py` 문서화 계획

이 문서는 `crawler.py` 스크립트에 대한 기술 문서를 작성하기 위한 체계적인 계획을 정의합니다. 제공된 전문가 가이드라인에 따라 명확성, 사용성, 점진적 학습을 목표로 합니다.

---

## 1단계: 잠재 고객 분석 및 요구 사항 평가

-   **주요 잠재 고객**:
    -   **개발자**: 스크립트를 유지보수하거나 새로운 기능을 추가해야 하는 동료 개발자.
    -   **사용자**: 스크립트를 직접 실행하여 데이터를 수집해야 하는 데이터 분석가 또는 팀원.
-   **사용자 목표**:
    -   개발자: 스크립트의 구조, 주요 함수, 데이터 흐름을 빠르게 파악하여 수정 및 확장이 용이해야 함.
    -   사용자: 스크립트 설치, 설정, 실행 방법을 명확히 이해하고 문제 발생 시 해결 방법을 찾을 수 있어야 함.
-   **문서 요구 사항**:
    -   스크립트의 목적과 기능에 대한 명확한 설명.
    -   필요한 라이브러리 및 설치 방법에 대한 안내.
    -   스크립트 실행 방법 및 설정 옵션에 대한 단계별 가이드.
    -   코드 내 주요 함수 및 클래스에 대한 상세한 설명 (Docstrings).
    -   크롤링 대상 웹사이트(`www.10000recipe.com`) 구조 변경 시 대처 방안 가이드.

## 2단계: 콘텐츠 구조 및 구성

-   **정보 아키텍처**:
    1.  `README.md`: 프로젝트 최상위 레벨의 개요, 빠른 시작 가이드.
    2.  `docs/`: 보다 상세한 문서를 위한 디렉토리.
        -   `docs/guide.md`: 상세 사용자 가이드.
        -   `docs/technical-spec.md`: 기술 사양 및 아키텍처.
    3.  `crawler.py` 내 Docstrings: 코드와 함께 제공되는 인라인 문서.
-   **구성**:
    -   **README.md**: 스크립트 개요, 주요 기능, 설치 방법, 기본 사용법, 라이선스 정보.
    -   **상세 사용자 가이드 (`guide.md`)**: 설정 옵션 상세, 다양한 실행 시나리오, 결과물(CSV, JSON 등) 포맷 설명, 문제 해결 가이드.
    -   **기술 사양 (`technical-spec.md`)**: 스크립트의 주요 로직, CSS 선택자 관리 방안, 데이터 저장 방식 및 형식, 확장 방안.
        -   **데이터 저장 형식**: `recipes.json`에 저장되는 재료 데이터는 `{'재료명': '수량'}` 형태의 딕셔너리로 구조화한다. (예: `{'감자': '2개', '스팸': '200g'}`)
    -   **인라인 문서**: 모든 함수와 클래스에 대해 역할, 파라미터, 반환 값, 사용 예제를 포함하는 Docstring 작성.

## 3단계: 문서 생성 패턴 적용

-   **Inline Documentation (Docstrings)**:
    -   `crawler.py` 내 모든 함수에 대해 Python 표준 (PEP 257)에 맞는 Docstring을 작성합니다.
    -   복잡한 로직(예: 상세 페이지 파싱)에는 별도의 주석으로 구현 이유를 설명합니다.
-   **User Guide (`README.md`, `guide.md`)**:
    -   **빠른 시작**: 사용자가 5분 안에 스크립트를 실행해 볼 수 있도록 간단한 예제를 제공합니다.
    -   **단계별 절차**: `1. 저장소 복제 -> 2. 라이브러리 설치 -> 3. 스크립트 실행` 과 같이 명확한 단계를 제시하고, 각 단계별 확인 방법을 포함합니다.
-   **Technical Specifications (`technical-spec.md`)**:
    -   크롤링 대상 사이트의 구조를 간략히 다이어그램으로 표현하고, 현재 사용 중인 CSS 선택자를 표로 정리합니다.
    -   향후 다른 레시피 사이트를 추가할 경우의 확장 전략을 제시합니다.

## 4단계: 품질 및 접근성 표준

-   **실행 가능한 예제**: 문서에 포함된 모든 코드 예제는 복사-붙여넣기 만으로 실행 가능해야 합니다.
-   **검증 단계**: "스크립트 실행 후 `recipes.csv` 파일이 생성되어야 합니다." 와 같이 사용자가 각 단계의 성공 여부를 확인할 수 있는 명확한 기준을 제공합니다.
-   **언어**: 전문 용어 사용을 최소화하고, 명확하고 간결한 문체를 유지합니다. (한국어 기준)
-   **유지보수**: `PLAN.md` 문서를 `docs/` 폴더에 유지하여 다른 문서들과 함께 관리하고, 코드 변경 시 관련 문서가 함께 업데이트되도록 가이드라인을 마련합니다.

---

--- 

## 5단계: 문서 고도화 (신규)

초기 문서화 작업이 완료되었으므로, 다음 단계로 프로젝트의 유지보수성과 확장성을 높이기 위한 심화 문서를 작성합니다.

-   **아키텍처 다이어그램**: `technical-spec.md`에 스크립트의 작동 흐름을 시각적으로 표현하는 다이어그램(Mermaid.js 등 활용)을 추가합니다.
-   **기여 가이드 (`CONTRIBUTING.md`)**: 다른 개발자가 프로젝트에 기여할 수 있도록 코딩 컨벤션, 브랜치 전략, PR 절차 등을 안내하는 문서를 작성합니다.
-   **변경 로그 (`CHANGELOG.md`)**: 프로젝트의 버전별 변경 사항을 추적하고 관리하는 문서를 추가합니다.
-   **성능 프로파일링 가이드**: (보류된) 비동기 전환과 같은 성능 개선 작업을 위한 사전 준비로, 현재 코드의 성능을 측정하고 병목 현상을 분석하는 방법을 문서화합니다.

---

--- 

## 6단계: 대규모 크롤링 고도화 (신규)

20만 개 이상의 전체 레시피를 안정적으로 수집하기 위해, 현재의 단일 페이지 스크립트를 다음과 같이 고도화합니다. 첫 목표는 100개의 데이터를 안정적으로 수집하는 것입니다.

1.  **페이지네이션(Pagination) 처리**: 
    -   **목표**: `[1], [2], ..., [>]` 형태의 모든 페이지를 순차적으로 방문합니다.
    -   **구현**: 다음 페이지로 이동하는 `>` 버튼 또는 `?page=N` URL 패턴을 분석하여, 목표한 수량(100개)을 채울 때까지 자동으로 탐색하는 로직을 추가합니다.

2.  **안정성 및 복원력 강화 (Robustness & Resilience)**:
    -   **체크포인팅 (Checkpointing)**: 마지막으로 성공한 페이지 번호를 별도 파일(`progress.txt`)에 기록합니다. 스크립트 재시작 시, 이 파일를 읽어 중단된 지점부터 크롤링을 재개하는 기능을 구현합니다.
    -   **재시도 로직**: 일시적인 네트워크 오류나 서버 에러 발생 시, 일정 시간 대기 후 몇 차례 재시도하는 로직을 추가합니다.
    -   **속도 제어 (Throttling)**: 서버에 과도한 부하를 주지 않고 IP 차단을 피하기 위해 각 요청 사이에 적절한 지연 시간(`time.sleep()`)을 추가합니다.

3.  **성능 고도화: 비동기 처리 도입**:
    -   **필요성**: 대규모 작업을 현실적인 시간 내에 완료하기 위해 비동기 처리가 필수적입니다.
    -   **구현**: `asyncio`와 `aiohttp`를 도입하여, 여러 페이지를 동시에 요청/처리하도록 `main` 함수와 `scrape_recipe_details` 함수를 재설계합니다.

4.  **데이터 저장 방식 변경**:
    -   **문제점**: 대용량 데이터를 단일 JSON 파일에 저장하면 메모리 문제를 유발합니다.
    -   **해결책**: **JSON Lines (`.jsonl`)** 형식으로 변경합니다. 각 레시피를 한 줄에 하나의 JSON 객체로 저장하여, 대용량 데이터를 스트리밍 방식으로 안전하게 기록합니다. 최종 파일명은 `recipes.jsonl`로 변경합니다.

---

**다음 단계**: 6단계 계획에 따라 `crawler.py`의 아키텍처를 비동기 방식으로 변경하고, 페이지네이션 및 JSON Lines 저장 방식을 구현합니다.