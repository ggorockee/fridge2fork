#!/usr/bin/env python3
"""
CSV ë°ì´í„°ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/migrate_csv_data.py [--chunk-size 100] [--max-records 1000]

Prerequisites:
    - DATABASE_URLì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
    - PostgreSQL ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨
    - Phase 2 (ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ)ê°€ ì™„ë£Œë˜ì–´ì•¼ í•¨
"""
import asyncio
import sys
import os
import pandas as pd
import chardet
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from tqdm import tqdm
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, text
from sqlalchemy.dialects.postgresql import insert

from app.models.recipe import Recipe, Ingredient, IngredientCategory, RecipeIngredient
from app.utils.ingredient_parser import IngredientParser, parse_ingredients_list

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CSVDataMigrator:
    """CSV ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, chunk_size: int = 100, max_records: Optional[int] = None):
        """
        Args:
            chunk_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
            max_records: ìµœëŒ€ ì²˜ë¦¬ ë ˆì½”ë“œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        self.chunk_size = chunk_size
        self.max_records = max_records
        self.engine = None
        self.async_session = None
        self.parser = IngredientParser()
        
        # í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…
        print("ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        print(f"POSTGRES_DB: {os.getenv('POSTGRES_DB', 'NOT SET')}")
        print(f"POSTGRES_USER: {os.getenv('POSTGRES_USER', 'NOT SET')}")
        print(f"POSTGRES_PASSWORD: {'SET' if os.getenv('POSTGRES_PASSWORD') else 'NOT SET'}")
        print(f"POSTGRES_SERVER: {os.getenv('POSTGRES_SERVER', 'NOT SET')}")
        print(f"POSTGRES_PORT: {os.getenv('POSTGRES_PORT', 'NOT SET')}")
        print(f"DATABASE_URL: {os.getenv('DATABASE_URL', 'NOT SET')}")
        
        # DATABASE_URLì´ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ êµ¬ì„± ì‹œë„
        if not os.getenv('DATABASE_URL'):
            db = os.getenv('POSTGRES_DB')
            user = os.getenv('POSTGRES_USER')
            password = os.getenv('POSTGRES_PASSWORD')
            server = os.getenv('POSTGRES_SERVER')
            port = os.getenv('POSTGRES_PORT')
            
            if all([db, user, password, server, port]):
                database_url = f"postgresql://{user}:{password}@{server}:{port}/{db}"
                os.environ['DATABASE_URL'] = database_url
                print(f"âœ… DATABASE_URL ìë™ êµ¬ì„±: postgresql://{user}:***@{server}:{port}/{db}")
            else:
                print("âš ï¸ DATABASE_URL êµ¬ì„±ì— í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # ìºì‹œ
        self.category_cache = {}  # ì¹´í…Œê³ ë¦¬ ID ìºì‹œ
        self.ingredient_cache = {}  # ì¬ë£Œ ID ìºì‹œ

        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'recipes_created': 0,
            'ingredients_created': 0,
            'recipe_ingredients_created': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }

    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™”"""
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            raise ValueError("DATABASE_URL environment variable is not set")

        # asyncpgë¥¼ ì‚¬ìš©í•˜ë„ë¡ URL ë³€í™˜
        if database_url.startswith("postgresql://"):
            database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)

        logger.info(f"Connecting to database...")
        self.engine = create_async_engine(
            database_url,
            echo=False,  # SQL ì¿¼ë¦¬ ì¶œë ¥ ë¹„í™œì„±í™”
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True
        )
        self.async_session = sessionmaker(self.engine, class_=AsyncSession, expire_on_commit=False)

        # ìºì‹œ ë¡œë“œ (ì„ íƒì )
        try:
            await self.load_caches()
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë”© ì‹¤íŒ¨, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„: {e}")
            # ìºì‹œ ë¡œë”© ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    async def load_caches(self):
        """ì¹´í…Œê³ ë¦¬ì™€ ì¬ë£Œ ìºì‹œ ë¡œë“œ"""
        async with self.async_session() as session:
            # ì¹´í…Œê³ ë¦¬ ìºì‹œ ë¡œë“œ
            try:
                result = await session.execute(select(IngredientCategory))
                categories = result.scalars().all()
                self.category_cache = {cat.name: cat.id for cat in categories}
                logger.info(f"Loaded {len(self.category_cache)} categories to cache")
            except Exception as e:
                logger.warning(f"ì¹´í…Œê³ ë¦¬ ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.category_cache = {}

            # ì¬ë£Œ ìºì‹œ ë¡œë“œ
            try:
                result = await session.execute(select(Ingredient))
                ingredients = result.scalars().all()
                self.ingredient_cache = {ing.normalized_name: ing.id for ing in ingredients}
                logger.info(f"Loaded {len(self.ingredient_cache)} ingredients to cache")
            except Exception as e:
                logger.warning(f"ì¬ë£Œ ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.ingredient_cache = {}

    def detect_encoding(self, file_path):
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        with open(file_path, 'rb') as f:
            raw_data = f.read(100000)
            result = chardet.detect(raw_data)
            return result['encoding']

    def read_csv_file(self, file_path: Path) -> pd.DataFrame:
        """CSV íŒŒì¼ ì½ê¸°"""
        logger.info("ğŸ” íŒŒì¼ ì¸ì½”ë”© ê°ì§€ ì¤‘...")
        encoding = self.detect_encoding(file_path)
        
        # í•œêµ­ì–´ CSV íŒŒì¼ì— ì¼ë°˜ì ì¸ ì¸ì½”ë”©ë“¤
        encodings_to_try = ['EUC-KR', 'CP949', 'UTF-8', 'UTF-8-SIG']
        if encoding and encoding not in encodings_to_try:
            encodings_to_try.insert(0, encoding)
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path.name}")
        logger.info(f"ğŸ”¤ ì‹œë„í•  ì¸ì½”ë”©: {', '.join(encodings_to_try)}")
        
        for enc in encodings_to_try:
            try:
                logger.info(f"ğŸ”„ {enc} ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...")
                df = pd.read_csv(file_path, encoding=enc, on_bad_lines='skip')
                logger.info(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {enc} ì¸ì½”ë”© ì‚¬ìš©")
                logger.info(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(df):,}ê°œ í–‰, {len(df.columns)}ê°œ ì—´")
                logger.info(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
                return df
            except Exception as e:
                logger.warning(f"âŒ {enc} ì¸ì½”ë”© ì‹¤íŒ¨: {str(e)[:100]}...")
                continue

        # ë§ˆì§€ë§‰ ì‹œë„: ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ì½ê¸°
        try:
            logger.warning("âš ï¸ ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨, ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ì½ê¸° ì‹œë„...")
            df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip', errors='ignore')
            logger.info(f"âœ… ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ë¡œë“œ ì„±ê³µ: {len(df):,}ê°œ í–‰")
            return df
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ì½ê¸° ì‹¤íŒ¨: {e}")
            raise ValueError(f"Failed to read {file_path.name} with any encoding")

    async def migrate_file(self, file_path: Path):
        """ë‹¨ì¼ CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("=" * 60)
        logger.info(f"ğŸ“š CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        logger.info(f"ğŸ“„ íŒŒì¼: {file_path.name}")
        logger.info(f"ğŸ“ ê²½ë¡œ: {file_path}")
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ“Š í¬ê¸°: {file_size_mb:.2f} MB")
        logger.info("=" * 60)

        # CSV íŒŒì¼ ì½ê¸°
        df = self.read_csv_file(file_path)
        total_rows = len(df)

        # ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ ì œí•œ
        if self.max_records:
            df = df.head(self.max_records)
            logger.info(f"âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.max_records:,}ê°œ ë ˆì½”ë“œë¡œ ì œí•œ")

        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ë§¤í•‘
        logger.info("ğŸ” ì»¬ëŸ¼ ë§¤í•‘ ê²€ìƒ‰ ì¤‘...")
        column_mapping = self.detect_columns(df)
        if not column_mapping:
            logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path.name}")
            return
        logger.info(f"âœ… ì»¬ëŸ¼ ë§¤í•‘ ì™„ë£Œ:")
        for key, value in column_mapping.items():
            logger.info(f"    - {key}: {value}")

        # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_chunks = (len(df) + self.chunk_size - 1) // self.chunk_size
        logger.info(f"ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘:")
        logger.info(f"    - ì´ ë ˆì½”ë“œ: {len(df):,}ê°œ")
        logger.info(f"    - ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
        logger.info(f"    - ì²­í¬ í¬ê¸°: {self.chunk_size}ê°œ")

        with tqdm(total=len(df), desc=f"Migrating {file_path.name}") as pbar:
            for chunk_idx in range(0, len(df), self.chunk_size):
                chunk = df.iloc[chunk_idx:chunk_idx + self.chunk_size]
                chunk_num = (chunk_idx // self.chunk_size) + 1
                logger.debug(f"ğŸ“¦ ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
                await self.process_chunk(chunk, column_mapping)
                pbar.update(len(chunk))

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if chunk_idx % (self.chunk_size * 10) == 0:
                    await asyncio.sleep(0.1)  # ë‹¤ë¥¸ ì‘ì—…ì— CPU ì–‘ë³´

    def detect_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ ê°ì§€"""
        column_mapping = {}

        # ë ˆì‹œí”¼ëª… ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if 'RCP_NM' in col or 'ë ˆì‹œí”¼' in col and 'ëª…' in col:
                column_mapping['title'] = col
                break

        # ì¬ë£Œ ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if 'RCP_PARTS_DTLS' in col or 'ì¬ë£Œ' in col:
                column_mapping['ingredients'] = col
                break

        # ìš”ë¦¬ë°©ë²• ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if 'RCP_WAY2' in col or 'ìš”ë¦¬ë°©ë²•' in col or 'ì¡°ë¦¬ë°©ë²•' in col:
                column_mapping['cooking_method'] = col
                break

        # ì´ë¯¸ì§€ URL ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if 'ATT_FILE' in col or 'IMG' in col or 'ì´ë¯¸ì§€' in col:
                column_mapping['image_url'] = col
                break

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if 'title' not in column_mapping:
            return None

        return column_mapping

    async def process_chunk(self, chunk: pd.DataFrame, column_mapping: Dict[str, str]):
        """ë°ì´í„° ì²­í¬ ì²˜ë¦¬"""
        async with self.async_session() as session:
            try:
                for _, row in chunk.iterrows():
                    await self.process_recipe(session, row, column_mapping)

                await session.commit()
                self.stats['total_processed'] += len(chunk)

            except Exception as e:
                logger.error(f"Error processing chunk: {e}")
                await session.rollback()
                self.stats['errors'] += len(chunk)

    async def process_recipe(self, session: AsyncSession, row: pd.Series, column_mapping: Dict[str, str]):
        """ë‹¨ì¼ ë ˆì‹œí”¼ ì²˜ë¦¬"""
        try:
            # ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ
            title = str(row[column_mapping['title']]) if pd.notna(row[column_mapping['title']]) else None
            if not title:
                return

            # URL ìƒì„± (ì‹¤ì œ URLì´ ì—†ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ ìƒì„±)
            recipe_url = f"recipe_{self.stats['total_processed'] + 1}"

            # ì¤‘ë³µ ì²´í¬
            result = await session.execute(
                select(Recipe).where(Recipe.url == recipe_url)
            )
            existing = result.scalar_one_or_none()
            if existing:
                self.stats['duplicates_skipped'] += 1
                return

            # ë ˆì‹œí”¼ ìƒì„±
            recipe = Recipe(
                url=recipe_url,
                title=title,
                image_url=str(row[column_mapping['image_url']]) if 'image_url' in column_mapping and pd.notna(row[column_mapping['image_url']]) else None,
                cooking_method=str(row[column_mapping['cooking_method']]) if 'cooking_method' in column_mapping and pd.notna(row[column_mapping['cooking_method']]) else None,
            )
            session.add(recipe)
            await session.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
            self.stats['recipes_created'] += 1

            # ì¬ë£Œ ì²˜ë¦¬
            if 'ingredients' in column_mapping and pd.notna(row[column_mapping['ingredients']]):
                ingredients_text = str(row[column_mapping['ingredients']])
                await self.process_ingredients(session, recipe.id, ingredients_text)

        except Exception as e:
            logger.error(f"Error processing recipe: {e}")
            self.stats['errors'] += 1

    async def process_ingredients(self, session: AsyncSession, recipe_id: int, ingredients_text: str):
        """ì¬ë£Œ ì²˜ë¦¬"""
        parsed_ingredients = parse_ingredients_list(ingredients_text)

        for idx, parsed_ing in enumerate(parsed_ingredients):
            try:
                # ì¬ë£Œ ì°¾ê¸° ë˜ëŠ” ìƒì„±
                ingredient_id = await self.get_or_create_ingredient(
                    session,
                    parsed_ing.normalized_name,
                    parsed_ing.name
                )

                # ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²° ìƒì„±
                recipe_ingredient = RecipeIngredient(
                    recipe_id=recipe_id,
                    ingredient_id=ingredient_id,
                    quantity_from=parsed_ing.quantity_from,
                    quantity_to=parsed_ing.quantity_to,
                    unit=parsed_ing.unit,
                    original_text=parsed_ing.original_text[:200] if parsed_ing.original_text else None,
                    is_vague=parsed_ing.is_vague,
                    vague_description=parsed_ing.vague_description[:50] if parsed_ing.vague_description else None,
                    importance=parsed_ing.importance,
                    display_order=idx
                )
                session.add(recipe_ingredient)
                self.stats['recipe_ingredients_created'] += 1

            except Exception as e:
                logger.error(f"Error processing ingredient '{parsed_ing.name}': {e}")

    async def get_or_create_ingredient(self, session: AsyncSession, normalized_name: str, original_name: str) -> int:
        """ì¬ë£Œ ì°¾ê¸° ë˜ëŠ” ìƒì„±"""
        # ìºì‹œì—ì„œ í™•ì¸
        if normalized_name in self.ingredient_cache:
            return self.ingredient_cache[normalized_name]

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸
        result = await session.execute(
            select(Ingredient).where(Ingredient.normalized_name == normalized_name)
        )
        ingredient = result.scalar_one_or_none()

        if ingredient:
            self.ingredient_cache[normalized_name] = ingredient.id
            return ingredient.id

        # ìƒˆ ì¬ë£Œ ìƒì„±
        category_name = self.parser.categorize_ingredient(normalized_name)
        category_id = self.category_cache.get(category_name)

        ingredient = Ingredient(
            name=original_name,
            normalized_name=normalized_name,
            category_id=category_id,
            is_ambiguous=False
        )
        session.add(ingredient)
        await session.flush()

        self.ingredient_cache[normalized_name] = ingredient.id
        self.stats['ingredients_created'] += 1

        return ingredient.id

    async def print_statistics(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„ ì¶œë ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„")
        logger.info("="*60)
        logger.info(f"ì´ ì²˜ë¦¬ ë ˆì½”ë“œ: {self.stats['total_processed']:,}")
        logger.info(f"ìƒì„±ëœ ë ˆì‹œí”¼: {self.stats['recipes_created']:,}")
        logger.info(f"ìƒì„±ëœ ì¬ë£Œ: {self.stats['ingredients_created']:,}")
        logger.info(f"ìƒì„±ëœ ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²°: {self.stats['recipe_ingredients_created']:,}")
        logger.info(f"ê±´ë„ˆë›´ ì¤‘ë³µ: {self.stats['duplicates_skipped']:,}")
        logger.info(f"ì˜¤ë¥˜: {self.stats['errors']:,}")

        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        async with self.async_session() as session:
            result = await session.execute(text("SELECT COUNT(*) FROM recipes"))
            total_recipes = result.scalar()
            result = await session.execute(text("SELECT COUNT(*) FROM ingredients"))
            total_ingredients = result.scalar()
            result = await session.execute(text("SELECT COUNT(*) FROM recipe_ingredients"))
            total_relations = result.scalar()

            logger.info("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©")
            logger.info(f"ì´ ë ˆì‹œí”¼ ìˆ˜: {total_recipes:,}")
            logger.info(f"ì´ ì¬ë£Œ ìˆ˜: {total_ingredients:,}")
            logger.info(f"ì´ ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²° ìˆ˜: {total_relations:,}")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.engine:
            await self.engine.dispose()


async def main(args):
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("\n" + "="*70)
    logger.info("ğŸš€ ë ˆì‹œí”¼ CSV ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    logger.info("="*70)

    # ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™”
    logger.info("ğŸ­ ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™” ì¤‘...")
    migrator = CSVDataMigrator(
        chunk_size=args.chunk_size,
        max_records=args.max_records
    )
    logger.info(f"    - ì²­í¬ í¬ê¸°: {args.chunk_size}")
    if args.max_records:
        logger.info(f"    - ìµœëŒ€ ë ˆì½”ë“œ: {args.max_records:,}")

    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        logger.info("ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        await migrator.initialize()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # CSV íŒŒì¼ ì°¾ê¸°
        logger.info("ğŸ” CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        datas_dir = project_root / "datas"
        logger.info(f"    - ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {datas_dir}")
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not datas_dir.exists():
            logger.error(f"âŒ datas ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {datas_dir}")
            logger.info("í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ë‚´ìš©:")
            for item in project_root.iterdir():
                logger.info(f"    - {item.name} ({'ë””ë ‰í† ë¦¬' if item.is_dir() else 'íŒŒì¼'})")
            return
        
        # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
        logger.info(f"datas ë””ë ‰í† ë¦¬ ë‚´ìš©:")
        for item in datas_dir.iterdir():
            logger.info(f"    - {item.name} ({'ë””ë ‰í† ë¦¬' if item.is_dir() else 'íŒŒì¼'})")
        
        # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ CSV íŒŒì¼ ê²€ìƒ‰
        csv_patterns = [
            "TB_RECIPE_SEARCH*.csv",
            "*.csv"
        ]
        
        csv_files = []
        for pattern in csv_patterns:
            found_files = sorted(datas_dir.glob(pattern))
            csv_files.extend(found_files)
            if found_files:
                logger.info(f"    - íŒ¨í„´ '{pattern}'ìœ¼ë¡œ {len(found_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        csv_files = sorted(list(set(csv_files)))

        if not csv_files:
            logger.error("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            logger.error(f"    - ê²€ìƒ‰ ê²½ë¡œ: {datas_dir}")
            logger.error("    - íŒ¨í„´: TB_RECIPE_SEARCH*.csv")
            return 1

        logger.info(f"âœ… {len(csv_files)}ê°œ CSV íŒŒì¼ ë°œê²¬:")
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"    {i}. {csv_file.name} ({csv_file.stat().st_size / (1024*1024):.2f} MB)")

        # ê° íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
        logger.info("\nğŸ”„ CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
        for idx, csv_file in enumerate(csv_files, 1):
            logger.info(f"\n[íŒŒì¼ {idx}/{len(csv_files)}] {csv_file.name} ì²˜ë¦¬ ì¤‘...")
            await migrator.migrate_file(csv_file)
            logger.info(f"âœ… [íŒŒì¼ {idx}/{len(csv_files)}] {csv_file.name} ì²˜ë¦¬ ì™„ë£Œ")

        # í†µê³„ ì¶œë ¥
        logger.info("\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„ ì¶œë ¥ ì¤‘...")
        await migrator.print_statistics()

        logger.info("\n" + "="*70)
        logger.info("ğŸ‰ ëª¨ë“  CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ!")
        logger.info("="*70)
        return 0

    except Exception as e:
        logger.error("\n" + "="*70)
        logger.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨")
        logger.error(f"ğŸ”¥ ì˜¤ë¥˜: {e}")
        logger.error("="*70)
        import traceback
        logger.error(traceback.format_exc())
        return 1

    finally:
        await migrator.cleanup()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Migrate CSV data to PostgreSQL")
    parser.add_argument("--chunk-size", type=int, default=100,
                       help="Number of records to process in each batch")
    parser.add_argument("--max-records", type=int, default=None,
                       help="Maximum number of records to process (for testing)")

    args = parser.parse_args()

    sys.exit(asyncio.run(main(args)))