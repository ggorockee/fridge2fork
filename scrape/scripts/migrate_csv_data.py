#!/usr/bin/env python3
"""
CSV ë°ì´í„°ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/migrate_csv_data.py [--chunk-size 100] [--max-records 1000]

Prerequisites:
    - DATABASE_URLì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
    - PostgreSQL ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨
    - Phase 2 (ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ)ê°€ ì™„ë£Œë˜ì–´ì•¼ í•¨
"""
import asyncio
import sys
import os
import pandas as pd
import chardet
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from tqdm import tqdm
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, text
from sqlalchemy.dialects.postgresql import insert

from app.models.recipe import Recipe, Ingredient, RecipeIngredient
from app.utils.ingredient_parser import IngredientParser, parse_ingredients_list

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CSVDataMigrator:
    """CSV ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, chunk_size: int = 100, max_records: Optional[int] = None):
        """
        Args:
            chunk_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
            max_records: ìµœëŒ€ ì²˜ë¦¬ ë ˆì½”ë“œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        self.chunk_size = chunk_size
        self.max_records = max_records
        self.engine = None
        self.async_session = None
        self.parser = IngredientParser()
        
        # í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…
        print("ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        print(f"POSTGRES_DB: {os.getenv('POSTGRES_DB', 'NOT SET')}")
        print(f"POSTGRES_USER: {os.getenv('POSTGRES_USER', 'NOT SET')}")
        print(f"POSTGRES_PASSWORD: {'SET' if os.getenv('POSTGRES_PASSWORD') else 'NOT SET'}")
        print(f"POSTGRES_SERVER: {os.getenv('POSTGRES_SERVER', 'NOT SET')}")
        print(f"POSTGRES_PORT: {os.getenv('POSTGRES_PORT', 'NOT SET')}")
        print(f"DATABASE_URL: {os.getenv('DATABASE_URL', 'NOT SET')}")
        
        # DATABASE_URLì´ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ êµ¬ì„± ì‹œë„
        if not os.getenv('DATABASE_URL'):
            db = os.getenv('POSTGRES_DB')
            user = os.getenv('POSTGRES_USER')
            password = os.getenv('POSTGRES_PASSWORD')
            server = os.getenv('POSTGRES_SERVER')
            port = os.getenv('POSTGRES_PORT')
            
            if all([db, user, password, server, port]):
                database_url = f"postgresql://{user}:{password}@{server}:{port}/{db}"
                os.environ['DATABASE_URL'] = database_url
                print(f"âœ… DATABASE_URL ìë™ êµ¬ì„±: postgresql://{user}:***@{server}:{port}/{db}")
            else:
                print("âš ï¸ DATABASE_URL êµ¬ì„±ì— í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # ìºì‹œ
        self.category_cache = {}  # ì¹´í…Œê³ ë¦¬ ID ìºì‹œ
        self.ingredient_cache = {}  # ì¬ë£Œ ID ìºì‹œ

        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'recipes_created': 0,
            'ingredients_created': 0,
            'recipe_ingredients_created': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }

    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™”"""
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            raise ValueError("DATABASE_URL environment variable is not set")

        # asyncpgë¥¼ ì‚¬ìš©í•˜ë„ë¡ URL ë³€í™˜
        if database_url.startswith("postgresql://"):
            database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)

        logger.info(f"Connecting to database...")
        self.engine = create_async_engine(
            database_url,
            echo=False,  # SQL ì¿¼ë¦¬ ì¶œë ¥ ë¹„í™œì„±í™”
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True
        )
        self.async_session = sessionmaker(self.engine, class_=AsyncSession, expire_on_commit=False)

        # ìºì‹œ ë¡œë“œ (ì„ íƒì )
        try:
            await self.load_caches()
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë”© ì‹¤íŒ¨, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„: {e}")
            # ìºì‹œ ë¡œë”© ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    async def load_caches(self):
        """ì¹´í…Œê³ ë¦¬ì™€ ì¬ë£Œ ìºì‹œ ë¡œë“œ"""
        async with self.async_session() as session:
            # ì¹´í…Œê³ ë¦¬ ê¸°ëŠ¥ ì œê±° (ì‹¤ì œ DBì— í…Œì´ë¸” ì—†ìŒ)
            self.category_cache = {}

            # ì¬ë£Œ ìºì‹œ ë¡œë“œ (ì»¬ëŸ¼ëª… ìˆ˜ì •)
            try:
                result = await session.execute(select(Ingredient))
                ingredients = result.scalars().all()
                self.ingredient_cache = {ing.name: ing.id for ing in ingredients}
                logger.info(f"Loaded {len(self.ingredient_cache)} ingredients to cache")
            except Exception as e:
                logger.warning(f"ì¬ë£Œ ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.ingredient_cache = {}

    def detect_encoding(self, file_path):
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        with open(file_path, 'rb') as f:
            raw_data = f.read(100000)
            result = chardet.detect(raw_data)
            return result['encoding']

    def read_csv_file(self, file_path: Path) -> pd.DataFrame:
        """CSV íŒŒì¼ ì½ê¸°"""
        logger.info("ğŸ” íŒŒì¼ ì¸ì½”ë”© ê°ì§€ ì¤‘...")
        encoding = self.detect_encoding(file_path)
        
        # í•œêµ­ì–´ CSV íŒŒì¼ì— ì¼ë°˜ì ì¸ ì¸ì½”ë”©ë“¤
        encodings_to_try = ['EUC-KR', 'CP949', 'UTF-8', 'UTF-8-SIG']
        if encoding and encoding not in encodings_to_try:
            encodings_to_try.insert(0, encoding)
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path.name}")
        logger.info(f"ğŸ”¤ ì‹œë„í•  ì¸ì½”ë”©: {', '.join(encodings_to_try)}")
        
        for enc in encodings_to_try:
            try:
                logger.info(f"ğŸ”„ {enc} ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„ ì¤‘...")
                df = pd.read_csv(file_path, encoding=enc, on_bad_lines='skip')
                logger.info(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {enc} ì¸ì½”ë”© ì‚¬ìš©")
                logger.info(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(df):,}ê°œ í–‰, {len(df.columns)}ê°œ ì—´")
                logger.info(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
                return df
            except Exception as e:
                logger.warning(f"âŒ {enc} ì¸ì½”ë”© ì‹¤íŒ¨: {str(e)[:100]}...")
                continue

        # ë§ˆì§€ë§‰ ì‹œë„: Latin-1 (ëª¨ë“  ë°”ì´íŠ¸ë¥¼ í—ˆìš©)
        try:
            logger.warning("âš ï¸ ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨, Latin-1ìœ¼ë¡œ ê°•ì œ ì½ê¸° ì‹œë„...")
            df = pd.read_csv(file_path, encoding='latin-1', on_bad_lines='skip')
            logger.info(f"âœ… Latin-1ìœ¼ë¡œ ë¡œë“œ ì„±ê³µ: {len(df):,}ê°œ í–‰")
            logger.warning("âš ï¸ ì¼ë¶€ í•œê¸€ í…ìŠ¤íŠ¸ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return df
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ì½ê¸° ì‹¤íŒ¨: {e}")

            # ì •ë§ ë§ˆì§€ë§‰ ì‹œë„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í›„ ì½ê¸°
            try:
                logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í›„ ì¬ì‹œë„...")
                import tempfile

                # ì„ì‹œ íŒŒì¼ë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬
                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as temp_file:
                    with open(file_path, 'rb') as f:
                        content = f.read()
                        # ë¬¸ì œê°€ ë˜ëŠ” ë°”ì´íŠ¸ ì œê±°
                        content = content.replace(b'\x00', b'')  # NULL ë°”ì´íŠ¸ ì œê±°
                        content = content.replace(b'\x82', b'')  # ë¬¸ì œ ë°”ì´íŠ¸ ì œê±°
                        content = content.replace(b'\xc9', b'')
                        content = content.replace(b'\xbe', b'')

                    # UTF-8ë¡œ ë‹¤ì‹œ ì¸ì½”ë”© ì‹œë„
                    try:
                        text = content.decode('cp949', errors='ignore')
                        temp_file.write(text)
                    except:
                        text = content.decode('utf-8', errors='ignore')
                        temp_file.write(text)

                    temp_path = temp_file.name

                # ì •ë¦¬ëœ íŒŒì¼ ì½ê¸°
                df = pd.read_csv(temp_path, encoding='utf-8', on_bad_lines='skip')
                os.unlink(temp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                logger.info(f"âœ… ì „ì²˜ë¦¬ í›„ ë¡œë“œ ì„±ê³µ: {len(df):,}ê°œ í–‰")
                return df

            except Exception as e2:
                logger.error(f"âŒ ì „ì²˜ë¦¬ í›„ì—ë„ ì‹¤íŒ¨: {e2}")
                raise ValueError(f"Failed to read {file_path.name} with any encoding")

    async def migrate_file(self, file_path: Path):
        """ë‹¨ì¼ CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("=" * 60)
        logger.info(f"ğŸ“š CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        logger.info(f"ğŸ“„ íŒŒì¼: {file_path.name}")
        logger.info(f"ğŸ“ ê²½ë¡œ: {file_path}")
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ“Š í¬ê¸°: {file_size_mb:.2f} MB")
        logger.info("=" * 60)

        # CSV íŒŒì¼ ì½ê¸°
        df = self.read_csv_file(file_path)

        # ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ ì œí•œ
        if self.max_records:
            df = df.head(self.max_records)
            logger.info(f"âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.max_records:,}ê°œ ë ˆì½”ë“œë¡œ ì œí•œ")

        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ë§¤í•‘
        logger.info("ğŸ” ì»¬ëŸ¼ ë§¤í•‘ ê²€ìƒ‰ ì¤‘...")
        column_mapping = self.detect_columns(df)
        if not column_mapping:
            logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path.name}")
            return
        logger.info(f"âœ… ì»¬ëŸ¼ ë§¤í•‘ ì™„ë£Œ:")
        for key, value in column_mapping.items():
            logger.info(f"    - {key}: {value}")

        # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_chunks = (len(df) + self.chunk_size - 1) // self.chunk_size
        logger.info(f"ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘:")
        logger.info(f"    - ì´ ë ˆì½”ë“œ: {len(df):,}ê°œ")
        logger.info(f"    - ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
        logger.info(f"    - ì²­í¬ í¬ê¸°: {self.chunk_size}ê°œ")

        with tqdm(total=len(df), desc=f"Migrating {file_path.name}") as pbar:
            for chunk_idx in range(0, len(df), self.chunk_size):
                chunk = df.iloc[chunk_idx:chunk_idx + self.chunk_size]
                chunk_num = (chunk_idx // self.chunk_size) + 1
                logger.debug(f"ğŸ“¦ ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
                await self.process_chunk(chunk, column_mapping)
                pbar.update(len(chunk))

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if chunk_idx % (self.chunk_size * 10) == 0:
                    await asyncio.sleep(0.1)  # ë‹¤ë¥¸ ì‘ì—…ì— CPU ì–‘ë³´

    def detect_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ ê°ì§€"""
        column_mapping = {}

        logger.info(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤: {', '.join(df.columns)}")

        # ì‹¤ì œ CSV ì»¬ëŸ¼ëª…ì— ë§ê²Œ ë§¤í•‘
        column_patterns = {
            'title': ['RCP_TTL', 'RCP_NM', 'ë ˆì‹œí”¼', 'ì œëª©'],
            'ingredients': ['CKG_MTRL_CN', 'RCP_PARTS_DTLS', 'ì¬ë£Œ', 'ì‹ì¬ë£Œ'],
            'cooking_method': ['CKG_MTH_ACTO_NM', 'RCP_WAY2', 'ìš”ë¦¬ë°©ë²•', 'ì¡°ë¦¬ë°©ë²•'],
            'category': ['CKG_KND_ACTO_NM', 'ìš”ë¦¬ì¢…ë¥˜'],
            'difficulty': ['CKG_DODF_NM', 'ë‚œì´ë„'],
            'time': ['CKG_TIME_NM', 'ì¡°ë¦¬ì‹œê°„'],
            'servings': ['CKG_INBUN_NM', 'ì¸ë¶„'],
            'image_url': ['ATT_FILE_NO_MAIN', 'IMG', 'ì´ë¯¸ì§€']
        }

        # ê° í•„ë“œë³„ë¡œ ì»¬ëŸ¼ ì°¾ê¸°
        for field, patterns in column_patterns.items():
            for pattern in patterns:
                for col in df.columns:
                    if pattern in col:
                        column_mapping[field] = col
                        logger.info(f"âœ… {field} ë§¤í•‘: {col}")
                        break
                if field in column_mapping:
                    break

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (titleë§Œ í•„ìˆ˜)
        if 'title' not in column_mapping:
            logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ 'title'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            return None

        logger.info(f"ğŸ—ºï¸ ìµœì¢… ë§¤í•‘: {column_mapping}")
        return column_mapping

    async def process_chunk(self, chunk: pd.DataFrame, column_mapping: Dict[str, str]):
        """ë°ì´í„° ì²­í¬ ì²˜ë¦¬ - ê°œë³„ ë ˆì‹œí”¼ë§ˆë‹¤ ë³„ë„ íŠ¸ëœì­ì…˜ ì‚¬ìš©"""
        successful_count = 0
        error_count = 0

        for idx, row in chunk.iterrows():
            async with self.async_session() as session:
                try:
                    # ì„¸ì…˜ì—ì„œ íŠ¸ëœì­ì…˜ ëª…ì‹œì  ì‹œì‘
                    async with session.begin():
                        await self.process_recipe(session, row, column_mapping)
                    successful_count += 1

                except Exception as e:
                    # ì—ëŸ¬ ì •ë³´ ë” ìì„¸íˆ ë¡œê¹…
                    recipe_info = ""
                    try:
                        if 'title' in column_mapping and pd.notna(row[column_mapping['title']]):
                            recipe_info = f" (Recipe: {str(row[column_mapping['title']])[:50]})"
                    except:
                        pass

                    logger.error(f"Error processing recipe at index {idx}{recipe_info}: {e}")
                    error_count += 1

        self.stats['total_processed'] += successful_count
        self.stats['errors'] += error_count

        if successful_count > 0:
            logger.info(f"âœ… ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {successful_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")

    async def process_recipe(self, session: AsyncSession, row: pd.Series, column_mapping: Dict[str, str]):
        """ë‹¨ì¼ ë ˆì‹œí”¼ ì²˜ë¦¬"""
        # ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ
        title = str(row[column_mapping['title']]) if pd.notna(row[column_mapping['title']]) else None
        if not title:
            return

        # ì¤‘ë³µ ì²´í¬ (ì œëª© ê¸°ì¤€)
        result = await session.execute(
            select(Recipe).where(Recipe.rcp_ttl == title)
        )
        existing = result.scalar_one_or_none()
        if existing:
            self.stats['duplicates_skipped'] += 1
            return

        # ë ˆì‹œí”¼ ìƒì„± (ì‹¤ì œ DB ì»¬ëŸ¼ëª… ì‚¬ìš©)
        recipe = Recipe(
            rcp_ttl=title,
            rcp_img_url=str(row[column_mapping['image_url']]) if 'image_url' in column_mapping and pd.notna(row[column_mapping['image_url']]) else None,
            ckg_mth_acto_nm=str(row[column_mapping['cooking_method']]) if 'cooking_method' in column_mapping and pd.notna(row[column_mapping['cooking_method']]) else None,
            ckg_ipdc=str(row[column_mapping.get('description', '')]) if 'description' in column_mapping and pd.notna(row[column_mapping['description']]) else None,
            ckg_inbun_nm=str(row[column_mapping.get('servings', '')]) if 'servings' in column_mapping and pd.notna(row[column_mapping['servings']]) else None,
            ckg_dodf_nm=str(row[column_mapping.get('difficulty', '')]) if 'difficulty' in column_mapping and pd.notna(row[column_mapping['difficulty']]) else None,
            ckg_time_nm=str(row[column_mapping.get('time', '')]) if 'time' in column_mapping and pd.notna(row[column_mapping['time']]) else None,
            ckg_knd_acto_nm=str(row[column_mapping.get('category', '')]) if 'category' in column_mapping and pd.notna(row[column_mapping['category']]) else None,
        )
        session.add(recipe)
        await session.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
        self.stats['recipes_created'] += 1

        # ì¬ë£Œ ì²˜ë¦¬
        if 'ingredients' in column_mapping and pd.notna(row[column_mapping['ingredients']]):
            ingredients_text = str(row[column_mapping['ingredients']])
            await self.process_ingredients(session, recipe.rcp_sno, ingredients_text)

    async def process_ingredients(self, session: AsyncSession, recipe_id: int, ingredients_text: str):
        """ì¬ë£Œ ì²˜ë¦¬"""
        parsed_ingredients = parse_ingredients_list(ingredients_text)

        for parsed_ing in parsed_ingredients:
            try:
                # ì¬ë£Œ ì°¾ê¸° ë˜ëŠ” ìƒì„±
                ingredient_id = await self.get_or_create_ingredient(
                    session,
                    parsed_ing.name,  # normalized_name ì œê±°
                    parsed_ing.name
                )

                # ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²° ìƒì„± (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)
                recipe_ingredient = RecipeIngredient(
                    rcp_sno=recipe_id,  # Recipeì˜ ì‹¤ì œ PK ì‚¬ìš©
                    ingredient_id=ingredient_id,
                    quantity_text=parsed_ing.original_text if hasattr(parsed_ing, 'original_text') else None,
                    quantity_from=parsed_ing.quantity_from,
                    quantity_to=parsed_ing.quantity_to,
                    unit=parsed_ing.unit,
                    is_vague=getattr(parsed_ing, 'is_vague', False),
                    display_order=0,
                    importance=getattr(parsed_ing, 'importance', 'normal')
                )
                session.add(recipe_ingredient)
                self.stats['recipe_ingredients_created'] += 1

            except Exception as e:
                logger.error(f"Error processing ingredient '{parsed_ing.name}': {e}")

    async def get_or_create_ingredient(self, session: AsyncSession, original_name: str, _unused: str) -> int:
        """ì¬ë£Œ ì°¾ê¸° ë˜ëŠ” ìƒì„± (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)"""
        # ìºì‹œì—ì„œ í™•ì¸ (name ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½)
        if original_name in self.ingredient_cache:
            return self.ingredient_cache[original_name]

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸ (name ì»¬ëŸ¼ ì‚¬ìš©)
        result = await session.execute(
            select(Ingredient).where(Ingredient.name == original_name)
        )
        ingredient = result.scalar_one_or_none()

        if ingredient:
            self.ingredient_cache[original_name] = ingredient.id
            return ingredient.id

        # ìƒˆ ì¬ë£Œ ìƒì„± (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)
        ingredient = Ingredient(
            name=original_name,
            original_name=original_name,
            is_common=False
        )
        session.add(ingredient)
        await session.flush()

        self.ingredient_cache[original_name] = ingredient.id
        self.stats['ingredients_created'] += 1

        return ingredient.id

    async def print_statistics(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„ ì¶œë ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„")
        logger.info("="*60)
        logger.info(f"ì´ ì²˜ë¦¬ ë ˆì½”ë“œ: {self.stats['total_processed']:,}")
        logger.info(f"ìƒì„±ëœ ë ˆì‹œí”¼: {self.stats['recipes_created']:,}")
        logger.info(f"ìƒì„±ëœ ì¬ë£Œ: {self.stats['ingredients_created']:,}")
        logger.info(f"ìƒì„±ëœ ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²°: {self.stats['recipe_ingredients_created']:,}")
        logger.info(f"ê±´ë„ˆë›´ ì¤‘ë³µ: {self.stats['duplicates_skipped']:,}")
        logger.info(f"ì˜¤ë¥˜: {self.stats['errors']:,}")

        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        async with self.async_session() as session:
            result = await session.execute(text("SELECT COUNT(*) FROM recipes"))
            total_recipes = result.scalar()
            result = await session.execute(text("SELECT COUNT(*) FROM ingredients"))
            total_ingredients = result.scalar()
            result = await session.execute(text("SELECT COUNT(*) FROM recipe_ingredients"))
            total_relations = result.scalar()

            logger.info("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©")
            logger.info(f"ì´ ë ˆì‹œí”¼ ìˆ˜: {total_recipes:,}")
            logger.info(f"ì´ ì¬ë£Œ ìˆ˜: {total_ingredients:,}")
            logger.info(f"ì´ ë ˆì‹œí”¼-ì¬ë£Œ ì—°ê²° ìˆ˜: {total_relations:,}")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.engine:
            await self.engine.dispose()


async def main(args):
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("\n" + "="*70)
    logger.info("ğŸš€ ë ˆì‹œí”¼ CSV ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    logger.info("="*70)

    # ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™”
    logger.info("ğŸ­ ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™” ì¤‘...")
    migrator = CSVDataMigrator(
        chunk_size=args.chunk_size,
        max_records=args.max_records
    )
    logger.info(f"    - ì²­í¬ í¬ê¸°: {args.chunk_size}")
    if args.max_records:
        logger.info(f"    - ìµœëŒ€ ë ˆì½”ë“œ: {args.max_records:,}")

    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        logger.info("ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        await migrator.initialize()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # CSV íŒŒì¼ ì°¾ê¸°
        logger.info("ğŸ” CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        datas_dir = project_root / "datas"
        logger.info(f"    - ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {datas_dir}")
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not datas_dir.exists():
            logger.error(f"âŒ datas ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {datas_dir}")
            logger.info("í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ë‚´ìš©:")
            for item in project_root.iterdir():
                logger.info(f"    - {item.name} ({'ë””ë ‰í† ë¦¬' if item.is_dir() else 'íŒŒì¼'})")
            return
        
        # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
        logger.info(f"datas ë””ë ‰í† ë¦¬ ë‚´ìš©:")
        for item in datas_dir.iterdir():
            logger.info(f"    - {item.name} ({'ë””ë ‰í† ë¦¬' if item.is_dir() else 'íŒŒì¼'})")
        
        # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ CSV íŒŒì¼ ê²€ìƒ‰
        csv_patterns = [
            "TB_RECIPE_SEARCH*.csv",
            "*.csv"
        ]
        
        csv_files = []
        for pattern in csv_patterns:
            found_files = sorted(datas_dir.glob(pattern))
            csv_files.extend(found_files)
            if found_files:
                logger.info(f"    - íŒ¨í„´ '{pattern}'ìœ¼ë¡œ {len(found_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        csv_files = sorted(list(set(csv_files)))

        if not csv_files:
            logger.error("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            logger.error(f"    - ê²€ìƒ‰ ê²½ë¡œ: {datas_dir}")
            logger.error("    - íŒ¨í„´: TB_RECIPE_SEARCH*.csv")
            return 1

        logger.info(f"âœ… {len(csv_files)}ê°œ CSV íŒŒì¼ ë°œê²¬:")
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"    {i}. {csv_file.name} ({csv_file.stat().st_size / (1024*1024):.2f} MB)")

        # ê° íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
        logger.info("\nğŸ”„ CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
        for idx, csv_file in enumerate(csv_files, 1):
            logger.info(f"\n[íŒŒì¼ {idx}/{len(csv_files)}] {csv_file.name} ì²˜ë¦¬ ì¤‘...")
            await migrator.migrate_file(csv_file)
            logger.info(f"âœ… [íŒŒì¼ {idx}/{len(csv_files)}] {csv_file.name} ì²˜ë¦¬ ì™„ë£Œ")

        # í†µê³„ ì¶œë ¥
        logger.info("\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„ ì¶œë ¥ ì¤‘...")
        await migrator.print_statistics()

        logger.info("\n" + "="*70)
        logger.info("ğŸ‰ ëª¨ë“  CSV íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ!")
        logger.info("="*70)
        return 0

    except Exception as e:
        logger.error("\n" + "="*70)
        logger.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨")
        logger.error(f"ğŸ”¥ ì˜¤ë¥˜: {e}")
        logger.error("="*70)
        import traceback
        logger.error(traceback.format_exc())
        return 1

    finally:
        await migrator.cleanup()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Migrate CSV data to PostgreSQL")
    parser.add_argument("--chunk-size", type=int, default=100,
                       help="Number of records to process in each batch")
    parser.add_argument("--max-records", type=int, default=None,
                       help="Maximum number of records to process (for testing)")

    args = parser.parse_args()

    sys.exit(asyncio.run(main(args)))