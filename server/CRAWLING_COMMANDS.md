# ğŸš€ Fridge2Fork í¬ë¡¤ë§ ëª…ë ¹ì–´ ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê¸°ë³¸ í¬ë¡¤ë§ ëª…ë ¹ì–´](#ê¸°ë³¸-í¬ë¡¤ë§-ëª…ë ¹ì–´)
2. [ì„±ëŠ¥ ìµœì í™” í¬ë¡¤ë§](#ì„±ëŠ¥-ìµœì í™”-í¬ë¡¤ë§)
3. [ì „ì²´ í¬ë¡¤ë§ ëª…ë ¹ì–´](#ì „ì²´-í¬ë¡¤ë§-ëª…ë ¹ì–´)
4. [ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬](#ëª¨ë‹ˆí„°ë§-ë°-ê´€ë¦¬)
5. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ğŸ¯ ê¸°ë³¸ í¬ë¡¤ë§ ëª…ë ¹ì–´

### í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„
```bash
# Conda í™˜ê²½ í™œì„±í™”
conda activate fridge2fork

# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì´ë™
cd /Users/woohyeon/woohalabs/fridge2fork/server

# ì˜ì¡´ì„± í™•ì¸
pip list | grep -E "(asyncio|aiohttp|beautifulsoup4)"
```

### ê¸°ë³¸ ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (100ê°œ ë ˆì‹œí”¼)
python scripts/run_crawling.py --target 100

# ğŸ“Š ì¤‘ê°„ ê·œëª¨ (1,000ê°œ ë ˆì‹œí”¼)
python scripts/run_crawling.py --target 1000

# ğŸ¯ ëª©í‘œ ë‹¬ì„± (10,000ê°œ ë ˆì‹œí”¼)
python scripts/run_crawling.py --target 10000

# âš™ï¸ ì»¤ìŠ¤í…€ ì„¤ì •
python scripts/run_crawling.py --target 5000 --batch-size 75 --delay 1.2
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” í¬ë¡¤ë§

### ë‹¨ê³„ë³„ ìµœì í™” í¬ë¡¤ë§
```bash
# ğŸ§ª 1ë‹¨ê³„: ì•ˆì „ í…ŒìŠ¤íŠ¸ (100ê°œ, 1.5ì´ˆ ë”œë ˆì´)
python scripts/optimized_crawling.py --test

# âš¡ 2ë‹¨ê³„: ë¹ ë¥¸ í¬ë¡¤ë§ (1,000ê°œ, 0.5ì´ˆ ë”œë ˆì´)
python scripts/optimized_crawling.py --fast --target 1000

# ğŸš€ 3ë‹¨ê³„: í„°ë³´ í¬ë¡¤ë§ (5,000ê°œ, 0.2ì´ˆ ë”œë ˆì´)
python scripts/optimized_crawling.py --turbo --target 5000

# ğŸ”¥ 4ë‹¨ê³„: ìµœëŒ€ ì„±ëŠ¥ (10,000ê°œ, 0.2ì´ˆ ë”œë ˆì´, 200ê°œ ë°°ì¹˜)
python scripts/optimized_crawling.py --turbo --batch-large --target 10000
```

### ëª¨ë“œë³„ ìµœì í™” ëª…ë ¹ì–´
```bash
# ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ (ì„œë²„ ë¶€í•˜ ìµœì†Œ)
python scripts/optimized_crawling.py --safe --target 10000

# âš–ï¸ ê· í˜• ëª¨ë“œ (ì„±ëŠ¥ê³¼ ì•ˆì •ì„± ê· í˜•)
python scripts/optimized_crawling.py --normal --target 5000

# ğŸš€ ê³ ì„±ëŠ¥ ëª¨ë“œ (ë¹ ë¥¸ í¬ë¡¤ë§)
python scripts/optimized_crawling.py --fast --target 8000

# ğŸ”¥ ê·¹í•œ ëª¨ë“œ (ìµœëŒ€ ì„±ëŠ¥, ì£¼ì˜ í•„ìš”)
python scripts/optimized_crawling.py --extreme --target 20000
```

---

## ğŸ¯ ì „ì²´ í¬ë¡¤ë§ ëª…ë ¹ì–´ (10,000ê°œ+ ë ˆì‹œí”¼)

### ğŸ† ì¶”ì²œ ì „ì²´ í¬ë¡¤ë§ ëª…ë ¹ì–´

#### ë°©ë²• 1: ì•ˆì „í•œ ëŒ€ëŸ‰ í¬ë¡¤ë§ (ê¶Œì¥)
```bash
# 3-4ì‹œê°„ ì†Œìš”, ì„œë²„ ë¶€í•˜ ìµœì†Œ, IP ì°¨ë‹¨ ìœ„í—˜ ë‚®ìŒ
python scripts/optimized_crawling.py --safe --target 10000
```

#### ë°©ë²• 2: ê· í˜•ì¡íŒ ëŒ€ëŸ‰ í¬ë¡¤ë§
```bash
# 1.5-2ì‹œê°„ ì†Œìš”, ì ë‹¹í•œ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±
python scripts/optimized_crawling.py --normal --target 10000
```

#### ë°©ë²• 3: ê³ ì† ëŒ€ëŸ‰ í¬ë¡¤ë§
```bash
# 1-1.5ì‹œê°„ ì†Œìš”, ë†’ì€ ì„±ëŠ¥, ëª¨ë‹ˆí„°ë§ í•„ìš”
python scripts/optimized_crawling.py --turbo --batch-large --target 10000
```

#### ë°©ë²• 4: ë‹¨ê³„ì  ëŒ€ëŸ‰ í¬ë¡¤ë§ (ìµœê³  ì•ˆì •ì„±)
```bash
# 1ë‹¨ê³„: 2,000ê°œ
python scripts/optimized_crawling.py --fast --target 2000

# 2ë‹¨ê³„: ì¶”ê°€ 3,000ê°œ (ì´ 5,000ê°œ)
python scripts/optimized_crawling.py --fast --target 3000

# 3ë‹¨ê³„: ì¶”ê°€ 5,000ê°œ (ì´ 10,000ê°œ)
python scripts/optimized_crawling.py --turbo --target 5000
```

### ğŸ”¥ ê·¹í•œ ì„±ëŠ¥ í¬ë¡¤ë§ (ê³ ê¸‰ ì‚¬ìš©ììš©)
```bash
# âš ï¸ ì£¼ì˜: ë†’ì€ ì„œë²„ ë¶€í•˜, IP ì°¨ë‹¨ ìœ„í—˜
python scripts/optimized_crawling.py --extreme --target 15000

# ğŸš¨ ìµœëŒ€ ì„±ëŠ¥: 20,000ê°œ ë ˆì‹œí”¼ (ë§¤ìš° ìœ„í—˜)
python scripts/optimized_crawling.py --custom \
  --target 20000 \
  --delay 0.1 \
  --batch-size 250
```

### ğŸ“Š ëŒ€ìš©ëŸ‰ í¬ë¡¤ë§ ì „ëµ

#### ì „ëµ A: ì‹œê°„ ìš°ì„  (ë¹ ë¥¸ ì™„ë£Œ)
```bash
# ì˜ˆìƒ ì‹œê°„: 45ë¶„-1ì‹œê°„
python scripts/optimized_crawling.py --turbo --batch-large --target 10000
```

#### ì „ëµ B: ì•ˆì •ì„± ìš°ì„  (ì•ˆì „í•œ ì™„ë£Œ)
```bash
# ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„
python scripts/optimized_crawling.py --safe --target 10000
```

#### ì „ëµ C: ë¶„í•  í¬ë¡¤ë§ (ìµœê³  ì•ˆì •ì„±)
```bash
# ì¹´í…Œê³ ë¦¬ë³„ ë¶„í•  í¬ë¡¤ë§ (ê°ê° 1,500-2,000ê°œì”©)
python scripts/run_crawling.py --target 2000 --delay 1.0  # 1ì°¨
sleep 1800  # 30ë¶„ íœ´ì‹
python scripts/run_crawling.py --target 2000 --delay 1.0  # 2ì°¨
sleep 1800  # 30ë¶„ íœ´ì‹
python scripts/run_crawling.py --target 2000 --delay 1.0  # 3ì°¨
sleep 1800  # 30ë¶„ íœ´ì‹
python scripts/run_crawling.py --target 2000 --delay 1.0  # 4ì°¨
sleep 1800  # 30ë¶„ íœ´ì‹
python scripts/run_crawling.py --target 2000 --delay 1.0  # 5ì°¨ (ì´ 10,000ê°œ)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```bash
# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f crawling.log

# ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ (ë³„ë„ í„°ë¯¸ë„)
watch -n 30 'echo "=== $(date) ===" && python -c "
import asyncio
import sys
sys.path.append(\".\")
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(f\"ì´ ë ˆì‹œí”¼: {stats.get(\"total_recipes\", 0):,}ê°œ\")
print(f\"ì´ ì¬ë£Œ: {stats.get(\"total_ingredients\", 0):,}ê°œ\")
for cat, count in stats.get(\"category_breakdown\", {}).items():
    print(f\"  - {cat}: {count:,}ê°œ\")
"'
```

### ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
```bash
# í˜„ì¬ ì €ì¥ëœ ë ˆì‹œí”¼ ìˆ˜ í™•ì¸
python -c "
import asyncio
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print('ğŸ“Š í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ:')
print(f'  â€¢ ì´ ë ˆì‹œí”¼: {stats.get(\"total_recipes\", 0):,}ê°œ')
print(f'  â€¢ ì´ ì¬ë£Œ: {stats.get(\"total_ingredients\", 0):,}ê°œ')
print('  â€¢ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:')
for category, count in stats.get('category_breakdown', {}).items():
    print(f'    - {category}: {count:,}ê°œ')
"

# Supabase ì§ì ‘ í™•ì¸
python -c "
import sys
sys.path.append('.')
# MCP Supabase í•¨ìˆ˜ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
print('Supabase ì—°ê²° í…ŒìŠ¤íŠ¸ í•„ìš”')
"
```

### ì„±ëŠ¥ ì¸¡ì •
```bash
# í¬ë¡¤ë§ ì†ë„ ì¸¡ì • (1ë¶„ê°„)
echo "í¬ë¡¤ë§ ì†ë„ ì¸¡ì • ì‹œì‘..."
BEFORE=$(python -c "
import asyncio
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(stats.get('total_recipes', 0))
")

sleep 60

AFTER=$(python -c "
import asyncio
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(stats.get('total_recipes', 0))
")

echo "1ë¶„ê°„ í¬ë¡¤ë§ëœ ë ˆì‹œí”¼: $((AFTER - BEFORE))ê°œ"
echo "ì‹œê°„ë‹¹ ì˜ˆìƒ í¬ë¡¤ë§: $(($(($AFTER - $BEFORE)) * 60))ê°œ"
```

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### í¬ë¡¤ë§ ì¤‘ë‹¨ ì‹œ ì¬ì‹œì‘
```bash
# í˜„ì¬ ìƒíƒœ í™•ì¸ í›„ ë¶€ì¡±í•œ ë§Œí¼ ì¶”ê°€ í¬ë¡¤ë§
CURRENT=$(python -c "
import asyncio
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(stats.get('total_recipes', 0))
")

TARGET=10000
REMAINING=$((TARGET - CURRENT))

if [ $REMAINING -gt 0 ]; then
    echo "ì¶”ê°€ë¡œ $REMAINING ê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ í•„ìš”"
    python scripts/optimized_crawling.py --fast --target $REMAINING
else
    echo "ëª©í‘œ ë‹¬ì„±! í˜„ì¬ $CURRENT ê°œ ë ˆì‹œí”¼ ì €ì¥ë¨"
fi
```

### ì˜¤ë¥˜ ë°œìƒ ì‹œ ë³µêµ¬
```bash
# í¬ë¡¤ë§ ë¡œê·¸ í™•ì¸
grep -i error crawling.log | tail -10

# ì‹¤íŒ¨í•œ ë ˆì‹œí”¼ ì¬ì‹œë„
python scripts/run_crawling.py --target 100 --delay 2.0

# ë°ì´í„°ë² ì´ìŠ¤ ì •í•©ì„± í™•ì¸
python -c "
import asyncio
from scripts.crawling.database import recipe_storage

async def check_integrity():
    # ë ˆì‹œí”¼ ìˆ˜ í™•ì¸
    recipes = await recipe_storage.supabase_client.execute_sql('SELECT COUNT(*) FROM recipes')
    
    # ì¬ë£Œ ì—†ëŠ” ë ˆì‹œí”¼ í™•ì¸
    orphan_recipes = await recipe_storage.supabase_client.execute_sql('''
        SELECT r.name FROM recipes r 
        LEFT JOIN recipe_ingredients ri ON r.id = ri.recipe_id 
        WHERE ri.recipe_id IS NULL
    ''')
    
    print(f'ì´ ë ˆì‹œí”¼: {recipes[0][\"count\"] if recipes else 0}ê°œ')
    print(f'ì¬ë£Œ ì—†ëŠ” ë ˆì‹œí”¼: {len(orphan_recipes)}ê°œ')

asyncio.run(check_integrity())
"
```

### ì„±ëŠ¥ íŠœë‹
```bash
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
top -p $(pgrep -f "python.*crawling")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
ps aux | grep python | grep crawling

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ
netstat -an | grep 443 | wc -l
```

---

## ğŸ¯ ìµœì¢… ì¶”ì²œ ëª…ë ¹ì–´

### ğŸ¥‡ ì´ˆë³´ììš© (ì•ˆì „í•˜ê³  í™•ì‹¤í•¨)
```bash
# ë‹¨ê³„ì  ì ‘ê·¼
python scripts/optimized_crawling.py --test        # 100ê°œ í…ŒìŠ¤íŠ¸
python scripts/optimized_crawling.py --safe --target 1000   # 1,000ê°œ
python scripts/optimized_crawling.py --safe --target 5000   # ì¶”ê°€ 4,000ê°œ
python scripts/optimized_crawling.py --safe --target 5000   # ì¶”ê°€ 5,000ê°œ (ì´ 10,000ê°œ)
```

### ğŸ¥ˆ ì¤‘ê¸‰ììš© (ê· í˜•ì¡íŒ ì„±ëŠ¥)
```bash
# íš¨ìœ¨ì ì¸ ëŒ€ëŸ‰ í¬ë¡¤ë§
python scripts/optimized_crawling.py --normal --target 10000
```

### ğŸ¥‰ ê³ ê¸‰ììš© (ìµœê³  ì„±ëŠ¥)
```bash
# ê³ ì† ëŒ€ëŸ‰ í¬ë¡¤ë§ (ëª¨ë‹ˆí„°ë§ í•„ìˆ˜)
python scripts/optimized_crawling.py --turbo --batch-large --target 10000
```

### ğŸ† ì „ë¬¸ê°€ìš© (ê·¹í•œ ì„±ëŠ¥)
```bash
# ìµœëŒ€ ì„±ëŠ¥ í¬ë¡¤ë§ (ìœ„í—˜ ìˆ˜ì¤€ ë†’ìŒ)
python scripts/optimized_crawling.py --extreme --target 15000
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì„œë²„ ë¶€í•˜**: ë”œë ˆì´ê°€ 0.5ì´ˆ ë¯¸ë§Œì¼ ë•ŒëŠ” ì„œë²„ ë¶€í•˜ ì£¼ì˜
2. **IP ì°¨ë‹¨**: ë„ˆë¬´ ë¹ ë¥¸ í¬ë¡¤ë§ ì‹œ IP ì°¨ë‹¨ ìœ„í—˜
3. **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
4. **ë°ì´í„° í’ˆì§ˆ**: ë¹ ë¥¸ í¬ë¡¤ë§ì¼ìˆ˜ë¡ ë°ì´í„° ê²€ì¦ ê°•í™” í•„ìš”
5. **ë°±ì—…**: í¬ë¡¤ë§ ì „ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ê¶Œì¥

---

## ğŸ‰ ì„±ê³µ ê¸°ì¤€

- âœ… **10,000ê°œ ì´ìƒ** ë ˆì‹œí”¼ ìˆ˜ì§‘
- âœ… **95% ì´ìƒ** ë°ì´í„° ì™„ì„±ë„
- âœ… **ì¤‘ë³µ ì œê±°** ì™„ë£Œ
- âœ… **ì¬ë£Œ í‘œì¤€í™”** ì™„ë£Œ
- âœ… **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜** ì •í™•ë„ 90% ì´ìƒ

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 9ì›” 22ì¼*

