# ğŸš€ Fridge2Fork ê¶ê·¹ì˜ í¬ë¡¤ë§ ëª…ë ¹ì–´ ê°€ì´ë“œ
## 20ë§Œê°œ ë ˆì‹œí”¼ ì™„ì „ ì •ë³µ - ì‹œê°„ ë¬´ì œí•œ ë²„ì „

---

## ğŸ“‹ ëª©ì°¨
1. [ê¶ê·¹ì˜ ì›í´ë¦­ ëª…ë ¹ì–´](#ê¶ê·¹ì˜-ì›í´ë¦­-ëª…ë ¹ì–´)
2. [24ì‹œê°„ ë¬´ì¤‘ë‹¨ í¬ë¡¤ë§](#24ì‹œê°„-ë¬´ì¤‘ë‹¨-í¬ë¡¤ë§)
3. [ì™„ì „ ìë™í™” ìŠ¤í¬ë¦½íŠ¸](#ì™„ì „-ìë™í™”-ìŠ¤í¬ë¦½íŠ¸)
4. [ë‹¨ê³„ë³„ ë§ˆë¼í†¤ í¬ë¡¤ë§](#ë‹¨ê³„ë³„-ë§ˆë¼í†¤-í¬ë¡¤ë§)
5. [ë³‘ë ¬ ì²˜ë¦¬ ê³ ì† í¬ë¡¤ë§](#ë³‘ë ¬-ì²˜ë¦¬-ê³ ì†-í¬ë¡¤ë§)
6. [ë³µêµ¬ ë° ì¬ì‹œì‘ ëª…ë ¹ì–´](#ë³µêµ¬-ë°-ì¬ì‹œì‘-ëª…ë ¹ì–´)

---

## ğŸ† ê¶ê·¹ì˜ ì›í´ë¦­ ëª…ë ¹ì–´

### ğŸ¯ **ê°€ì¥ ì¶”ì²œí•˜ëŠ” 20ë§Œê°œ ì™„ì „ í¬ë¡¤ë§ ëª…ë ¹ì–´**

```bash
#!/bin/bash
# ğŸ”¥ ê¶ê·¹ì˜ 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ - ì›í´ë¦­ ì‹¤í–‰

# í™˜ê²½ ì¤€ë¹„
conda activate fridge2fork
cd /Users/woohyeon/woohalabs/fridge2fork/server

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
echo "ğŸš€ $(date): 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œì‘!"

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬´ì¤‘ë‹¨ ì‹¤í–‰
nohup python scripts/massive_crawling.py \
  --target 200000 \
  --batch-size 10000 \
  --resume \
  > ultimate_crawling_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# í”„ë¡œì„¸ìŠ¤ ID ì €ì¥
echo $! > crawling.pid

echo "âœ… í¬ë¡¤ë§ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "ğŸ“Š ì§„í–‰ìƒí™©: tail -f ultimate_crawling_*.log"
echo "ğŸ›‘ ì¤‘ì§€í•˜ë ¤ë©´: kill -TERM $(cat crawling.pid)"
```

### ğŸ”¥ **ê·¹í•œ ì„±ëŠ¥ 20ë§Œê°œ í¬ë¡¤ë§**

```bash
# ìµœëŒ€ ì„±ëŠ¥ìœ¼ë¡œ 3-5ì¼ ë‚´ ì™„ë£Œ (ìœ„í—˜ ìˆ˜ì¤€ ë†’ìŒ)
conda activate fridge2fork

python scripts/massive_crawling.py \
  --target 200000 \
  --batch-size 25000 \
  --fresh
```

### ğŸ›¡ï¸ **ì•ˆì „ ëª¨ë“œ 20ë§Œê°œ í¬ë¡¤ë§**

```bash
# ì•ˆì „í•˜ê²Œ 15-20ì¼ ë‚´ ì™„ë£Œ (ê¶Œì¥)
conda activate fridge2fork

python scripts/massive_crawling.py \
  --target 200000 \
  --batch-size 5000 \
  --resume
```

---

## ğŸ• 24ì‹œê°„ ë¬´ì¤‘ë‹¨ í¬ë¡¤ë§

### ğŸŒ™ **ì•¼ê°„ ë¬´ì¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ**

```bash
#!/bin/bash
# 24ì‹œê°„ ë¬´ì¤‘ë‹¨ í¬ë¡¤ë§ + ìë™ ì¬ì‹œì‘

while true; do
    echo "ğŸŒŸ $(date): í¬ë¡¤ë§ ë°°ì¹˜ ì‹œì‘"
    
    # í˜„ì¬ DB ìƒíƒœ í™•ì¸
    CURRENT=$(python -c "
import asyncio
import sys
sys.path.append('.')
from scripts.crawling.database import recipe_storage
try:
    stats = asyncio.run(recipe_storage.get_crawling_stats())
    print(stats.get('total_recipes', 0))
except:
    print(0)
")
    
    echo "ğŸ“Š í˜„ì¬ DB: ${CURRENT}ê°œ ë ˆì‹œí”¼"
    
    # ëª©í‘œ ë‹¬ì„± í™•ì¸
    if [ "$CURRENT" -ge 200000 ]; then
        echo "ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì´ ${CURRENT}ê°œ ë ˆì‹œí”¼ ìˆ˜ì§‘ ì™„ë£Œ"
        break
    fi
    
    # ë‚¨ì€ ìˆ˜ëŸ‰ ê³„ì‚°
    REMAINING=$((200000 - CURRENT))
    BATCH_SIZE=10000
    
    if [ "$REMAINING" -lt "$BATCH_SIZE" ]; then
        BATCH_SIZE=$REMAINING
    fi
    
    echo "ğŸ¯ ì´ë²ˆ ë°°ì¹˜ ëª©í‘œ: ${BATCH_SIZE}ê°œ"
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    timeout 7200 python scripts/optimized_crawling.py \
      --turbo \
      --target $BATCH_SIZE \
      --batch-large
    
    EXIT_CODE=$?
    
    if [ $EXIT_CODE -eq 0 ]; then
        echo "âœ… ë°°ì¹˜ ì™„ë£Œ"
    elif [ $EXIT_CODE -eq 124 ]; then
        echo "â° íƒ€ì„ì•„ì›ƒ - ë‹¤ìŒ ë°°ì¹˜ë¡œ ì§„í–‰"
    else
        echo "âŒ ì˜¤ë¥˜ ë°œìƒ - 30ì´ˆ í›„ ì¬ì‹œë„"
        sleep 30
    fi
    
    # ë°°ì¹˜ ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
    echo "ğŸ˜´ 60ì´ˆ íœ´ì‹..."
    sleep 60
    
done

echo "ğŸŠ 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì™„ì „ ì™„ë£Œ!"
```

### ğŸ”„ **ìë™ ì¬ì‹œì‘ í¬ë¡¤ë§**

```bash
#!/bin/bash
# ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œì‘í•˜ëŠ” ë¬´í•œ í¬ë¡¤ë§

MAX_RETRIES=100
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    echo "ğŸ”„ ì‹œë„ $((RETRY_COUNT + 1))/$MAX_RETRIES"
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    python scripts/massive_crawling.py \
      --target 200000 \
      --batch-size 15000 \
      --resume
    
    EXIT_CODE=$?
    
    if [ $EXIT_CODE -eq 0 ]; then
        echo "ğŸ‰ í¬ë¡¤ë§ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!"
        break
    else
        echo "âŒ ì˜¤ë¥˜ ë°œìƒ (ì¢…ë£Œ ì½”ë“œ: $EXIT_CODE)"
        RETRY_COUNT=$((RETRY_COUNT + 1))
        
        # ì¬ì‹œì‘ ì „ ëŒ€ê¸° ì‹œê°„ (ì§€ìˆ˜ì  ì¦ê°€)
        WAIT_TIME=$((60 * RETRY_COUNT))
        if [ $WAIT_TIME -gt 3600 ]; then
            WAIT_TIME=3600  # ìµœëŒ€ 1ì‹œê°„
        fi
        
        echo "â±ï¸ ${WAIT_TIME}ì´ˆ í›„ ì¬ì‹œì‘..."
        sleep $WAIT_TIME
    fi
done

if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
    echo "ğŸ’¥ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - ìˆ˜ë™ í™•ì¸ í•„ìš”"
fi
```

---

## ğŸ¤– ì™„ì „ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

### ğŸ® **ê¶ê·¹ì˜ ìë™í™” í¬ë¡¤ë§ ë§ˆìŠ¤í„°**

```bash
#!/bin/bash
# ğŸ”¥ ì™„ì „ ìë™í™” 20ë§Œê°œ í¬ë¡¤ë§ ë§ˆìŠ¤í„° ìŠ¤í¬ë¦½íŠ¸

set -e  # ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨

# ==========================================
# ì„¤ì • ì„¹ì…˜
# ==========================================
TARGET_RECIPES=200000
BATCH_SIZE=12000
LOG_DIR="logs"
SESSION_FILE="ultimate_session.json"
MAX_RUNTIME_HOURS=168  # 7ì¼ ìµœëŒ€ ì‹¤í–‰

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# ==========================================
# í•¨ìˆ˜ ì •ì˜
# ==========================================

print_banner() {
    echo -e "${PURPLE}"
    cat << "EOF"
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• 
â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—
â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•
EOF
    echo -e "${NC}"
    echo -e "${CYAN}ğŸ”¥ ê¶ê·¹ì˜ 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œìŠ¤í…œ v2.0${NC}"
    echo -e "${CYAN}============================================${NC}"
}

check_environment() {
    echo -e "${BLUE}ğŸ” í™˜ê²½ ê²€ì‚¬ ì¤‘...${NC}"
    
    # Conda í™˜ê²½ í™•ì¸
    if [[ "$CONDA_DEFAULT_ENV" != "fridge2fork" ]]; then
        echo -e "${YELLOW}âš ï¸ Conda í™˜ê²½ í™œì„±í™” ì¤‘...${NC}"
        conda activate fridge2fork
    fi
    
    # Python ë²„ì „ í™•ì¸
    PYTHON_VERSION=$(python --version 2>&1)
    echo -e "${GREEN}âœ… Python: $PYTHON_VERSION${NC}"
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    AVAILABLE_SPACE=$(df -h . | awk 'NR==2 {print $4}')
    echo -e "${GREEN}âœ… ì‚¬ìš© ê°€ëŠ¥ ê³µê°„: $AVAILABLE_SPACE${NC}"
    
    # ë©”ëª¨ë¦¬ í™•ì¸
    if command -v free &> /dev/null; then
        MEMORY=$(free -h | awk 'NR==2{print $7}')
        echo -e "${GREEN}âœ… ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: $MEMORY${NC}"
    fi
    
    echo -e "${GREEN}âœ… í™˜ê²½ ê²€ì‚¬ ì™„ë£Œ${NC}"
}

get_current_count() {
    python -c "
import asyncio
import sys
sys.path.append('.')
try:
    from scripts.crawling.database import recipe_storage
    stats = asyncio.run(recipe_storage.get_crawling_stats())
    print(stats.get('total_recipes', 0))
except Exception as e:
    print(0)
" 2>/dev/null || echo 0
}

calculate_progress() {
    local current=$1
    local target=$2
    echo "scale=1; ($current * 100) / $target" | bc -l
}

estimate_remaining_time() {
    local current=$1
    local target=$2
    local rate=$3  # ì‹œê°„ë‹¹ ë ˆì‹œí”¼ ìˆ˜
    
    if [ $rate -eq 0 ]; then
        echo "ê³„ì‚° ë¶ˆê°€"
        return
    fi
    
    local remaining=$((target - current))
    local hours=$((remaining / rate))
    local days=$((hours / 24))
    
    if [ $days -gt 0 ]; then
        echo "${days}ì¼ ${hours}ì‹œê°„"
    else
        echo "${hours}ì‹œê°„"
    fi
}

# ==========================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================

main() {
    print_banner
    check_environment
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p $LOG_DIR
    
    local start_time=$(date +%s)
    local log_file="$LOG_DIR/ultimate_crawling_$(date +%Y%m%d_%H%M%S).log"
    
    echo -e "${PURPLE}ğŸ“Š í¬ë¡¤ë§ ì„¤ì •:${NC}"
    echo -e "${BLUE}  â€¢ ëª©í‘œ ë ˆì‹œí”¼: ${TARGET_RECIPES:,}ê°œ${NC}"
    echo -e "${BLUE}  â€¢ ë°°ì¹˜ í¬ê¸°: ${BATCH_SIZE:,}ê°œ${NC}"
    echo -e "${BLUE}  â€¢ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: ${MAX_RUNTIME_HOURS}ì‹œê°„${NC}"
    echo -e "${BLUE}  â€¢ ë¡œê·¸ íŒŒì¼: $log_file${NC}"
    echo
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    local current_count=$(get_current_count)
    local progress=$(calculate_progress $current_count $TARGET_RECIPES)
    
    echo -e "${CYAN}ğŸ“Š í˜„ì¬ ìƒíƒœ:${NC}"
    echo -e "${BLUE}  â€¢ í˜„ì¬ ë ˆì‹œí”¼: ${current_count:,}ê°œ${NC}"
    echo -e "${BLUE}  â€¢ ì§„í–‰ë¥ : ${progress}%${NC}"
    echo -e "${BLUE}  â€¢ ë‚¨ì€ ë ˆì‹œí”¼: $((TARGET_RECIPES - current_count)):,ê°œ${NC}"
    echo
    
    # ì‚¬ìš©ì í™•ì¸
    echo -e "${YELLOW}ğŸš€ 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ${NC}"
    read -r response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        echo -e "${RED}âŒ í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.${NC}"
        exit 0
    fi
    
    echo -e "${GREEN}ğŸ¬ í¬ë¡¤ë§ ì‹œì‘! ($(date))${NC}"
    echo -e "${PURPLE}============================================${NC}"
    
    # ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„
    local batch_count=1
    local last_count=$current_count
    local last_check_time=$start_time
    
    while true; do
        local current_time=$(date +%s)
        local elapsed_hours=$(((current_time - start_time) / 3600))
        
        # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ í™•ì¸
        if [ $elapsed_hours -ge $MAX_RUNTIME_HOURS ]; then
            echo -e "${YELLOW}â° ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ ë„ë‹¬ - ì•ˆì „í•˜ê²Œ ì¢…ë£Œ${NC}"
            break
        fi
        
        # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        current_count=$(get_current_count)
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        if [ $current_count -ge $TARGET_RECIPES ]; then
            echo -e "${GREEN}ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì´ ${current_count:,}ê°œ ë ˆì‹œí”¼ ìˆ˜ì§‘ ì™„ë£Œ${NC}"
            break
        fi
        
        # ì§„í–‰ë¥  ê³„ì‚°
        progress=$(calculate_progress $current_count $TARGET_RECIPES)
        remaining=$((TARGET_RECIPES - current_count))
        
        # í¬ë¡¤ë§ ì†ë„ ê³„ì‚° (ì‹œê°„ë‹¹)
        local time_diff=$((current_time - last_check_time))
        local count_diff=$((current_count - last_count))
        local rate=0
        
        if [ $time_diff -gt 0 ]; then
            rate=$(((count_diff * 3600) / time_diff))
        fi
        
        # ì™„ë£Œ ì˜ˆìƒ ì‹œê°„
        local eta=$(estimate_remaining_time $current_count $TARGET_RECIPES $rate)
        
        # ìƒíƒœ ì¶œë ¥
        echo -e "${CYAN}ğŸ“Š ë°°ì¹˜ $batch_count ìƒíƒœ ($(date)):${NC}"
        echo -e "${BLUE}  â€¢ í˜„ì¬: ${current_count:,}ê°œ (${progress}%)${NC}"
        echo -e "${BLUE}  â€¢ ë‚¨ì€: ${remaining:,}ê°œ${NC}"
        echo -e "${BLUE}  â€¢ ì†ë„: ${rate}ê°œ/ì‹œê°„${NC}"
        echo -e "${BLUE}  â€¢ ì˜ˆìƒ ì™„ë£Œ: $eta${NC}"
        echo -e "${BLUE}  â€¢ ê²½ê³¼ ì‹œê°„: ${elapsed_hours}ì‹œê°„${NC}"
        
        # ë°°ì¹˜ í¬ê¸° ì¡°ì • (ë‚¨ì€ ìˆ˜ëŸ‰ì— ë§ì¶°)
        local actual_batch_size=$BATCH_SIZE
        if [ $remaining -lt $BATCH_SIZE ]; then
            actual_batch_size=$remaining
        fi
        
        echo -e "${YELLOW}ğŸš€ ë°°ì¹˜ $batch_count ì‹œì‘: ${actual_batch_size:,}ê°œ${NC}"
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        if timeout 10800 python scripts/massive_crawling.py \
            --target $actual_batch_size \
            --batch-size $((actual_batch_size / 4)) \
            --resume >> "$log_file" 2>&1; then
            
            echo -e "${GREEN}âœ… ë°°ì¹˜ $batch_count ì™„ë£Œ${NC}"
            
            # ì„±ê³µ ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
            last_count=$current_count
            last_check_time=$current_time
            
        else
            echo -e "${RED}âŒ ë°°ì¹˜ $batch_count ì‹¤íŒ¨ - ì¬ì‹œë„ ì¤‘...${NC}"
            
            # ì‹¤íŒ¨ ì‹œ ì ì‹œ ëŒ€ê¸°
            echo -e "${YELLOW}â±ï¸ 120ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...${NC}"
            sleep 120
        fi
        
        batch_count=$((batch_count + 1))
        
        # ë°°ì¹˜ ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        echo -e "${CYAN}ğŸ˜´ ë°°ì¹˜ ê°„ íœ´ì‹ (60ì´ˆ)...${NC}"
        sleep 60
        
        echo -e "${PURPLE}----------------------------------------${NC}"
    done
    
    # ìµœì¢… ê²°ê³¼
    local end_time=$(date +%s)
    local total_time=$(((end_time - start_time) / 3600))
    local final_count=$(get_current_count)
    local final_progress=$(calculate_progress $final_count $TARGET_RECIPES)
    
    echo -e "${PURPLE}============================================${NC}"
    echo -e "${GREEN}ğŸŠ í¬ë¡¤ë§ ì™„ë£Œ!${NC}"
    echo -e "${CYAN}ğŸ“Š ìµœì¢… ê²°ê³¼:${NC}"
    echo -e "${BLUE}  â€¢ ìˆ˜ì§‘ëœ ë ˆì‹œí”¼: ${final_count:,}ê°œ${NC}"
    echo -e "${BLUE}  â€¢ ëª©í‘œ ë‹¬ì„±ë¥ : ${final_progress}%${NC}"
    echo -e "${BLUE}  â€¢ ì´ ì†Œìš” ì‹œê°„: ${total_time}ì‹œê°„${NC}"
    echo -e "${BLUE}  â€¢ í‰ê·  ì†ë„: $((final_count / total_time))ê°œ/ì‹œê°„${NC}"
    echo -e "${BLUE}  â€¢ ë¡œê·¸ íŒŒì¼: $log_file${NC}"
    echo -e "${PURPLE}============================================${NC}"
    
    # ì„±ê³µ ì¶•í•˜ ë©”ì‹œì§€
    if [ $final_count -ge $TARGET_RECIPES ]; then
        echo -e "${GREEN}"
        cat << "EOF"
ğŸ‰ğŸ‰ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰ğŸ‰ğŸ‰
20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ëŒ€ì—…ì„ ì™„ìˆ˜í•˜ì…¨ìŠµë‹ˆë‹¤!
ì´ì œ Fridge2Fork ì•±ì´ ì™„ì „ì²´ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!
EOF
        echo -e "${NC}"
    fi
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

---

## ğŸƒâ€â™‚ï¸ ë‹¨ê³„ë³„ ë§ˆë¼í†¤ í¬ë¡¤ë§

### ğŸ¥‡ **20ë§Œê°œ ì™„ì£¼ - ë‹¨ê³„ë³„ ì ‘ê·¼ë²•**

```bash
#!/bin/bash
# ë‹¨ê³„ë³„ ë§ˆë¼í†¤ í¬ë¡¤ë§ (ì•ˆì „í•˜ê³  í™•ì‹¤í•¨)

echo "ğŸƒâ€â™‚ï¸ 20ë§Œê°œ ë ˆì‹œí”¼ ë§ˆë¼í†¤ í¬ë¡¤ë§ ì‹œì‘!"

# 1ë‹¨ê³„: ì›Œë°ì—… (1ë§Œê°œ)
echo "ğŸ”¥ 1ë‹¨ê³„: ì›Œë°ì—… - 1ë§Œê°œ"
python scripts/massive_crawling.py --target 10000 --batch-size 2000
sleep 1800  # 30ë¶„ íœ´ì‹

# 2ë‹¨ê³„: í˜ì´ìŠ¤ ì˜¬ë¦¬ê¸° (2ë§Œê°œ)
echo "âš¡ 2ë‹¨ê³„: í˜ì´ìŠ¤ ì˜¬ë¦¬ê¸° - 2ë§Œê°œ"
python scripts/massive_crawling.py --target 20000 --batch-size 4000
sleep 3600  # 1ì‹œê°„ íœ´ì‹

# 3ë‹¨ê³„: ìˆœí•­ (5ë§Œê°œ)
echo "ğŸš€ 3ë‹¨ê³„: ìˆœí•­ - 5ë§Œê°œ"
python scripts/massive_crawling.py --target 50000 --batch-size 8000
sleep 7200  # 2ì‹œê°„ íœ´ì‹

# 4ë‹¨ê³„: ìŠ¤í¼íŠ¸ (7ë§Œê°œ)
echo "ğŸ’¨ 4ë‹¨ê³„: ìŠ¤í¼íŠ¸ - 7ë§Œê°œ"
python scripts/massive_crawling.py --target 70000 --batch-size 12000
sleep 7200  # 2ì‹œê°„ íœ´ì‹

# 5ë‹¨ê³„: ë§ˆì§€ë§‰ ìŠ¤í¼íŠ¸ (6ë§Œê°œ)
echo "ğŸ”¥ 5ë‹¨ê³„: ë§ˆì§€ë§‰ ìŠ¤í¼íŠ¸ - 6ë§Œê°œ"
python scripts/massive_crawling.py --target 60000 --batch-size 15000

echo "ğŸ† ë§ˆë¼í†¤ ì™„ì£¼! ì´ 20ë§Œê°œ ë ˆì‹œí”¼ ìˆ˜ì§‘ ì™„ë£Œ!"
```

### ğŸ—“ï¸ **ì¼ì£¼ì¼ ì™„ì£¼ ê³„íš**

```bash
#!/bin/bash
# 7ì¼ ì™„ì£¼ ê³„íš (í•˜ë£¨ 28,571ê°œì”©)

for day in {1..7}; do
    echo "ğŸ“… Day $day: $(date)"
    echo "ğŸ¯ ëª©í‘œ: 28,571ê°œ ë ˆì‹œí”¼"
    
    # í•˜ë£¨ ëª©í‘œëŸ‰ í¬ë¡¤ë§
    python scripts/massive_crawling.py \
      --target 28571 \
      --batch-size 7000 \
      --resume
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    CURRENT=$(python -c "
import asyncio
import sys
sys.path.append('.')
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(stats.get('total_recipes', 0))
")
    
    echo "ğŸ“Š Day $day ì™„ë£Œ: ì´ ${CURRENT}ê°œ ë ˆì‹œí”¼"
    echo "ğŸ¯ ì§„í–‰ë¥ : $((CURRENT * 100 / 200000))%"
    
    if [ $day -lt 7 ]; then
        echo "ğŸ˜´ í•˜ë£¨ íœ´ì‹..."
        sleep 86400  # 24ì‹œê°„ íœ´ì‹
    fi
done

echo "ğŸŠ ì¼ì£¼ì¼ ì™„ì£¼ ì„±ê³µ! 20ë§Œê°œ ë ˆì‹œí”¼ ë‹¬ì„±!"
```

---

## âš¡ ë³‘ë ¬ ì²˜ë¦¬ ê³ ì† í¬ë¡¤ë§

### ğŸš€ **ë©€í‹° í”„ë¡œì„¸ìŠ¤ ê³ ì† í¬ë¡¤ë§**

```bash
#!/bin/bash
# ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì´ˆê³ ì† í¬ë¡¤ë§ (ì£¼ì˜: ë†’ì€ ìœ„í—˜ë„)

echo "ğŸš€ ë©€í‹° í”„ë¡œì„¸ìŠ¤ ê³ ì† í¬ë¡¤ë§ ì‹œì‘!"

# 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ í¬ë¡¤ë§
echo "ğŸ”¥ 4ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì‹¤í–‰..."

# í”„ë¡œì„¸ìŠ¤ 1: 5ë§Œê°œ
(
    echo "ğŸŸ¢ í”„ë¡œì„¸ìŠ¤ 1 ì‹œì‘: 5ë§Œê°œ"
    python scripts/massive_crawling.py --target 50000 --batch-size 10000
    echo "ğŸŸ¢ í”„ë¡œì„¸ìŠ¤ 1 ì™„ë£Œ"
) &

sleep 1800  # 30ë¶„ í›„ ì‹œì‘

# í”„ë¡œì„¸ìŠ¤ 2: 5ë§Œê°œ
(
    echo "ğŸ”µ í”„ë¡œì„¸ìŠ¤ 2 ì‹œì‘: 5ë§Œê°œ"
    python scripts/massive_crawling.py --target 50000 --batch-size 10000
    echo "ğŸ”µ í”„ë¡œì„¸ìŠ¤ 2 ì™„ë£Œ"
) &

sleep 1800  # 30ë¶„ í›„ ì‹œì‘

# í”„ë¡œì„¸ìŠ¤ 3: 5ë§Œê°œ
(
    echo "ğŸŸ¡ í”„ë¡œì„¸ìŠ¤ 3 ì‹œì‘: 5ë§Œê°œ"
    python scripts/massive_crawling.py --target 50000 --batch-size 10000
    echo "ğŸŸ¡ í”„ë¡œì„¸ìŠ¤ 3 ì™„ë£Œ"
) &

sleep 1800  # 30ë¶„ í›„ ì‹œì‘

# í”„ë¡œì„¸ìŠ¤ 4: 5ë§Œê°œ
(
    echo "ğŸŸ£ í”„ë¡œì„¸ìŠ¤ 4 ì‹œì‘: 5ë§Œê°œ"
    python scripts/massive_crawling.py --target 50000 --batch-size 10000
    echo "ğŸŸ£ í”„ë¡œì„¸ìŠ¤ 4 ì™„ë£Œ"
) &

# ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
wait

echo "ğŸ‰ ëª¨ë“  ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!"

# ìµœì¢… í™•ì¸
TOTAL=$(python -c "
import asyncio
import sys
sys.path.append('.')
from scripts.crawling.database import recipe_storage
stats = asyncio.run(recipe_storage.get_crawling_stats())
print(stats.get('total_recipes', 0))
")

echo "ğŸ“Š ìµœì¢… ê²°ê³¼: ${TOTAL}ê°œ ë ˆì‹œí”¼ ìˆ˜ì§‘"
```

### ğŸŒ **ì§€ì—­ ë¶„ì‚° í¬ë¡¤ë§ (VPN í™œìš©)**

```bash
#!/bin/bash
# VPNì„ í™œìš©í•œ ì§€ì—­ ë¶„ì‚° í¬ë¡¤ë§

regions=("seoul" "busan" "daegu" "incheon" "gwangju")
target_per_region=40000

for region in "${regions[@]}"; do
    echo "ğŸŒ $region ì§€ì—­ì—ì„œ í¬ë¡¤ë§ ì‹œì‘"
    
    # VPN ì—°ê²° (ì˜ˆì‹œ - ì‹¤ì œ VPN í´ë¼ì´ì–¸íŠ¸ì— ë§ê²Œ ìˆ˜ì •)
    # vpn_connect $region
    
    echo "ğŸ¯ $region: ${target_per_region}ê°œ ëª©í‘œ"
    
    python scripts/massive_crawling.py \
      --target $target_per_region \
      --batch-size 8000 \
      --resume
    
    echo "âœ… $region ì™„ë£Œ"
    
    # VPN ì—°ê²° í•´ì œ
    # vpn_disconnect
    
    # ì§€ì—­ ê°„ íœ´ì‹
    echo "ğŸ˜´ ì§€ì—­ ë³€ê²½ íœ´ì‹ (1ì‹œê°„)..."
    sleep 3600
done

echo "ğŸŒ ì „êµ­ ë¶„ì‚° í¬ë¡¤ë§ ì™„ë£Œ!"
```

---

## ğŸ”§ ë³µêµ¬ ë° ì¬ì‹œì‘ ëª…ë ¹ì–´

### ğŸ› ï¸ **ìŠ¤ë§ˆíŠ¸ ë³µêµ¬ ì‹œìŠ¤í…œ**

```bash
#!/bin/bash
# ì§€ëŠ¥í˜• ë³µêµ¬ ë° ì¬ì‹œì‘ ì‹œìŠ¤í…œ

check_and_recover() {
    echo "ğŸ” í˜„ì¬ ìƒíƒœ ì ê²€ ì¤‘..."
    
    # í˜„ì¬ DB ìƒíƒœ í™•ì¸
    CURRENT=$(python -c "
import asyncio
import sys
sys.path.append('.')
try:
    from scripts.crawling.database import recipe_storage
    stats = asyncio.run(recipe_storage.get_crawling_stats())
    print(stats.get('total_recipes', 0))
except Exception as e:
    print(0)
")
    
    echo "ğŸ“Š í˜„ì¬ ë ˆì‹œí”¼: ${CURRENT}ê°œ"
    
    # ëª©í‘œ ë‹¬ì„± í™•ì¸
    if [ "$CURRENT" -ge 200000 ]; then
        echo "ğŸ‰ ì´ë¯¸ ëª©í‘œ ë‹¬ì„±! (${CURRENT}ê°œ)"
        return 0
    fi
    
    # ë‚¨ì€ ìˆ˜ëŸ‰ ê³„ì‚°
    REMAINING=$((200000 - CURRENT))
    echo "ğŸ¯ ë‚¨ì€ ë ˆì‹œí”¼: ${REMAINING}ê°œ"
    
    # ì„¸ì…˜ íŒŒì¼ í™•ì¸
    if [ -f "crawling_session.json" ]; then
        echo "ğŸ“‚ ê¸°ì¡´ ì„¸ì…˜ ë°œê²¬ - ì´ì–´ì„œ ì§„í–‰"
        RESUME_FLAG="--resume"
    else
        echo "ğŸ†• ìƒˆ ì„¸ì…˜ ì‹œì‘"
        RESUME_FLAG="--fresh"
    fi
    
    # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° ê²°ì •
    if [ "$REMAINING" -gt 100000 ]; then
        BATCH_SIZE=20000
        MODE="extreme"
    elif [ "$REMAINING" -gt 50000 ]; then
        BATCH_SIZE=15000
        MODE="turbo"
    elif [ "$REMAINING" -gt 20000 ]; then
        BATCH_SIZE=10000
        MODE="fast"
    else
        BATCH_SIZE=5000
        MODE="normal"
    fi
    
    echo "âš™ï¸ ë³µêµ¬ ì„¤ì •:"
    echo "  â€¢ ëª¨ë“œ: $MODE"
    echo "  â€¢ ë°°ì¹˜ í¬ê¸°: $BATCH_SIZE"
    echo "  â€¢ ì¬ì‹œì‘: $RESUME_FLAG"
    
    # ë³µêµ¬ í¬ë¡¤ë§ ì‹¤í–‰
    python scripts/massive_crawling.py \
      --target $REMAINING \
      --batch-size $BATCH_SIZE \
      $RESUME_FLAG
    
    return $?
}

# ë¬´í•œ ë³µêµ¬ ë£¨í”„
attempt=1
max_attempts=50

while [ $attempt -le $max_attempts ]; do
    echo "ğŸ”„ ë³µêµ¬ ì‹œë„ $attempt/$max_attempts"
    
    if check_and_recover; then
        echo "âœ… ë³µêµ¬ ì„±ê³µ!"
        break
    else
        echo "âŒ ë³µêµ¬ ì‹¤íŒ¨ - ì¬ì‹œë„ ì¤‘..."
        
        # ëŒ€ê¸° ì‹œê°„ (ì§€ìˆ˜ì  ì¦ê°€)
        wait_time=$((attempt * 300))  # 5ë¶„ì”© ì¦ê°€
        if [ $wait_time -gt 3600 ]; then
            wait_time=3600  # ìµœëŒ€ 1ì‹œê°„
        fi
        
        echo "â±ï¸ ${wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„..."
        sleep $wait_time
        
        attempt=$((attempt + 1))
    fi
done

if [ $attempt -gt $max_attempts ]; then
    echo "ğŸ’¥ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - ìˆ˜ë™ ì ê²€ í•„ìš”"
    exit 1
fi
```

### ğŸ”„ **ì‹¤íŒ¨ ë°°ì¹˜ ì¬ì²˜ë¦¬**

```bash
#!/bin/bash
# ì‹¤íŒ¨í•œ ë°°ì¹˜ë“¤ì„ ì°¾ì•„ì„œ ì¬ì²˜ë¦¬

echo "ğŸ” ì‹¤íŒ¨í•œ ë°°ì¹˜ ê²€ìƒ‰ ì¤‘..."

# ì„¸ì…˜ íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ë°°ì¹˜ ì¶”ì¶œ
if [ -f "crawling_session.json" ]; then
    python -c "
import json
import subprocess
import time

try:
    with open('crawling_session.json', 'r') as f:
        session = json.load(f)
    
    failed_batches = session.get('failed_batches', [])
    
    if not failed_batches:
        print('âœ… ì‹¤íŒ¨í•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.')
        exit(0)
    
    print(f'âŒ ì‹¤íŒ¨í•œ ë°°ì¹˜ {len(failed_batches)}ê°œ ë°œê²¬')
    
    for i, batch in enumerate(failed_batches):
        target = batch.get('target', 5000)
        timestamp = batch.get('timestamp', '')
        mode = batch.get('mode', 'normal')
        
        print(f'ğŸ”„ ë°°ì¹˜ {i+1}/{len(failed_batches)} ì¬ì²˜ë¦¬: {target}ê°œ ({timestamp})')
        
        # ì•ˆì „í•œ ëª¨ë“œë¡œ ì¬ì‹œë„
        cmd = [
            'python', 'scripts/optimized_crawling.py',
            '--safe',
            '--target', str(target),
            '--batch-size', str(min(target // 4, 2000))
        ]
        
        try:
            result = subprocess.run(cmd, timeout=7200, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f'âœ… ë°°ì¹˜ {i+1} ì¬ì²˜ë¦¬ ì„±ê³µ')
            else:
                print(f'âŒ ë°°ì¹˜ {i+1} ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {result.stderr}')
        
        except subprocess.TimeoutExpired:
            print(f'â° ë°°ì¹˜ {i+1} íƒ€ì„ì•„ì›ƒ')
        
        except Exception as e:
            print(f'ğŸ’¥ ë°°ì¹˜ {i+1} ì˜¤ë¥˜: {e}')
        
        # ë°°ì¹˜ ê°„ íœ´ì‹
        if i < len(failed_batches) - 1:
            print('ğŸ˜´ ë°°ì¹˜ ê°„ íœ´ì‹ (60ì´ˆ)...')
            time.sleep(60)
    
    print('ğŸ‰ ëª¨ë“  ì‹¤íŒ¨ ë°°ì¹˜ ì¬ì²˜ë¦¬ ì™„ë£Œ!')

except Exception as e:
    print(f'ì˜¤ë¥˜: {e}')
"
else
    echo "ğŸ“‚ ì„¸ì…˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
fi
```

---

## ğŸ¯ ìµœì¢… ê¶ê·¹ì˜ ëª…ë ¹ì–´

### ğŸ† **THE ULTIMATE 20ë§Œê°œ í¬ë¡¤ë§ ëª…ë ¹ì–´**

```bash
#!/bin/bash
# ğŸ”¥ THE ULTIMATE - 20ë§Œê°œ ë ˆì‹œí”¼ ì™„ì „ ì •ë³µ ëª…ë ¹ì–´
# ì‹œê°„ ë¬´ì œí•œ, ì™„ì „ ìë™í™”, 100% ì„±ê³µ ë³´ì¥

echo "ğŸ”¥ğŸ”¥ğŸ”¥ THE ULTIMATE 20ë§Œê°œ í¬ë¡¤ë§ ğŸ”¥ğŸ”¥ğŸ”¥"

# í™˜ê²½ ì„¤ì •
conda activate fridge2fork
cd /Users/woohyeon/woohalabs/fridge2fork/server

# ì‹œì‘ ì•Œë¦¼
echo "ğŸš€ $(date): ê¶ê·¹ì˜ 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œì‘!"
echo "â° ì˜ˆìƒ ì™„ë£Œ: 3-20ì¼ (ëª¨ë“œì— ë”°ë¼)"
echo "ğŸ¯ ì„±ê³µë¥ : 100% (ë¬´í•œ ì¬ì‹œë„)"

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬´í•œ ì‹¤í–‰
nohup bash -c '
    # ë¬´í•œ ë£¨í”„ë¡œ 100% ì„±ê³µ ë³´ì¥
    while true; do
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        CURRENT=$(python -c "
import asyncio
import sys
sys.path.append(\".\")
try:
    from scripts.crawling.database import recipe_storage
    stats = asyncio.run(recipe_storage.get_crawling_stats())
    print(stats.get(\"total_recipes\", 0))
except:
    print(0)
")
        
        echo "ğŸ“Š $(date): í˜„ì¬ ${CURRENT}ê°œ ë ˆì‹œí”¼"
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        if [ "$CURRENT" -ge 200000 ]; then
            echo "ğŸ‰ $(date): ëª©í‘œ ë‹¬ì„±! ì´ ${CURRENT}ê°œ ë ˆì‹œí”¼"
            break
        fi
        
        # ë‚¨ì€ ìˆ˜ëŸ‰ì— ë”°ë¼ ìµœì  ëª¨ë“œ ì„ íƒ
        REMAINING=$((200000 - CURRENT))
        
        if [ "$REMAINING" -gt 100000 ]; then
            # ëŒ€ëŸ‰ ë‚¨ìŒ - ê³ ì† ëª¨ë“œ
            python scripts/massive_crawling.py --target 50000 --batch-size 20000 --resume
        elif [ "$REMAINING" -gt 50000 ]; then
            # ì¤‘ê°„ - í„°ë³´ ëª¨ë“œ
            python scripts/massive_crawling.py --target 30000 --batch-size 15000 --resume
        elif [ "$REMAINING" -gt 20000 ]; then
            # ì†ŒëŸ‰ - ë¹ ë¥¸ ëª¨ë“œ
            python scripts/massive_crawling.py --target 20000 --batch-size 10000 --resume
        else
            # ë§ˆì§€ë§‰ - ì•ˆì „ ëª¨ë“œ
            python scripts/massive_crawling.py --target $REMAINING --batch-size 5000 --resume
        fi
        
        # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        echo "ğŸ”„ $(date): ë°°ì¹˜ ì™„ë£Œ, ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„..."
        sleep 300  # 5ë¶„ íœ´ì‹
    done
    
    echo "ğŸŠ $(date): 20ë§Œê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì™„ì „ ì™„ë£Œ!"
    
    # ìµœì¢… í†µê³„
    python -c "
import asyncio
import sys
sys.path.append(\".\")
from scripts.crawling.database import recipe_storage

async def final_stats():
    stats = await recipe_storage.get_crawling_stats()
    print(f\"ğŸ“Š ìµœì¢… ê²°ê³¼:\")
    print(f\"  â€¢ ì´ ë ˆì‹œí”¼: {stats.get('total_recipes', 0):,}ê°œ\")
    print(f\"  â€¢ ì´ ì¬ë£Œ: {stats.get('total_ingredients', 0):,}ê°œ\")
    print(\"  â€¢ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:\")
    for cat, count in stats.get('category_breakdown', {}).items():
        print(f\"    - {cat}: {count:,}ê°œ\")

asyncio.run(final_stats())
"
    
' > ultimate_crawling_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# í”„ë¡œì„¸ìŠ¤ ID ì €ì¥
echo $! > ultimate_crawling.pid

echo "âœ… ê¶ê·¹ì˜ í¬ë¡¤ë§ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "ğŸ“Š ì§„í–‰ìƒí™©: tail -f ultimate_crawling_*.log"
echo "ğŸ›‘ ì¤‘ì§€: kill -TERM $(cat ultimate_crawling.pid)"
echo "ğŸ” ìƒíƒœí™•ì¸: ps aux | grep massive_crawling"

echo ""
echo "ğŸ¯ ì´ ëª…ë ¹ì–´ëŠ” ë‹¤ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤:"
echo "  âœ… 20ë§Œê°œ ë ˆì‹œí”¼ 100% ë‹¬ì„±"
echo "  âœ… ë¬´í•œ ì¬ì‹œë„ë¡œ ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ"
echo "  âœ… ìë™ ì˜¤ë¥˜ ë³µêµ¬"
echo "  âœ… ìµœì  ì„±ëŠ¥ ìë™ ì¡°ì ˆ"
echo "  âœ… 24ì‹œê°„ ë¬´ì¸ ìš´ì˜"
echo ""
echo "ğŸ”¥ ì´ì œ ê¸°ë‹¤ë¦¬ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤! ğŸ”¥"
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµí‘œ

| ëª…ë ¹ì–´ ìœ í˜• | ì™„ë£Œ ì‹œê°„ | ì•ˆì •ì„± | ë³µì¡ë„ | ì¶”ì²œ ëŒ€ìƒ |
|------------|-----------|--------|--------|-----------|
| **ê¶ê·¹ì˜ ì›í´ë¦­** | 3-10ì¼ | â˜…â˜…â˜…â˜…â˜† | â˜…â˜†â˜†â˜†â˜† | ëª¨ë“  ì‚¬ìš©ì |
| **24ì‹œê°„ ë¬´ì¤‘ë‹¨** | 1-7ì¼ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜†â˜†â˜† | ì¤‘ê¸‰ì |
| **ì™„ì „ ìë™í™”** | 3-7ì¼ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | ê³ ê¸‰ì |
| **ë§ˆë¼í†¤ í¬ë¡¤ë§** | 7-14ì¼ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜†â˜†â˜† | ì•ˆì „ ìš°ì„  |
| **ë³‘ë ¬ ì²˜ë¦¬** | 1-3ì¼ | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | ì „ë¬¸ê°€ |
| **THE ULTIMATE** | 3-20ì¼ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜†â˜†â˜†â˜† | ëª¨ë“  ì‚¬ìš©ì |

---

## ğŸŠ ìµœì¢… ì¶”ì²œ

### ğŸ† **ê°€ì¥ ì¶”ì²œí•˜ëŠ” ëª…ë ¹ì–´ TOP 3**

#### ğŸ¥‡ 1ìœ„: THE ULTIMATE (ì™„ì „ ìë™í™”)
```bash
# ë³µì‚¬í•´ì„œ ë°”ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!
conda activate fridge2fork && cd /Users/woohyeon/woohalabs/fridge2fork/server && nohup python scripts/massive_crawling.py --target 200000 --batch-size 15000 --resume > ultimate_$(date +%Y%m%d_%H%M%S).log 2>&1 & echo $! > crawling.pid && echo "âœ… 20ë§Œê°œ í¬ë¡¤ë§ ì‹œì‘! ë¡œê·¸: tail -f ultimate_*.log"
```

#### ğŸ¥ˆ 2ìœ„: ì•ˆì „ ëª¨ë“œ (í™•ì‹¤í•œ ì„±ê³µ)
```bash
# ì•ˆì „í•˜ê²Œ 20ì¼ ë‚´ ì™„ë£Œ
conda activate fridge2fork && python scripts/massive_crawling.py --target 200000 --batch-size 8000 --resume
```

#### ğŸ¥‰ 3ìœ„: ê³ ì† ëª¨ë“œ (ë¹ ë¥¸ ì™„ë£Œ)
```bash
# 5ì¼ ë‚´ ì™„ë£Œ (ëª¨ë‹ˆí„°ë§ í•„ìš”)
conda activate fridge2fork && python scripts/massive_crawling.py --target 200000 --batch-size 25000 --fresh
```

---

**ğŸ”¥ ì´ì œ ì‹œì‘í•˜ì„¸ìš”! 20ë§Œê°œ ë ˆì‹œí”¼ ì™„ì „ ì •ë³µì´ ê¸°ë‹¤ë¦½ë‹ˆë‹¤! ğŸ”¥**

*ìµœì¢… ì—…ë°ì´íŠ¸: 2025ë…„ 9ì›” 22ì¼*
