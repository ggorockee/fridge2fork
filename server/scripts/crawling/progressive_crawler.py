#!/usr/bin/env python3
"""
ì§„í–‰í˜• í¬ë¡¤ëŸ¬ - í˜ì´ì§€ ì§„í–‰ ìƒíƒœë¥¼ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ì—†ì´ ìƒˆë¡œìš´ ë ˆì‹œí”¼ ìˆ˜ì§‘
"""
import json
import os
import time
import logging
from typing import Dict, List, Optional
from datetime import datetime

class ProgressiveCrawler:
    """ì§„í–‰í˜• í¬ë¡¤ëŸ¬ - í˜ì´ì§€ ìƒíƒœ ì €ì¥"""
    
    def __init__(self):
        self.progress_file = "crawling_progress.json"
        self.crawled_urls_file = "crawled_urls.json"
        self.logger = logging.getLogger(__name__)
        
        # ì§„í–‰ ìƒíƒœ ë¡œë“œ
        self.progress = self.load_progress()
        self.crawled_urls = self.load_crawled_urls()
    
    def load_progress(self) -> Dict:
        """í¬ë¡¤ë§ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return {
            "current_category": 0,
            "current_page": 1,
            "total_crawled": 0,
            "categories_completed": [],
            "last_update": datetime.now().isoformat()
        }
    
    def save_progress(self):
        """í¬ë¡¤ë§ ì§„í–‰ ìƒíƒœ ì €ì¥"""
        try:
            self.progress["last_update"] = datetime.now().isoformat()
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_crawled_urls(self) -> set:
        """ì´ë¯¸ í¬ë¡¤ë§í•œ URL ëª©ë¡ ë¡œë“œ"""
        if os.path.exists(self.crawled_urls_file):
            try:
                with open(self.crawled_urls_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except Exception as e:
                print(f"URL ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return set()
    
    def save_crawled_urls(self):
        """í¬ë¡¤ë§í•œ URL ëª©ë¡ ì €ì¥"""
        try:
            with open(self.crawled_urls_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.crawled_urls), f, indent=2)
        except Exception as e:
            print(f"URL ëª©ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def get_next_urls_to_crawl(self, target_count: int) -> List[str]:
        """ë‹¤ìŒì— í¬ë¡¤ë§í•  URL ëª©ë¡ ë°˜í™˜"""
        categories = [
            "", "1", "2", "3", "4", "5", "6", "7", "8", "9", 
            "10", "11", "12", "13", "14", "15", "16", "17"
        ]
        
        new_urls = []
        current_category_idx = self.progress["current_category"]
        current_page = self.progress["current_page"]
        
        print(f"ğŸ“ í˜„ì¬ ìœ„ì¹˜: ì¹´í…Œê³ ë¦¬ {current_category_idx}, í˜ì´ì§€ {current_page}")
        print(f"ğŸ¯ ëª©í‘œ: {target_count}ê°œ ìƒˆ URL")
        
        while len(new_urls) < target_count and current_category_idx < len(categories):
            category = categories[current_category_idx]
            
            print(f"ğŸ” ì¹´í…Œê³ ë¦¬ '{category}' í˜ì´ì§€ {current_page} íƒìƒ‰ ì¤‘...")
            
            # í˜„ì¬ í˜ì´ì§€ì˜ URL ìˆ˜ì§‘ (ì‹œë®¬ë ˆì´ì…˜)
            page_urls = self.simulate_get_page_urls(category, current_page)
            
            if not page_urls:
                print(f"âŒ ì¹´í…Œê³ ë¦¬ '{category}' ì™„ë£Œ - ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ")
                self.progress["categories_completed"].append(category)
                current_category_idx += 1
                current_page = 1
                continue
            
            # ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
            fresh_urls = [url for url in page_urls if url not in self.crawled_urls]
            
            if not fresh_urls:
                print(f"â­ï¸ í˜ì´ì§€ {current_page} - ëª¨ë‘ í¬ë¡¤ë§ë¨, ë‹¤ìŒ í˜ì´ì§€ë¡œ")
                current_page += 1
                
                # í˜ì´ì§€ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ
                if current_page > 100:
                    print(f"ğŸ“„ ì¹´í…Œê³ ë¦¬ '{category}' ìµœëŒ€ í˜ì´ì§€ ë„ë‹¬ - ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ")
                    self.progress["categories_completed"].append(category)
                    current_category_idx += 1
                    current_page = 1
                continue
            
            # ìƒˆ URL ì¶”ê°€
            needed = min(len(fresh_urls), target_count - len(new_urls))
            selected_urls = fresh_urls[:needed]
            
            new_urls.extend(selected_urls)
            self.crawled_urls.update(selected_urls)
            
            print(f"âœ… í˜ì´ì§€ {current_page}: {len(selected_urls)}ê°œ ìƒˆ URL ì¶”ê°€ (ì´ {len(new_urls)}ê°œ)")
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ
            current_page += 1
        
        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.progress["current_category"] = current_category_idx
        self.progress["current_page"] = current_page
        self.progress["total_crawled"] += len(new_urls)
        
        # ìƒíƒœ ì €ì¥
        self.save_progress()
        self.save_crawled_urls()
        
        print(f"ğŸ‰ ì´ {len(new_urls)}ê°œ ìƒˆ URL ì¤€ë¹„ ì™„ë£Œ!")
        return new_urls
    
    def simulate_get_page_urls(self, category: str, page: int) -> List[str]:
        """í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜"""
        # ì‹¤ì œë¡œëŠ” MCP Playwrightë¡œ í˜ì´ì§€ ì ‘ì†í•˜ì—¬ URL ìˆ˜ì§‘
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        
        # ê° í˜ì´ì§€ë‹¹ ì•½ 20ê°œ ë ˆì‹œí”¼ê°€ ìˆë‹¤ê³  ê°€ì •
        base_id = (int(category) if category.isdigit() else 0) * 10000 + page * 20
        
        urls = []
        for i in range(20):
            recipe_id = base_id + i
            url = f"https://www.10000recipe.com/recipe/{recipe_id}"
            urls.append(url)
        
        # ì¼ë¶€ ì¹´í…Œê³ ë¦¬ëŠ” í˜ì´ì§€ê°€ ì ì„ ìˆ˜ ìˆìŒ
        if page > 50 and category in ["14", "15", "16", "17"]:
            return []  # ë§ˆì§€ë§‰ ì¹´í…Œê³ ë¦¬ë“¤ì€ í˜ì´ì§€ê°€ ì ìŒ
        
        return urls
    
    def get_crawling_stats(self) -> Dict:
        """í¬ë¡¤ë§ í†µê³„ ë°˜í™˜"""
        return {
            "current_category": self.progress["current_category"],
            "current_page": self.progress["current_page"],
            "total_urls_found": len(self.crawled_urls),
            "categories_completed": len(self.progress["categories_completed"]),
            "estimated_remaining": max(0, 200000 - len(self.crawled_urls))
        }
    
    def reset_progress(self):
        """ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        if os.path.exists(self.progress_file):
            os.remove(self.progress_file)
        if os.path.exists(self.crawled_urls_file):
            os.remove(self.crawled_urls_file)
        
        self.progress = self.load_progress()
        self.crawled_urls = set()
        
        print("ğŸ”„ ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    crawler = ProgressiveCrawler()
    
    print("ğŸ§ª ì§„í–‰í˜• í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í˜„ì¬ ìƒíƒœ
    stats = crawler.get_crawling_stats()
    print(f"ğŸ“Š í˜„ì¬ ìƒíƒœ:")
    print(f"  â€¢ í˜„ì¬ ì¹´í…Œê³ ë¦¬: {stats['current_category']}")
    print(f"  â€¢ í˜„ì¬ í˜ì´ì§€: {stats['current_page']}")
    print(f"  â€¢ ë°œê²¬í•œ URL: {stats['total_urls_found']:,}ê°œ")
    print(f"  â€¢ ì™„ë£Œ ì¹´í…Œê³ ë¦¬: {stats['categories_completed']}ê°œ")
    print()
    
    # 1ì°¨ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
    print("1ï¸âƒ£ ì²« ë²ˆì§¸ 100ê°œ URL ìˆ˜ì§‘:")
    urls1 = crawler.get_next_urls_to_crawl(100)
    print(f"ê²°ê³¼: {len(urls1)}ê°œ URL")
    print()
    
    # 2ì°¨ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
    print("2ï¸âƒ£ ë‘ ë²ˆì§¸ 100ê°œ URL ìˆ˜ì§‘:")
    urls2 = crawler.get_next_urls_to_crawl(100)
    print(f"ê²°ê³¼: {len(urls2)}ê°œ URL")
    print()
    
    # 3ì°¨ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
    print("3ï¸âƒ£ ì„¸ ë²ˆì§¸ 100ê°œ URL ìˆ˜ì§‘:")
    urls3 = crawler.get_next_urls_to_crawl(100)
    print(f"ê²°ê³¼: {len(urls3)}ê°œ URL")
    print()
    
    # ì¤‘ë³µ í™•ì¸
    all_urls = set(urls1 + urls2 + urls3)
    print(f"ğŸ” ì¤‘ë³µ í™•ì¸:")
    print(f"  â€¢ 1ì°¨: {len(urls1)}ê°œ")
    print(f"  â€¢ 2ì°¨: {len(urls2)}ê°œ") 
    print(f"  â€¢ 3ì°¨: {len(urls3)}ê°œ")
    print(f"  â€¢ ì´í•©: {len(urls1) + len(urls2) + len(urls3)}ê°œ")
    print(f"  â€¢ ê³ ìœ : {len(all_urls)}ê°œ")
    print(f"  â€¢ ì¤‘ë³µ: {len(urls1) + len(urls2) + len(urls3) - len(all_urls)}ê°œ")
    
    if len(all_urls) == len(urls1) + len(urls2) + len(urls3):
        print("âœ… ì™„ë²½! ì¤‘ë³µ ì—†ì´ ìƒˆë¡œìš´ URLë§Œ ìˆ˜ì§‘ë¨!")
    else:
        print("âŒ ì¤‘ë³µ ë°œìƒ")

if __name__ == "__main__":
    main()


