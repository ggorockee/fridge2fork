#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë²„ì „
"""
import asyncio
import argparse
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.crawling.progressive_crawler import ProgressiveCrawler

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ì¤‘ë³µ ì—†ì´ ì—°ì† í¬ë¡¤ë§")
    parser.add_argument("--target", type=int, default=100, help="í¬ë¡¤ë§í•  ë ˆì‹œí”¼ ìˆ˜")
    parser.add_argument("--reset", action="store_true", help="ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”")
    parser.add_argument("--status", action="store_true", help="í˜„ì¬ ì§„í–‰ ìƒíƒœ í™•ì¸")
    
    args = parser.parse_args()
    
    crawler = ProgressiveCrawler()
    
    if args.reset:
        crawler.reset_progress()
        print("ğŸ”„ ì§„í–‰ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    if args.status:
        stats = crawler.get_crawling_stats()
        print("ğŸ“Š í˜„ì¬ í¬ë¡¤ë§ ì§„í–‰ ìƒíƒœ:")
        print(f"  â€¢ í˜„ì¬ ì¹´í…Œê³ ë¦¬: {stats['current_category']}")
        print(f"  â€¢ í˜„ì¬ í˜ì´ì§€: {stats['current_page']}")
        print(f"  â€¢ ë°œê²¬í•œ URL: {stats['total_urls_found']:,}ê°œ")
        print(f"  â€¢ ì™„ë£Œ ì¹´í…Œê³ ë¦¬: {stats['categories_completed']}ê°œ")
        print(f"  â€¢ ì˜ˆìƒ ë‚¨ì€ ë ˆì‹œí”¼: {stats['estimated_remaining']:,}ê°œ")
        return
    
    print("ğŸ§  ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print(f"ğŸ¯ ëª©í‘œ: {args.target}ê°œ ë ˆì‹œí”¼")
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    stats = crawler.get_crawling_stats()
    print(f"ğŸ“ ì‹œì‘ ìœ„ì¹˜: ì¹´í…Œê³ ë¦¬ {stats['current_category']}, í˜ì´ì§€ {stats['current_page']}")
    
    # ìƒˆë¡œìš´ URL ìˆ˜ì§‘
    print(f"ğŸ” {args.target}ê°œ ìƒˆ URL ìˆ˜ì§‘ ì¤‘...")
    new_urls = crawler.get_next_urls_to_crawl(args.target)
    
    if not new_urls:
        print("âŒ ë” ì´ìƒ ìƒˆë¡œìš´ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë‚˜ ì‚¬ì´íŠ¸ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
        return
    
    print(f"âœ… {len(new_urls)}ê°œ ìƒˆ URL ìˆ˜ì§‘ ì™„ë£Œ!")
    
    # ì‹¤ì œ í¬ë¡¤ë§ì€ ì—¬ê¸°ì„œ ì§„í–‰
    print("ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
    print(f"python scripts/massive_crawling.py --target {len(new_urls)}")
    
    # ìµœì¢… ìƒíƒœ
    final_stats = crawler.get_crawling_stats()
    print(f"\nğŸ“ˆ ì—…ë°ì´íŠ¸ëœ ì§„í–‰ ìƒíƒœ:")
    print(f"  â€¢ í˜„ì¬ ì¹´í…Œê³ ë¦¬: {final_stats['current_category']}")
    print(f"  â€¢ í˜„ì¬ í˜ì´ì§€: {final_stats['current_page']}")
    print(f"  â€¢ ì´ ë°œê²¬ URL: {final_stats['total_urls_found']:,}ê°œ")

if __name__ == "__main__":
    main()
