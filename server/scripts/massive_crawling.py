#!/usr/bin/env python3
"""
ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (20ë§Œê°œ ë ˆì‹œí”¼)
- ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥
- ì§„í–‰ë¥  ì¶”ì 
- ì˜¤ë¥˜ ë³µêµ¬
- 24ì‹œê°„ ë¬´ì¸ ìš´ì˜
"""
import asyncio
import argparse
import sys
import os
import time
import json
import signal
from datetime import datetime, timedelta
from typing import Dict, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class MassiveCrawler:
    """ëŒ€ê·œëª¨ í¬ë¡¤ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.target_total = 200000  # 20ë§Œê°œ ëª©í‘œ
        self.batch_size = 5000      # ë°°ì¹˜ë‹¹ 5,000ê°œ
        self.session_file = "crawling_session.json"
        self.log_file = f"massive_crawling_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        self.is_running = True
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
    
    def signal_handler(self, signum, frame):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ - ì•ˆì „í•œ ì¢…ë£Œ"""
        print(f"\nğŸ›‘ ì‹œê·¸ë„ {signum} ìˆ˜ì‹  - ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.is_running = False
    
    def save_session(self, progress: Dict):
        """ì„¸ì…˜ ì •ë³´ ì €ì¥"""
        try:
            with open(self.session_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ì„¸ì…˜ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_session(self) -> Dict:
        """ì„¸ì…˜ ì •ë³´ ë¡œë“œ"""
        try:
            if os.path.exists(self.session_file):
                with open(self.session_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"ì„¸ì…˜ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return {
            "total_crawled": 0,
            "current_batch": 1,
            "start_time": datetime.now().isoformat(),
            "last_update": datetime.now().isoformat(),
            "failed_batches": [],
            "completed_batches": []
        }
    
    async def get_current_db_count(self) -> int:
        """í˜„ì¬ DBì— ì €ì¥ëœ ë ˆì‹œí”¼ ìˆ˜ ì¡°íšŒ"""
        try:
            from scripts.crawling.database import recipe_storage
            stats = await recipe_storage.get_crawling_stats()
            return stats.get('total_recipes', 0)
        except Exception as e:
            print(f"DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return 0
    
    def calculate_optimal_settings(self, remaining: int, time_limit_hours: int = 24) -> Dict:
        """ìµœì  í¬ë¡¤ë§ ì„¤ì • ê³„ì‚°"""
        # ì‹œê°„ë‹¹ ëª©í‘œ ë ˆì‹œí”¼ ìˆ˜
        recipes_per_hour = remaining / time_limit_hours
        
        if recipes_per_hour <= 500:
            return {
                "mode": "safe",
                "delay": 2.0,
                "batch_size": 50,
                "description": "ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ"
            }
        elif recipes_per_hour <= 1500:
            return {
                "mode": "normal", 
                "delay": 1.0,
                "batch_size": 100,
                "description": "âš–ï¸ ê· í˜• ëª¨ë“œ"
            }
        elif recipes_per_hour <= 3000:
            return {
                "mode": "fast",
                "delay": 0.5,
                "batch_size": 150,
                "description": "âš¡ ë¹ ë¥¸ ëª¨ë“œ"
            }
        elif recipes_per_hour <= 5000:
            return {
                "mode": "turbo",
                "delay": 0.3,
                "batch_size": 200,
                "description": "ğŸš€ í„°ë³´ ëª¨ë“œ"
            }
        else:
            return {
                "mode": "extreme",
                "delay": 0.1,
                "batch_size": 300,
                "description": "ğŸ”¥ ê·¹í•œ ëª¨ë“œ (ìœ„í—˜)"
            }
    
    def estimate_completion_time(self, remaining: int, settings: Dict) -> str:
        """ì™„ë£Œ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°"""
        recipes_per_second = 1 / settings["delay"]
        total_seconds = remaining / recipes_per_second
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë° ê¸°íƒ€ ì˜¤ë²„í—¤ë“œ ê³ ë ¤ (30% ì¶”ê°€)
        total_seconds *= 1.3
        
        days = int(total_seconds // 86400)
        hours = int((total_seconds % 86400) // 3600)
        minutes = int((total_seconds % 3600) // 60)
        
        if days > 0:
            return f"{days}ì¼ {hours}ì‹œê°„ {minutes}ë¶„"
        elif hours > 0:
            return f"{hours}ì‹œê°„ {minutes}ë¶„"
        else:
            return f"{minutes}ë¶„"
    
    async def run_batch_crawling(self, target: int, settings: Dict) -> bool:
        """ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            import subprocess
            
            cmd = [
                "python", "scripts/optimized_crawling.py",
                f"--{settings['mode']}",
                "--target", str(target)
            ]
            
            print(f"ğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {target}ê°œ ({settings['description']})")
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=7200  # 2ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                print(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {target}ê°œ")
                return True
            else:
                print(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° ë°°ì¹˜ íƒ€ì„ì•„ì›ƒ: {target}ê°œ")
            return False
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì˜¤ë¥˜: {e}")
            return False
    
    def log_progress(self, message: str):
        """ì§„í–‰ìƒí™© ë¡œê¹…"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        
        print(log_message)
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_message + '\n')
        except Exception as e:
            print(f"ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def massive_crawling(self, resume: bool = True):
        """ëŒ€ê·œëª¨ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        self.log_progress("ğŸš€ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œì‘ (20ë§Œê°œ ëª©í‘œ)")
        
        # ì„¸ì…˜ ë¡œë“œ
        if resume:
            progress = self.load_session()
            self.log_progress(f"ğŸ“‚ ê¸°ì¡´ ì„¸ì…˜ ë¡œë“œ: {progress.get('total_crawled', 0)}ê°œ ì™„ë£Œ")
        else:
            progress = self.load_session()
            progress.update({
                "total_crawled": 0,
                "current_batch": 1,
                "start_time": datetime.now().isoformat(),
                "failed_batches": [],
                "completed_batches": []
            })
        
        start_time = datetime.fromisoformat(progress["start_time"])
        
        while self.is_running and progress["total_crawled"] < self.target_total:
            try:
                # í˜„ì¬ DB ìƒíƒœ í™•ì¸
                current_db_count = await self.get_current_db_count()
                remaining = self.target_total - current_db_count
                
                if remaining <= 0:
                    self.log_progress(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! DBì— {current_db_count:,}ê°œ ë ˆì‹œí”¼ ì €ì¥ë¨")
                    break
                
                # ìµœì  ì„¤ì • ê³„ì‚°
                elapsed_hours = (datetime.now() - start_time).total_seconds() / 3600
                remaining_hours = max(24 - elapsed_hours, 1)  # ìµœì†Œ 1ì‹œê°„
                
                settings = self.calculate_optimal_settings(remaining, remaining_hours)
                estimated_time = self.estimate_completion_time(remaining, settings)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                progress_percent = (current_db_count / self.target_total) * 100
                self.log_progress("=" * 60)
                self.log_progress(f"ğŸ“Š ì§„í–‰ë¥ : {progress_percent:.1f}% ({current_db_count:,}/{self.target_total:,})")
                self.log_progress(f"ğŸ“‹ ë‚¨ì€ ë ˆì‹œí”¼: {remaining:,}ê°œ")
                self.log_progress(f"âš™ï¸ í˜„ì¬ ëª¨ë“œ: {settings['description']}")
                self.log_progress(f"â° ì˜ˆìƒ ì™„ë£Œ: {estimated_time}")
                self.log_progress(f"ğŸ• ê²½ê³¼ ì‹œê°„: {elapsed_hours:.1f}ì‹œê°„")
                
                # ë°°ì¹˜ í¬ê¸° ê²°ì • (ë‚¨ì€ ìˆ˜ëŸ‰ì— ë§ì¶° ì¡°ì •)
                batch_target = min(self.batch_size, remaining)
                
                # ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
                success = await self.run_batch_crawling(batch_target, settings)
                
                if success:
                    progress["completed_batches"].append({
                        "batch": progress["current_batch"],
                        "target": batch_target,
                        "timestamp": datetime.now().isoformat(),
                        "mode": settings["mode"]
                    })
                    progress["total_crawled"] += batch_target
                    self.log_progress(f"âœ… ë°°ì¹˜ {progress['current_batch']} ì™„ë£Œ")
                else:
                    progress["failed_batches"].append({
                        "batch": progress["current_batch"],
                        "target": batch_target,
                        "timestamp": datetime.now().isoformat(),
                        "mode": settings["mode"]
                    })
                    self.log_progress(f"âŒ ë°°ì¹˜ {progress['current_batch']} ì‹¤íŒ¨")
                    
                    # ì‹¤íŒ¨ ì‹œ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    self.log_progress("â±ï¸ 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(30)
                
                progress["current_batch"] += 1
                progress["last_update"] = datetime.now().isoformat()
                
                # ì„¸ì…˜ ì €ì¥
                self.save_session(progress)
                
                # ë°°ì¹˜ ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                if settings["mode"] in ["turbo", "extreme"]:
                    rest_time = 60  # 1ë¶„ íœ´ì‹
                    self.log_progress(f"ğŸ˜´ ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ {rest_time}ì´ˆ íœ´ì‹...")
                    await asyncio.sleep(rest_time)
                else:
                    await asyncio.sleep(10)  # 10ì´ˆ íœ´ì‹
                
            except Exception as e:
                self.log_progress(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                self.log_progress("â±ï¸ 60ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                await asyncio.sleep(60)
        
        # ìµœì¢… ê²°ê³¼
        final_count = await self.get_current_db_count()
        total_time = datetime.now() - start_time
        
        self.log_progress("=" * 60)
        self.log_progress("ğŸŠ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        self.log_progress(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {final_count:,}ê°œ ë ˆì‹œí”¼")
        self.log_progress(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±ë¥ : {(final_count/self.target_total)*100:.1f}%")
        self.log_progress(f"â° ì´ ì†Œìš” ì‹œê°„: {total_time}")
        self.log_progress(f"ğŸ“ˆ ì„±ê³µí•œ ë°°ì¹˜: {len(progress.get('completed_batches', []))}ê°œ")
        self.log_progress(f"âŒ ì‹¤íŒ¨í•œ ë°°ì¹˜: {len(progress.get('failed_batches', []))}ê°œ")
        
        # ì„¸ì…˜ íŒŒì¼ ì •ë¦¬
        if final_count >= self.target_total:
            try:
                os.remove(self.session_file)
                self.log_progress("ğŸ§¹ ì„¸ì…˜ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ëŒ€ê·œëª¨ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (20ë§Œê°œ)")
    parser.add_argument("--target", type=int, default=200000, help="ëª©í‘œ ë ˆì‹œí”¼ ìˆ˜")
    parser.add_argument("--batch-size", type=int, default=5000, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--resume", action="store_true", default=True, help="ê¸°ì¡´ ì„¸ì…˜ ì´ì–´ì„œ ì§„í–‰")
    parser.add_argument("--fresh", action="store_true", help="ìƒˆë¡œ ì‹œì‘")
    
    args = parser.parse_args()
    
    print("ğŸ”¥ Fridge2Fork ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print(f"ğŸ¯ ëª©í‘œ: {args.target:,}ê°œ ë ˆì‹œí”¼")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size:,}ê°œ")
    print(f"ğŸ”„ ì´ì–´ì„œ ì§„í–‰: {'ì˜ˆ' if args.resume and not args.fresh else 'ì•„ë‹ˆì˜¤'}")
    print("=" * 60)
    
    crawler = MassiveCrawler()
    crawler.target_total = args.target
    crawler.batch_size = args.batch_size
    
    try:
        asyncio.run(crawler.massive_crawling(resume=args.resume and not args.fresh))
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
