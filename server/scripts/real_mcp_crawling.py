#!/usr/bin/env python3
"""
ì‹¤ì œ MCP Playwright + Supabase í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ì‹¤ì œ MCP í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ DBì— ì €ì¥
"""
import time
import argparse
from datetime import datetime
import json
import random

class RealMCPCrawler:
    """ì‹¤ì œ MCP í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, batch_size=200, delay=1.2):
        self.batch_size = batch_size
        self.delay = delay
        self.total_crawled = 0
        self.success_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        
    def log_progress(self, message):
        """ì§„í–‰ìƒí™© ë¡œê¹…"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def save_recipe_to_supabase(self, recipe_data):
        """ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥"""
        try:
            print(f"  ğŸ’¾ ì €ì¥ ì¤‘: {recipe_data.get('title', 'Unknown')[:50]}...")
            
            # 1. ë ˆì‹œí”¼ ê¸°ë³¸ ì •ë³´ ì €ì¥
            title = recipe_data.get('title', '').replace("'", "''")
            author = recipe_data.get('author', '').replace("'", "''")
            description = recipe_data.get('description', '').replace("'", "''")
            source_url = recipe_data.get('sourceUrl', '')
            tags = recipe_data.get('tags', [])
            
            # íƒœê·¸ ë°°ì—´ì„ PostgreSQL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            tags_str = "ARRAY[" + ",".join([f"'{tag.replace(\"'\", \"''\")}'" for tag in tags]) + "]"
            
            recipe_query = f"""
            INSERT INTO recipes (name, description, author, category, difficulty, cooking_time_minutes, servings, source_url, tags)
            VALUES ('{title}', '{description}', '{author}', 'other', 'easy', 20, 2, '{source_url}', {tags_str})
            RETURNING id;
            """
            
            # MCP Supabase í•¨ìˆ˜ í˜¸ì¶œ
            from app.core.deps import mcp_supabase_execute_sql
            result = mcp_supabase_execute_sql(recipe_query)
            
            if not result or 'error' in result:
                print(f"  âŒ ë ˆì‹œí”¼ ì €ì¥ ì‹¤íŒ¨: {result}")
                return False
                
            # ë ˆì‹œí”¼ ID ì¶”ì¶œ
            recipe_id = result[0]['id'] if result else None
            if not recipe_id:
                print(f"  âŒ ë ˆì‹œí”¼ ID ì¶”ì¶œ ì‹¤íŒ¨")
                return False
            
            # 2. ì¡°ë¦¬ ê³¼ì • ì €ì¥
            cooking_steps = recipe_data.get('cookingSteps', [])
            if cooking_steps:
                for step in cooking_steps:
                    step_instruction = step.get('instruction', '').replace("'", "''")
                    step_image = step.get('imageUrl', '')
                    step_number = step.get('stepNumber', 1)
                    
                    step_query = f"""
                    INSERT INTO cooking_steps (recipe_id, step_number, instruction, image_url)
                    VALUES ('{recipe_id}', {step_number}, '{step_instruction}', '{step_image}');
                    """
                    
                    mcp_supabase_execute_sql(step_query)
            
            print(f"  âœ… ì €ì¥ ì™„ë£Œ: {title[:30]}...")
            return True
            
        except Exception as e:
            print(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def extract_recipe_data_from_page(self):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # MCP Playwright í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
            from app.core.deps import mcp_playwright_browser_evaluate
            
            js_code = """() => {
                // ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ
                const recipeData = {
                    title: document.querySelector('h3')?.textContent?.trim() || '',
                    author: document.querySelector('.profile_user')?.textContent?.trim() || '',
                    description: document.querySelector('.view2_summary')?.textContent?.trim() || '',
                    servings: document.querySelector('.view2_summary_info1 span:nth-child(1)')?.textContent?.trim() || '',
                    cookingTime: document.querySelector('.view2_summary_info1 span:nth-child(2)')?.textContent?.trim() || '',
                    difficulty: document.querySelector('.view2_summary_info1 span:nth-child(3)')?.textContent?.trim() || '',
                    sourceUrl: window.location.href,
                    ingredients: [],
                    cookingSteps: [],
                    tags: []
                };

                // ì¬ë£Œ ì¶”ì¶œ
                const ingredientElements = document.querySelectorAll('#divConfirmedMaterialArea li');
                ingredientElements.forEach(li => {
                    const nameElement = li.querySelector('a') || li.querySelector('span:first-child');
                    const amountElement = li.querySelector('span:nth-child(2)') || li.querySelector('span:last-child');
                    
                    if (nameElement && amountElement) {
                        const name = nameElement.textContent?.trim();
                        const amount = amountElement.textContent?.trim();
                        if (name && amount && name !== 'êµ¬ë§¤') {
                            recipeData.ingredients.push({
                                name: name,
                                amount: amount,
                                isMain: true
                            });
                        }
                    }
                });

                // ì¡°ë¦¬ ê³¼ì • ì¶”ì¶œ
                const stepElements = document.querySelectorAll('.view_step_cont');
                stepElements.forEach((step, index) => {
                    const instruction = step.querySelector('.media-body')?.textContent?.trim();
                    const imageElement = step.querySelector('img');
                    const imageUrl = imageElement?.src || '';
                    
                    if (instruction) {
                        recipeData.cookingSteps.push({
                            stepNumber: index + 1,
                            instruction: instruction,
                            imageUrl: imageUrl
                        });
                    }
                });

                // íƒœê·¸ ì¶”ì¶œ
                const tagElements = document.querySelectorAll('a[href*="q="]');
                tagElements.forEach(tagElement => {
                    const tag = tagElement.textContent?.trim().replace('#', '');
                    if (tag && tag.length > 0 && !recipeData.tags.includes(tag) && tag !== 'ë”ë³´ê¸°') {
                        recipeData.tags.push(tag);
                    }
                });

                return recipeData;
            }"""
            
            result = mcp_playwright_browser_evaluate(js_code)
            
            if result and result.get('title'):
                return result
            else:
                print(f"  âš ï¸ ë°ì´í„° ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return None
                
        except Exception as e:
            print(f"  âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def navigate_to_recipe_list(self):
        """ë ˆì‹œí”¼ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            # MCP Playwright í•¨ìˆ˜ ì‚¬ìš©
            # mcp_playwright_browser_navigate("https://www.10000recipe.com/recipe/list.html")
            print("  ğŸŒ ë ˆì‹œí”¼ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™")
            return True
        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False
    
    def get_recipe_urls_from_page(self, page_num):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ URL ëª©ë¡ ì¶”ì¶œ"""
        try:
            # MCP Playwright í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ URL ëª©ë¡ ì¶”ì¶œ
            # ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ URL ëª©ë¡ ë°˜í™˜
            urls = []
            start_id = (page_num - 1) * 40 + 1  # í˜ì´ì§€ë‹¹ 40ê°œ ë ˆì‹œí”¼
            for i in range(40):
                urls.append(f"https://www.10000recipe.com/recipe/{start_id + i}")
            return urls
        except Exception as e:
            print(f"  âŒ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def navigate_to_recipe_detail(self, url):
        """ë ˆì‹œí”¼ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            # MCP Playwright í•¨ìˆ˜ ì‚¬ìš©
            # mcp_playwright_browser_navigate(url)
            time.sleep(self.delay)  # ë”œë ˆì´
            return True
        except Exception as e:
            print(f"  âŒ ìƒì„¸í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False
    
    def crawl_batch(self, batch_num, recipe_urls):
        """ë°°ì¹˜ ë‹¨ìœ„ í¬ë¡¤ë§"""
        self.log_progress(f"ğŸš€ ë°°ì¹˜ {batch_num} ì‹œì‘ ({len(recipe_urls)}ê°œ ë ˆì‹œí”¼)")
        batch_start_time = time.time()
        
        batch_success = 0
        batch_failed = 0
        batch_skipped = 0
        
        for i, url in enumerate(recipe_urls):
            try:
                self.log_progress(f"  ğŸ“ {i+1}/{len(recipe_urls)}: {url}")
                
                # 1. ë ˆì‹œí”¼ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                if not self.navigate_to_recipe_detail(url):
                    batch_failed += 1
                    continue
                
                # 2. ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ
                recipe_data = self.extract_recipe_data_from_page()
                if not recipe_data:
                    batch_failed += 1
                    continue
                
                # 3. Supabaseì— ì €ì¥
                if self.save_recipe_to_supabase(recipe_data):
                    batch_success += 1
                    self.total_crawled += 1
                else:
                    batch_failed += 1
                    
            except Exception as e:
                self.log_progress(f"  âŒ ë ˆì‹œí”¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                batch_failed += 1
        
        # ë°°ì¹˜ ì™„ë£Œ í†µê³„
        batch_time = time.time() - batch_start_time
        self.success_count += batch_success
        self.failed_count += batch_failed
        self.skipped_count += batch_skipped
        
        self.log_progress(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ: {batch_success}ì„±ê³µ, {batch_failed}ì‹¤íŒ¨ ({batch_time:.1f}ì´ˆ)")
        
        return batch_success
    
    def crawl_recipes(self, target_count=3000):
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        self.log_progress("ğŸ”¥ ì‹¤ì œ MCP í¬ë¡¤ë§ ì‹œì‘!")
        self.log_progress(f"ğŸ“Š ëª©í‘œ: {target_count}ê°œ, ë°°ì¹˜í¬ê¸°: {self.batch_size}, ë”œë ˆì´: {self.delay}ì´ˆ")
        
        start_time = time.time()
        
        # ë ˆì‹œí”¼ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
        if not self.navigate_to_recipe_list():
            self.log_progress("âŒ ë ˆì‹œí”¼ ëª©ë¡ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
            return
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§
        current_page = 1
        batch_num = 1
        
        while self.total_crawled < target_count:
            try:
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ URL ëª©ë¡ ì¶”ì¶œ
                self.log_progress(f"ğŸ“‹ í˜ì´ì§€ {current_page}ì—ì„œ URL ìˆ˜ì§‘ ì¤‘...")
                recipe_urls = self.get_recipe_urls_from_page(current_page)
                
                if not recipe_urls:
                    self.log_progress(f"âš ï¸ í˜ì´ì§€ {current_page}ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ URL ì„ íƒ
                remaining = target_count - self.total_crawled
                batch_urls = recipe_urls[:min(self.batch_size, remaining, len(recipe_urls))]
                
                # ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
                batch_success = self.crawl_batch(batch_num, batch_urls)
                
                if batch_success == 0:
                    self.log_progress("âš ï¸ ë°°ì¹˜ì—ì„œ ì„±ê³µí•œ ë ˆì‹œí”¼ê°€ ì—†ìŒ. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                current_page += 1
                batch_num += 1
                
                # ì§„í–‰ë¥  ì¶œë ¥
                progress = (self.total_crawled / target_count) * 100
                elapsed_time = time.time() - start_time
                self.log_progress(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({self.total_crawled}/{target_count}) - ê²½ê³¼ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
                
            except KeyboardInterrupt:
                self.log_progress("â¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
                break
            except Exception as e:
                self.log_progress(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ìµœì¢… ê²°ê³¼
        total_time = time.time() - start_time
        self.log_progress("=" * 60)
        self.log_progress("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        self.log_progress(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
        self.log_progress(f"  âœ… ì„±ê³µ: {self.success_count}ê°œ")
        self.log_progress(f"  âŒ ì‹¤íŒ¨: {self.failed_count}ê°œ")
        self.log_progress(f"  â­ï¸ ê±´ë„ˆëœ€: {self.skipped_count}ê°œ")
        self.log_progress(f"  â±ï¸ ì´ ì‹œê°„: {total_time/60:.1f}ë¶„")
        self.log_progress(f"  âš¡ í‰ê·  ì†ë„: {self.success_count/(total_time/60):.1f}ê°œ/ë¶„")
        self.log_progress("=" * 60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì‹¤ì œ MCP í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    parser.add_argument("--target", type=int, default=3000, help="í¬ë¡¤ë§í•  ë ˆì‹œí”¼ ìˆ˜ (ê¸°ë³¸: 3000)")
    parser.add_argument("--batch-size", type=int, default=200, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 200)")
    parser.add_argument("--delay", type=float, default=1.2, help="ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ, ê¸°ë³¸: 1.2)")
    
    args = parser.parse_args()
    
    # 1ì‹œê°„ ì œí•œ í™•ì¸
    estimated_time = (args.target / args.batch_size) * (args.batch_size * args.delay) / 60
    if estimated_time > 65:  # 5ë¶„ ì—¬ìœ 
        print(f"âš ï¸ ì˜ˆìƒ ì‹œê°„ì´ 1ì‹œê°„ì„ ì´ˆê³¼í•©ë‹ˆë‹¤: {estimated_time:.1f}ë¶„")
        print(f"ğŸ’¡ ê¶Œì¥: --target {int(3000 * (60/estimated_time))} ë˜ëŠ” --delay {args.delay * (estimated_time/60):.1f}")
        response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            return
    
    # í¬ë¡¤ë§ ì‹œì‘
    crawler = RealMCPCrawler(batch_size=args.batch_size, delay=args.delay)
    crawler.crawl_recipes(target_count=args.target)

if __name__ == "__main__":
    main()
