#!/usr/bin/env python3
"""
ëŒ€ëŸ‰ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import asyncio
import argparse
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawling.crawler import crawler
from crawling.config import CrawlingConfig

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë§Œê°œì˜ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    parser.add_argument(
        "--target", 
        type=int, 
        default=CrawlingConfig.TOTAL_TARGET_RECIPES,
        help=f"í¬ë¡¤ë§í•  ë ˆì‹œí”¼ ìˆ˜ (ê¸°ë³¸ê°’: {CrawlingConfig.TOTAL_TARGET_RECIPES})"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=CrawlingConfig.BATCH_SIZE,
        help=f"ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: {CrawlingConfig.BATCH_SIZE})"
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=CrawlingConfig.DELAY_BETWEEN_REQUESTS,
        help=f"ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ, ê¸°ë³¸ê°’: {CrawlingConfig.DELAY_BETWEEN_REQUESTS})"
    )
    
    args = parser.parse_args()
    
    print("ğŸ³ Fridge2Fork ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print(f"ğŸ“Š ëª©í‘œ ë ˆì‹œí”¼ ìˆ˜: {args.target:,}ê°œ")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size}ê°œ")
    print(f"â±ï¸  ìš”ì²­ ë”œë ˆì´: {args.delay}ì´ˆ")
    print("=" * 50)
    
    # ì„¤ì • ì—…ë°ì´íŠ¸
    crawler.config.TOTAL_TARGET_RECIPES = args.target
    crawler.config.BATCH_SIZE = args.batch_size
    crawler.config.DELAY_BETWEEN_REQUESTS = args.delay
    
    try:
        # í¬ë¡¤ë§ ì‹œì‘
        results = await crawler.crawl_recipes(args.target)
        
        print("\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 50)
        print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì €ì¥ëœ ë ˆì‹œí”¼: {results['success']:,}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ë ˆì‹œí”¼: {results['failed']:,}ê°œ") 
        print(f"â­ï¸  ê±´ë„ˆë›´ ë ˆì‹œí”¼: {results['skipped']:,}ê°œ")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ ë ˆì‹œí”¼: {results['total_crawled']:,}ê°œ")
        
        # ì„±ê³µë¥  ê³„ì‚°
        if results['total_crawled'] > 0:
            success_rate = (results['success'] / results['total_crawled']) * 100
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        print("=" * 50)
        
        # ìµœì¢… ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        stats = await crawler.storage.get_crawling_stats()
        if stats:
            print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:")
            print(f"  â€¢ ì´ ë ˆì‹œí”¼: {stats.get('total_recipes', 0):,}ê°œ")
            print(f"  â€¢ ì´ ì¬ë£Œ: {stats.get('total_ingredients', 0):,}ê°œ")
            
            category_breakdown = stats.get('category_breakdown', {})
            if category_breakdown:
                print("  â€¢ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
                for category, count in category_breakdown.items():
                    print(f"    - {category}: {count:,}ê°œ")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


