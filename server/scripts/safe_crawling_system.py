#!/usr/bin/env python3
"""
ì•ˆì „í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - í˜ì´ì§€ë„¤ì´ì…˜ ë¬¸ì œ í•´ê²°
URL ì„ ìˆ˜ì§‘ ë°©ì‹ìœ¼ë¡œ ëˆ„ë½ ì—†ëŠ” í¬ë¡¤ë§
"""
import asyncio
import json
import os
import time
import argparse
from typing import List, Set, Dict
from datetime import datetime

class SafeCrawlingSystem:
    """ì•ˆì „í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.all_urls_file = "all_recipe_urls.json"
        self.crawled_urls_file = "crawled_recipe_urls.json"
        self.failed_urls_file = "failed_recipe_urls.json"
        
    def collect_all_urls(self, max_pages: int = 1000) -> List[str]:
        """1ë‹¨ê³„: ëª¨ë“  ë ˆì‹œí”¼ URLì„ ë¨¼ì € ìˆ˜ì§‘"""
        print("ğŸ” 1ë‹¨ê³„: ì „ì²´ ë ˆì‹œí”¼ URL ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        all_urls = set()
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëª¨ë“  í˜ì´ì§€ ìˆœíšŒ
        categories = ["", "1", "2", "3", "4", "5", "6", "7", "8", "9", 
                     "10", "11", "12", "13", "14", "15", "16", "17"]
        
        for category in categories:
            print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ '{category}' URL ìˆ˜ì§‘ ì¤‘...")
            
            for page in range(1, max_pages + 1):
                try:
                    # ì‹¤ì œë¡œëŠ” MCP Playwrightë¡œ í˜ì´ì§€ ì ‘ì†
                    # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
                    page_urls = self._simulate_get_page_urls(category, page)
                    
                    if not page_urls:
                        print(f"  ğŸ“„ í˜ì´ì§€ {page}: ë” ì´ìƒ ë ˆì‹œí”¼ ì—†ìŒ - ë‹¤ìŒ ì¹´í…Œê³ ë¦¬")
                        break
                    
                    new_urls = [url for url in page_urls if url not in all_urls]
                    all_urls.update(new_urls)
                    
                    print(f"  ğŸ“„ í˜ì´ì§€ {page}: {len(new_urls)}ê°œ ìƒˆ URL (ì´ {len(all_urls)}ê°œ)")
                    
                    # ë¹ ë¥¸ ìˆ˜ì§‘ì„ ìœ„í•´ ë”œë ˆì´ ìµœì†Œí™”
                    time.sleep(0.1)
                    
                except Exception as e:
                    print(f"  âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                    continue
        
        # URL ëª©ë¡ ì €ì¥
        url_list = list(all_urls)
        self._save_urls(url_list, self.all_urls_file)
        
        print("=" * 60)
        print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(url_list):,}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {self.all_urls_file}")
        
        return url_list
    
    def crawl_from_url_range(self, start_idx: int, end_idx: int) -> Dict[str, int]:
        """2ë‹¨ê³„: ì§€ì •ëœ ë²”ìœ„ì˜ URLì„ í¬ë¡¤ë§"""
        print(f"ğŸš€ 2ë‹¨ê³„: URL ë²”ìœ„ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“Š ë²”ìœ„: {start_idx:,} ~ {end_idx:,}")
        print("=" * 60)
        
        # ì „ì²´ URL ëª©ë¡ ë¡œë“œ
        all_urls = self._load_urls(self.all_urls_file)
        if not all_urls:
            print("âŒ URL ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € collect_all_urlsë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return {"success": 0, "failed": 0, "skipped": 0}
        
        # ì´ë¯¸ í¬ë¡¤ë§í•œ URL ë¡œë“œ
        crawled_urls = set(self._load_urls(self.crawled_urls_file))
        failed_urls = set(self._load_urls(self.failed_urls_file))
        
        # í¬ë¡¤ë§í•  URL ì¶”ì¶œ
        target_urls = all_urls[start_idx:end_idx]
        print(f"ğŸ¯ ëŒ€ìƒ URL: {len(target_urls):,}ê°œ")
        
        results = {"success": 0, "failed": 0, "skipped": 0}
        
        for i, url in enumerate(target_urls, 1):
            try:
                print(f"ğŸ” ì§„í–‰: {i:,}/{len(target_urls):,} - {url}")
                
                # ì´ë¯¸ ì²˜ë¦¬í•œ URL ê±´ë„ˆë›°ê¸°
                if url in crawled_urls:
                    results["skipped"] += 1
                    print(f"  â­ï¸ ì´ë¯¸ í¬ë¡¤ë§ë¨")
                    continue
                
                if url in failed_urls:
                    results["skipped"] += 1
                    print(f"  â­ï¸ ì´ì „ì— ì‹¤íŒ¨í•¨")
                    continue
                
                # ì‹¤ì œ í¬ë¡¤ë§ (ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜)
                success = self._simulate_crawl_recipe(url)
                
                if success:
                    results["success"] += 1
                    crawled_urls.add(url)
                    print(f"  âœ… ì„±ê³µ")
                else:
                    results["failed"] += 1
                    failed_urls.add(url)
                    print(f"  âŒ ì‹¤íŒ¨")
                
                # ì§„í–‰ ìƒí™© ì €ì¥ (100ê°œë§ˆë‹¤)
                if i % 100 == 0:
                    self._save_urls(list(crawled_urls), self.crawled_urls_file)
                    self._save_urls(list(failed_urls), self.failed_urls_file)
                    print(f"  ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥ë¨")
                
                # ë”œë ˆì´
                time.sleep(0.5)
                
            except Exception as e:
                results["failed"] += 1
                failed_urls.add(url)
                print(f"  ğŸ’¥ ì˜¤ë¥˜: {e}")
        
        # ìµœì¢… ì €ì¥
        self._save_urls(list(crawled_urls), self.crawled_urls_file)
        self._save_urls(list(failed_urls), self.failed_urls_file)
        
        print("=" * 60)
        print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"  âœ… ì„±ê³µ: {results['success']:,}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {results['failed']:,}ê°œ")
        print(f"  â­ï¸ ê±´ë„ˆëœ€: {results['skipped']:,}ê°œ")
        
        return results
    
    def get_crawling_status(self) -> Dict:
        """í¬ë¡¤ë§ ìƒíƒœ í™•ì¸"""
        all_urls = self._load_urls(self.all_urls_file)
        crawled_urls = self._load_urls(self.crawled_urls_file)
        failed_urls = self._load_urls(self.failed_urls_file)
        
        total = len(all_urls)
        completed = len(crawled_urls)
        failed = len(failed_urls)
        remaining = total - completed - failed
        
        return {
            "total_urls": total,
            "completed": completed,
            "failed": failed,
            "remaining": remaining,
            "progress_percent": (completed / total * 100) if total > 0 else 0
        }
    
    def _simulate_get_page_urls(self, category: str, page: int) -> List[str]:
        """í˜ì´ì§€ì—ì„œ URL ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜"""
        # ì‹¤ì œë¡œëŠ” MCP Playwright ì‚¬ìš©
        base_id = (int(category) if category.isdigit() else 0) * 100000 + page * 20
        
        urls = []
        for i in range(20):  # í˜ì´ì§€ë‹¹ 20ê°œ ë ˆì‹œí”¼
            recipe_id = base_id + i
            url = f"https://www.10000recipe.com/recipe/{recipe_id}"
            urls.append(url)
        
        # ì¼ë¶€ ì¹´í…Œê³ ë¦¬ëŠ” í˜ì´ì§€ê°€ ì ìŒ
        if page > 100:
            return []
        
        return urls
    
    def _simulate_crawl_recipe(self, url: str) -> bool:
        """ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜"""
        # ì‹¤ì œë¡œëŠ” MCP Playwright + ë°ì´í„° íŒŒì‹± + DB ì €ì¥
        # 90% ì„±ê³µë¥ ë¡œ ì‹œë®¬ë ˆì´ì…˜
        import random
        return random.random() > 0.1
    
    def _load_urls(self, filename: str) -> List[str]:
        """URL ëª©ë¡ ë¡œë“œ"""
        if not os.path.exists(filename):
            return []
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"URL ë¡œë“œ ì˜¤ë¥˜ ({filename}): {e}")
            return []
    
    def _save_urls(self, urls: List[str], filename: str):
        """URL ëª©ë¡ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(urls, f, indent=2)
        except Exception as e:
            print(f"URL ì €ì¥ ì˜¤ë¥˜ ({filename}): {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì•ˆì „í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    parser.add_argument("--collect-urls", action="store_true", help="1ë‹¨ê³„: ëª¨ë“  URL ìˆ˜ì§‘")
    parser.add_argument("--crawl", action="store_true", help="2ë‹¨ê³„: URL ë²”ìœ„ í¬ë¡¤ë§")
    parser.add_argument("--start", type=int, default=0, help="í¬ë¡¤ë§ ì‹œì‘ ì¸ë±ìŠ¤")
    parser.add_argument("--end", type=int, default=1000, help="í¬ë¡¤ë§ ì¢…ë£Œ ì¸ë±ìŠ¤")
    parser.add_argument("--status", action="store_true", help="í¬ë¡¤ë§ ìƒíƒœ í™•ì¸")
    
    args = parser.parse_args()
    
    crawler = SafeCrawlingSystem()
    
    if args.collect_urls:
        print("ğŸ¯ ì‚¬ìš©ì ì§€ì  ì‚¬í•­ í•´ê²°:")
        print("- í˜ì´ì§€ë„¤ì´ì…˜ ë¬¸ì œ â†’ URL ì„ ìˆ˜ì§‘ìœ¼ë¡œ í•´ê²°")
        print("- ì¤‘ê°„ ì—…ë°ì´íŠ¸ ë¬¸ì œ â†’ ì•ˆì •ì ì¸ URL ê¸°ë°˜ í¬ë¡¤ë§")
        print("- ëˆ„ë½/ì¤‘ë³µ ë¬¸ì œ â†’ ì™„ì „ ì œê±°")
        print()
        
        urls = crawler.collect_all_urls()
        print(f"\nğŸ‰ ì´ì œ {len(urls):,}ê°œ URLì„ ì•ˆì „í•˜ê²Œ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        
    elif args.crawl:
        results = crawler.crawl_from_url_range(args.start, args.end)
        
    elif args.status:
        status = crawler.get_crawling_status()
        print("ğŸ“Š í¬ë¡¤ë§ ìƒíƒœ:")
        print(f"  â€¢ ì „ì²´ URL: {status['total_urls']:,}ê°œ")
        print(f"  â€¢ ì™„ë£Œ: {status['completed']:,}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {status['failed']:,}ê°œ")
        print(f"  â€¢ ë‚¨ì€: {status['remaining']:,}ê°œ")
        print(f"  â€¢ ì§„í–‰ë¥ : {status['progress_percent']:.1f}%")
        
    else:
        print("ğŸš¨ í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ì˜ ë¬¸ì œì :")
        print("1. ì¤‘ê°„ì— ìƒˆ ë ˆì‹œí”¼ ì¶”ê°€ â†’ ê¸°ì¡´ ë ˆì‹œí”¼ê°€ ë’¤ë¡œ ë°€ë¦¼")
        print("2. ë ˆì‹œí”¼ ì‚­ì œ â†’ ë’¤ ë ˆì‹œí”¼ë“¤ì´ ì•ìœ¼ë¡œ ë‹¹ê²¨ì§")
        print("3. ì •ë ¬ ìˆœì„œ ë³€ê²½ â†’ ì „ì²´ ìˆœì„œ ë°”ë€œ")
        print("4. ê²°ê³¼: ëˆ„ë½ ë˜ëŠ” ì¤‘ë³µ ë°œìƒ!")
        print()
        print("âœ… í•´ê²°ì±…: URL ì„ ìˆ˜ì§‘ ë°©ì‹")
        print("python scripts/safe_crawling_system.py --collect-urls")
        print("python scripts/safe_crawling_system.py --crawl --start 0 --end 10000")

if __name__ == "__main__":
    main()

