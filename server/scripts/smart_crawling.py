#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ì¤‘ë³µ ì—†ì´ ì—°ì† í¬ë¡¤ë§
"""
import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.crawling.progressive_crawler import ProgressiveCrawler

async def smart_batch_crawling(batch_count: int = 3, recipes_per_batch: int = 100):
    """ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ í¬ë¡¤ë§"""
    
    print("ğŸ§  ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘!")
    print("=" * 60)
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"  â€¢ ë°°ì¹˜ ìˆ˜: {batch_count}ê°œ")
    print(f"  â€¢ ë°°ì¹˜ë‹¹ ë ˆì‹œí”¼: {recipes_per_batch}ê°œ")
    print(f"  â€¢ ì´ ëª©í‘œ: {batch_count * recipes_per_batch}ê°œ")
    print("=" * 60)
    
    crawler = ProgressiveCrawler()
    total_crawled = 0
    
    for batch_num in range(1, batch_count + 1):
        print(f"\nğŸš€ ë°°ì¹˜ {batch_num}/{batch_count} ì‹œì‘")
        print("-" * 40)
        
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        stats = crawler.get_crawling_stats()
        print(f"ğŸ“ í˜„ì¬ ìœ„ì¹˜: ì¹´í…Œê³ ë¦¬ {stats['current_category']}, í˜ì´ì§€ {stats['current_page']}")
        
        # ìƒˆë¡œìš´ URL ìˆ˜ì§‘
        new_urls = crawler.get_next_urls_to_crawl(recipes_per_batch)
        
        if not new_urls:
            print("âŒ ë” ì´ìƒ ìƒˆë¡œìš´ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        print(f"âœ… {len(new_urls)}ê°œ ìƒˆ URL ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜ (ì—¬ê¸°ì„œëŠ” ì„±ê³µìœ¼ë¡œ ê°€ì •)
        success_count = len(new_urls)  # ì‹¤ì œë¡œëŠ” í¬ë¡¤ë§ ê²°ê³¼
        total_crawled += success_count
        
        print(f"ğŸ“Š ë°°ì¹˜ {batch_num} ê²°ê³¼:")
        print(f"  â€¢ ìƒˆ URL: {len(new_urls)}ê°œ")
        print(f"  â€¢ ì„±ê³µ: {success_count}ê°œ")
        print(f"  â€¢ ëˆ„ì  ì´ê³„: {total_crawled}ê°œ")
        
        # ë°°ì¹˜ ê°„ íœ´ì‹
        if batch_num < batch_count:
            print("ğŸ˜´ ë°°ì¹˜ ê°„ íœ´ì‹ (5ì´ˆ)...")
            await asyncio.sleep(5)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {total_crawled}ê°œ ë ˆì‹œí”¼ ìˆ˜ì§‘")
    
    # ìµœì¢… í†µê³„
    final_stats = crawler.get_crawling_stats()
    print(f"ğŸ“ˆ ì§„í–‰ ìƒí™©:")
    print(f"  â€¢ í˜„ì¬ ì¹´í…Œê³ ë¦¬: {final_stats['current_category']}")
    print(f"  â€¢ í˜„ì¬ í˜ì´ì§€: {final_stats['current_page']}")
    print(f"  â€¢ ì´ ë°œê²¬ URL: {final_stats['total_urls_found']:,}ê°œ")
    print(f"  â€¢ ì™„ë£Œ ì¹´í…Œê³ ë¦¬: {final_stats['categories_completed']}ê°œ")
    
    return total_crawled

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ ì‚¬ìš©ì ì§ˆë¬¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("1. 100ê°œë¥¼ í–ˆì–´")
    print("2. ê·¸ë‹¤ìŒì— ë‹¤ì‹œ 100ê°œ í–ˆì–´") 
    print("3. ê·¸ ë‹¤ìŒì— ë‹¤ì‹œ 100ê°œë¥¼ í–ˆì–´")
    print("â†’ ì´ 300ê°œê°€ ë˜ë‚˜ìš”? ì•„ë‹ˆë©´ 100ê°œë§Œ ë˜ë‚˜ìš”?")
    print()
    
    # ê¸°ì¡´ ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
    crawler = ProgressiveCrawler()
    crawler.reset_progress()
    
    # 3ë²ˆì˜ 100ê°œ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
    total = await smart_batch_crawling(batch_count=3, recipes_per_batch=100)
    
    print(f"\nğŸŠ ë‹µ: ì´ {total}ê°œ ë ˆì‹œí”¼ê°€ ìˆ˜ì§‘ë©ë‹ˆë‹¤!")
    print("âœ… ì¤‘ë³µ ì—†ì´ ìƒˆë¡œìš´ ë ˆì‹œí”¼ë§Œ ê³„ì† ìˆ˜ì§‘ë¨")
    print("âœ… 1ì°¨: 1-5í˜ì´ì§€, 2ì°¨: 6-10í˜ì´ì§€, 3ì°¨: 11-15í˜ì´ì§€")
    print("âœ… í˜ì´ì§€ ì§„í–‰ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ì´ì–´ì„œ ê³„ì†ë¨")

if __name__ == "__main__":
    asyncio.run(main())

